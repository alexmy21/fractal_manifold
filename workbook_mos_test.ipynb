{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eed538c",
   "metadata": {},
   "source": [
    "# Manifold OS Test Notebook\n",
    "\n",
    "**Testing the complete pipeline:**\n",
    "1. Synthetic data generation\n",
    "2. Token absorption via Kernel\n",
    "3. HLLSet creation and operations\n",
    "4. N-token ingestion via ManifoldOS\n",
    "5. Persistent storage to DuckDB\n",
    "6. Fractal loop primitives (new!)\n",
    "\n",
    "**Architecture layers tested:**\n",
    "- Layer 1: HLLSet (Register Layer)\n",
    "- Layer 2: BSS Morphisms (Lattice Construction)\n",
    "- Layer 3: Fractal Loop (Scale Hierarchy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9b3dbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_BITS=10, τ=0.7, ρ=0.3\n",
      "✓ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "# Core imports\n",
    "from core import (\n",
    "    HLLSet, Kernel, compute_sha1,\n",
    "    P_BITS, SHARED_SEED, DEFAULT_TAU, DEFAULT_RHO\n",
    ")\n",
    "\n",
    "# Fractal imports (new!)\n",
    "from core import (\n",
    "    n_tokenize, multi_scale_tokenize, overlap,\n",
    "    build_token_tower, entanglement_number\n",
    ")\n",
    "\n",
    "# ManifoldOS\n",
    "from core.manifold_os import ManifoldOS, TokenizationConfig\n",
    "\n",
    "print(f\"P_BITS={P_BITS}, τ={DEFAULT_TAU}, ρ={DEFAULT_RHO}\")\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401a2da3",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Data\n",
    "\n",
    "Create test data with known structure for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7da3c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 50 sentences\n",
      "\n",
      "Sample sentences:\n",
      "  • pending customer validates invoice\n",
      "  • completed product updates category\n",
      "  • pending supplier rejects customer\n",
      "  • new order processes invoice\n",
      "  • new category processes category\n"
     ]
    }
   ],
   "source": [
    "# Synthetic data generator\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Sample vocabularies\n",
    "NOUNS = ['customer', 'order', 'product', 'invoice', 'payment', 'account', 'user', 'item', 'category', 'supplier']\n",
    "VERBS = ['creates', 'updates', 'deletes', 'processes', 'validates', 'approves', 'rejects', 'modifies']\n",
    "ADJECTIVES = ['new', 'pending', 'active', 'completed', 'cancelled', 'urgent', 'standard', 'premium']\n",
    "\n",
    "def generate_sentence() -> str:\n",
    "    \"\"\"Generate a simple sentence.\"\"\"\n",
    "    adj = random.choice(ADJECTIVES)\n",
    "    noun = random.choice(NOUNS)\n",
    "    verb = random.choice(VERBS)\n",
    "    noun2 = random.choice(NOUNS)\n",
    "    return f\"{adj} {noun} {verb} {noun2}\"\n",
    "\n",
    "def generate_dataset(n_sentences: int = 100) -> list:\n",
    "    \"\"\"Generate dataset of sentences.\"\"\"\n",
    "    return [generate_sentence() for _ in range(n_sentences)]\n",
    "\n",
    "# Generate test data\n",
    "test_sentences = generate_dataset(50)\n",
    "print(f\"Generated {len(test_sentences)} sentences\")\n",
    "print(\"\\nSample sentences:\")\n",
    "for s in test_sentences[:5]:\n",
    "    print(f\"  • {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56077603",
   "metadata": {},
   "source": [
    "## 2. Direct Kernel Operations\n",
    "\n",
    "Test the stateless Kernel for token absorption and HLLSet operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60510682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'pending customer validates invoice'\n",
      "Tokens: ['pending', 'customer', 'validates', 'invoice']\n",
      "\n",
      "HLLSet created:\n",
      "  Name (SHA1): d005c83ea421f96d...\n",
      "  Cardinality: 4.0\n",
      "  P_BITS: 10\n"
     ]
    }
   ],
   "source": [
    "# Initialize kernel\n",
    "kernel = Kernel()\n",
    "\n",
    "# Absorb tokens from first sentence\n",
    "sentence1 = test_sentences[0]\n",
    "tokens1 = sentence1.split()\n",
    "print(f\"Sentence: '{sentence1}'\")\n",
    "print(f\"Tokens: {tokens1}\")\n",
    "\n",
    "# Create HLLSet via absorb\n",
    "hll1 = kernel.absorb(tokens1)\n",
    "print(f\"\\nHLLSet created:\")\n",
    "print(f\"  Name (SHA1): {hll1.name[:16]}...\")\n",
    "print(f\"  Cardinality: {hll1.cardinality():.1f}\")\n",
    "print(f\"  P_BITS: {hll1.p_bits}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2ce338e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HLL1: 'pending customer validates invoice' -> card=4.0\n",
      "HLL2: 'completed product updates category' -> card=5.0\n",
      "\n",
      "Union: card=9.0\n",
      "Intersection: card=0.0\n",
      "Jaccard similarity: 0.000\n"
     ]
    }
   ],
   "source": [
    "# Test HLLSet operations: union, intersection, difference\n",
    "sentence2 = test_sentences[1]\n",
    "tokens2 = sentence2.split()\n",
    "hll2 = kernel.absorb(tokens2)\n",
    "\n",
    "print(f\"HLL1: '{sentence1}' -> card={hll1.cardinality():.1f}\")\n",
    "print(f\"HLL2: '{sentence2}' -> card={hll2.cardinality():.1f}\")\n",
    "\n",
    "# Union\n",
    "hll_union = kernel.union(hll1, hll2)\n",
    "print(f\"\\nUnion: card={hll_union.cardinality():.1f}\")\n",
    "\n",
    "# Intersection\n",
    "hll_inter = kernel.intersection(hll1, hll2)\n",
    "print(f\"Intersection: card={hll_inter.cardinality():.1f}\")\n",
    "\n",
    "# Similarity (Jaccard)\n",
    "similarity = kernel.similarity(hll1, hll2)\n",
    "print(f\"Jaccard similarity: {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e951a1cf",
   "metadata": {},
   "source": [
    "## 3. Fractal Loop Primitives (New!)\n",
    "\n",
    "Test the new fractal_core module: n-tokenization, overlap, scale hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "854ce745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokens (20): ['pending', 'customer', 'validates', 'invoice', 'completed', 'product', 'updates', 'category', 'pending', 'supplier']...\n",
      "\n",
      "1-grams: 20 tokens\n",
      "2-grams: 19 tokens\n",
      "3-grams: 18 tokens\n",
      "\n",
      "Sample bigrams: ['pending|customer', 'customer|validates', 'validates|invoice', 'invoice|completed', 'completed|product']\n",
      "Sample trigrams: ['pending|customer|validates', 'customer|validates|invoice', 'validates|invoice|completed']\n"
     ]
    }
   ],
   "source": [
    "# Test n-tokenization\n",
    "all_tokens = ' '.join(test_sentences[:5]).split()\n",
    "print(f\"Sample tokens ({len(all_tokens)}): {all_tokens[:10]}...\")\n",
    "\n",
    "# Generate n-grams\n",
    "unigrams = n_tokenize(all_tokens, 1)\n",
    "bigrams = n_tokenize(all_tokens, 2)\n",
    "trigrams = n_tokenize(all_tokens, 3)\n",
    "\n",
    "print(f\"\\n1-grams: {len(unigrams)} tokens\")\n",
    "print(f\"2-grams: {len(bigrams)} tokens\")\n",
    "print(f\"3-grams: {len(trigrams)} tokens\")\n",
    "\n",
    "print(f\"\\nSample bigrams: {bigrams[:5]}\")\n",
    "print(f\"Sample trigrams: {trigrams[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc79385b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-scale tokenization:\n",
      "  Scale 1: 20 tokens\n",
      "  Scale 2: 19 tokens\n",
      "  Scale 3: 18 tokens\n",
      "  Scale 4: 17 tokens\n"
     ]
    }
   ],
   "source": [
    "# Test multi-scale tokenization\n",
    "scales = multi_scale_tokenize(all_tokens, scales=(1, 2, 3, 4))\n",
    "print(\"Multi-scale tokenization:\")\n",
    "for n, toks in scales.items():\n",
    "    print(f\"  Scale {n}: {len(toks)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcf4c140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fractal Tower (depth=3):\n",
      "  ScaleLevel(L0, 1-gram, card=14, src=tokens)\n",
      "  ScaleLevel(L1, 2-gram, card=21, src=tokens)\n",
      "  ScaleLevel(L2, 3-gram, card=19, src=tokens)\n",
      "\n",
      "Entanglement matrix:\n",
      "  E(0,0) = 1.000\n",
      "  E(0,1) = 0.000\n",
      "  E(0,2) = 0.000\n",
      "  E(1,1) = 1.000\n",
      "  E(1,2) = 0.000\n",
      "  E(2,2) = 1.000\n"
     ]
    }
   ],
   "source": [
    "# Build fractal tower\n",
    "tower = build_token_tower(all_tokens, scales=(1, 2, 3))\n",
    "\n",
    "print(f\"Fractal Tower (depth={tower.depth}):\")\n",
    "for level in tower.levels:\n",
    "    print(f\"  {level}\")\n",
    "\n",
    "print(f\"\\nEntanglement matrix:\")\n",
    "for (m, n), e in sorted(tower.entanglement.items()):\n",
    "    if m <= n:\n",
    "        print(f\"  E({m},{n}) = {e:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5da7bac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap between scales:\n",
      "  Scale 0 card: 14.0\n",
      "  Scale 1 card: 21.0\n",
      "  Overlap: 0.000\n"
     ]
    }
   ],
   "source": [
    "# Test overlap measure (used for lattice topology analysis)\n",
    "hll_scale1 = tower.levels[0].hllset\n",
    "hll_scale2 = tower.levels[1].hllset if len(tower.levels) > 1 else hll_scale1\n",
    "\n",
    "print(f\"Overlap between scales:\")\n",
    "print(f\"  Scale 0 card: {hll_scale1.cardinality():.1f}\")\n",
    "print(f\"  Scale 1 card: {hll_scale2.cardinality():.1f}\")\n",
    "print(f\"  Overlap: {overlap(hll_scale1, hll_scale2):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a4b239",
   "metadata": {},
   "source": [
    "## 4. ManifoldOS Initialization\n",
    "\n",
    "Initialize MOS with DuckDB persistent storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66970538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storage path: /tmp/mos_test_6s8nkzhk\n",
      "DuckDB path: /tmp/mos_test_6s8nkzhk/metadata.duckdb\n",
      "✓ Extension registered: storage v1.4.4\n",
      "✓ Extension registered: storage v1.4.4\n",
      "\n",
      "✓ ManifoldOS initialized\n",
      "  Kernel: <core.kernel.Kernel object at 0x7f229c3fd810>\n",
      "  Store: <core.manifold_os.PersistentStore object at 0x7f229c502a50>\n"
     ]
    }
   ],
   "source": [
    "# Create temp directory for storage\n",
    "temp_dir = tempfile.mkdtemp(prefix=\"mos_test_\")\n",
    "storage_path = Path(temp_dir)\n",
    "db_path = storage_path / \"metadata.duckdb\"\n",
    "\n",
    "print(f\"Storage path: {storage_path}\")\n",
    "print(f\"DuckDB path: {db_path}\")\n",
    "\n",
    "# Initialize ManifoldOS with DuckDB extension\n",
    "mos = ManifoldOS(\n",
    "    storage_path=storage_path,\n",
    "    extensions={\n",
    "        'storage': {\n",
    "            'type': 'duckdb',\n",
    "            'db_path': str(db_path)\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ ManifoldOS initialized\")\n",
    "print(f\"  Kernel: {mos.kernel}\")\n",
    "print(f\"  Store: {mos.store}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7e41b9",
   "metadata": {},
   "source": [
    "## 5. Data Ingestion via ManifoldOS\n",
    "\n",
    "Ingest synthetic data using the n-token algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72ea51f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting: 'pending customer validates invoice'\n",
      "  ✓ LUT committed: n=1, hash=f72ee3615f29dac0..., id=4\n",
      "  ✓ LUT committed: n=2, hash=4007656e8395f470..., id=3\n",
      "  ✓ LUT committed: n=3, hash=58b6804622e07ece..., id=2\n",
      "\n",
      "✓ Ingestion successful\n",
      "  Original tokens: ['pending', 'customer', 'validates', 'invoice']\n",
      "  N-token groups: [1, 2, 3]\n",
      "  HLLSets created: [1, 2, 3]\n",
      "    n=1: card=4.0, hash=f72ee3615f29dac0...\n",
      "    n=2: card=5.0, hash=4007656e8395f470...\n",
      "    n=3: card=4.0, hash=58b6804622e07ece...\n"
     ]
    }
   ],
   "source": [
    "# Ingest single sentence\n",
    "test_sentence = test_sentences[0]\n",
    "print(f\"Ingesting: '{test_sentence}'\")\n",
    "\n",
    "representation = mos.ingest(\n",
    "    raw_data=test_sentence,\n",
    "    metadata={'source': 'synthetic', 'test_id': 1}\n",
    ")\n",
    "\n",
    "if representation:\n",
    "    print(f\"\\n✓ Ingestion successful\")\n",
    "    print(f\"  Original tokens: {representation.original_tokens}\")\n",
    "    print(f\"  N-token groups: {list(representation.n_token_groups.keys())}\")\n",
    "    print(f\"  HLLSets created: {list(representation.hllsets.keys())}\")\n",
    "    \n",
    "    for n, hll in representation.hllsets.items():\n",
    "        print(f\"    n={n}: card={hll.cardinality():.1f}, hash={hll.name[:16]}...\")\n",
    "else:\n",
    "    print(\"✗ Ingestion failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0ded571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting batch of 50 sentences...\n",
      "  ✓ LUT committed: n=1, hash=f72ee3615f29dac0..., id=4\n",
      "  ✓ LUT committed: n=2, hash=4007656e8395f470..., id=3\n",
      "  ✓ LUT committed: n=3, hash=58b6804622e07ece..., id=2\n",
      "  ✓ LUT committed: n=1, hash=b0110e06f2357894..., id=4\n",
      "  ✓ LUT committed: n=2, hash=a6ee2563047e734b..., id=3\n",
      "  ✓ LUT committed: n=3, hash=a7cba9403ce6e239..., id=2\n",
      "  ✓ LUT committed: n=1, hash=2e326b8af5cad4b9..., id=4\n",
      "  ✓ LUT committed: n=2, hash=acf70d64b0e98ad5..., id=3\n",
      "  ✓ LUT committed: n=3, hash=151580b440b2817d..., id=2\n",
      "  ✓ LUT committed: n=1, hash=60b799e35b6e5cae..., id=4\n",
      "  ✓ LUT committed: n=2, hash=d3e96dd5c06ce4ab..., id=3\n",
      "  ✓ LUT committed: n=3, hash=db25275426ce65ce..., id=2\n",
      "  ✓ LUT committed: n=1, hash=1fc20e542e54eff8..., id=3\n",
      "  ✓ LUT committed: n=2, hash=1c0bc6c10dd178e7..., id=3\n",
      "  ✓ LUT committed: n=3, hash=f25a78670b0c1d1c..., id=2\n",
      "  ✓ LUT committed: n=1, hash=13b504b881dd9529..., id=4\n",
      "  ✓ LUT committed: n=2, hash=629d5c6d157b935e..., id=3\n",
      "  ✓ LUT committed: n=3, hash=80391783831c5b95..., id=2\n",
      "  ✓ LUT committed: n=1, hash=3960eccd2ed0bdae..., id=4\n",
      "  ✓ LUT committed: n=2, hash=6884ab88de71cbd9..., id=3\n",
      "  ✓ LUT committed: n=3, hash=2c358bf1359cb96f..., id=2\n",
      "  ✓ LUT committed: n=1, hash=16ea23a959f170c4..., id=4\n",
      "  ✓ LUT committed: n=2, hash=90c599ada459c74b..., id=3\n",
      "  ✓ LUT committed: n=3, hash=ac03ae5e73ca2600..., id=2\n",
      "  ✓ LUT committed: n=1, hash=0802290d3961551e..., id=4\n",
      "  ✓ LUT committed: n=2, hash=f68f2407ae9d3a5f..., id=3\n",
      "  ✓ LUT committed: n=3, hash=17543d9f025e9857..., id=2\n",
      "  ✓ LUT committed: n=1, hash=dda9b6c346f0cd2a..., id=4\n",
      "  ✓ LUT committed: n=2, hash=35f6aa0353ab3453..., id=3\n",
      "  ✓ LUT committed: n=3, hash=c7576b56a3e3bae2..., id=2\n",
      "\n",
      "✓ Batch ingestion complete: 10 successful\n"
     ]
    }
   ],
   "source": [
    "# Ingest batch of sentences\n",
    "print(f\"Ingesting batch of {len(test_sentences)} sentences...\")\n",
    "\n",
    "batch_results = []\n",
    "for i, sentence in enumerate(test_sentences[:10]):  # First 10 for demo\n",
    "    rep = mos.ingest(\n",
    "        raw_data=sentence,\n",
    "        metadata={'source': 'synthetic', 'batch_idx': i}\n",
    "    )\n",
    "    if rep:\n",
    "        batch_results.append(rep)\n",
    "\n",
    "print(f\"\\n✓ Batch ingestion complete: {len(batch_results)} successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b85566b",
   "metadata": {},
   "source": [
    "## 6. Query Persistent Storage\n",
    "\n",
    "Query the DuckDB store for metadata and tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80629efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LUT store available\n"
     ]
    }
   ],
   "source": [
    "# Check if LUT store is available\n",
    "if hasattr(mos, 'lut_store') and mos.lut_store:\n",
    "    print(\"✓ LUT store available\")\n",
    "    \n",
    "    # Query stored HLLSets\n",
    "    if hasattr(mos.lut_store, 'list_hllsets'):\n",
    "        hllsets = mos.lut_store.list_hllsets()\n",
    "        print(f\"\\nStored HLLSets: {len(hllsets)}\")\n",
    "        for h in hllsets[:5]:\n",
    "            print(f\"  • {h}\")\n",
    "else:\n",
    "    print(\"LUT store not configured - checking direct DuckDB access\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a64c462",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionException",
     "evalue": "Connection Error: Can't open a connection to same database file with a different configuration than existing connections",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionException\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mduckdb\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m db_path.exists():\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     conn = \u001b[43mduckdb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdb_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# List tables\u001b[39;00m\n\u001b[32m      8\u001b[39m     tables = conn.execute(\u001b[33m\"\u001b[39m\u001b[33mSHOW TABLES\u001b[39m\u001b[33m\"\u001b[39m).fetchall()\n",
      "\u001b[31mConnectionException\u001b[39m: Connection Error: Can't open a connection to same database file with a different configuration than existing connections"
     ]
    }
   ],
   "source": [
    "# Direct DuckDB query to verify data\n",
    "import duckdb\n",
    "\n",
    "if db_path.exists():\n",
    "    conn = duckdb.connect(str(db_path), read_only=True)\n",
    "    \n",
    "    # List tables\n",
    "    tables = conn.execute(\"SHOW TABLES\").fetchall()\n",
    "    print(f\"Tables in DuckDB: {[t[0] for t in tables]}\")\n",
    "    \n",
    "    # Query each table\n",
    "    for table_name, in tables:\n",
    "        count = conn.execute(f\"SELECT COUNT(*) FROM {table_name}\").fetchone()[0]\n",
    "        print(f\"\\n{table_name}: {count} rows\")\n",
    "        \n",
    "        # Sample data\n",
    "        sample = conn.execute(f\"SELECT * FROM {table_name} LIMIT 3\").fetchall()\n",
    "        if sample:\n",
    "            columns = [desc[0] for desc in conn.description]\n",
    "            print(f\"  Columns: {columns}\")\n",
    "            for row in sample:\n",
    "                print(f\"  {row}\")\n",
    "    \n",
    "    conn.close()\n",
    "else:\n",
    "    print(f\"DuckDB file not found at {db_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771db764",
   "metadata": {},
   "source": [
    "## 7. Store Artifacts Directly\n",
    "\n",
    "Test the artifact storage API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7814d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store raw artifact\n",
    "test_data = b\"This is test artifact data for fractal manifold\"\n",
    "artifact_id = mos.store_artifact(\n",
    "    data=test_data,\n",
    "    metadata={'type': 'test', 'description': 'Test artifact'}\n",
    ")\n",
    "\n",
    "print(f\"Artifact stored:\")\n",
    "print(f\"  ID: {artifact_id}\")\n",
    "print(f\"  Size: {len(test_data)} bytes\")\n",
    "\n",
    "# Retrieve and verify\n",
    "retrieved = mos.retrieve_artifact(artifact_id)\n",
    "print(f\"\\nRetrieved: {retrieved == test_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285c9cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store HLLSet as artifact\n",
    "hll_test = HLLSet.from_batch(['artifact', 'test', 'hllset', 'storage'])\n",
    "hll_bytes = hll_test.dump_roaring()\n",
    "\n",
    "hll_artifact_id = mos.store_artifact(\n",
    "    data=hll_bytes,\n",
    "    metadata={\n",
    "        'type': 'hllset',\n",
    "        'hll_name': hll_test.name,\n",
    "        'cardinality': hll_test.cardinality()\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"HLLSet stored as artifact:\")\n",
    "print(f\"  Artifact ID: {hll_artifact_id}\")\n",
    "print(f\"  HLLSet name: {hll_test.name[:16]}...\")\n",
    "print(f\"  Cardinality: {hll_test.cardinality():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e18cd6c",
   "metadata": {},
   "source": [
    "## 8. Create Entanglements\n",
    "\n",
    "Test the entanglement relationship API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60734349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create entanglement between artifacts\n",
    "mos.create_entanglement(\n",
    "    source_id=artifact_id,\n",
    "    target_id=hll_artifact_id,\n",
    "    strength=0.85\n",
    ")\n",
    "\n",
    "print(f\"Created entanglement: {artifact_id[:16]}... ↔ {hll_artifact_id[:16]}...\")\n",
    "\n",
    "# Query entanglements\n",
    "entanglements = mos.get_entanglements(artifact_id)\n",
    "print(f\"\\nEntanglements for {artifact_id[:16]}...:\")\n",
    "for target, strength in entanglements.items():\n",
    "    print(f\"  → {target[:16]}... (strength={strength})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fe3823",
   "metadata": {},
   "source": [
    "## 9. Complete Pipeline Test\n",
    "\n",
    "Run complete pipeline: synthetic data → kernel → HLLSet → MOS → DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0bd088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fresh dataset\n",
    "pipeline_data = generate_dataset(20)\n",
    "print(f\"Pipeline test with {len(pipeline_data)} sentences\\n\")\n",
    "\n",
    "# Track metrics\n",
    "start_time = time.time()\n",
    "total_tokens = 0\n",
    "hllsets_created = []\n",
    "\n",
    "for i, sentence in enumerate(pipeline_data):\n",
    "    # Ingest\n",
    "    rep = mos.ingest(sentence, metadata={'pipeline_test': True, 'idx': i})\n",
    "    \n",
    "    if rep:\n",
    "        total_tokens += len(rep.original_tokens)\n",
    "        for n, hll in rep.hllsets.items():\n",
    "            hllsets_created.append((n, hll.name, hll.cardinality()))\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"Pipeline completed in {elapsed:.2f}s\")\n",
    "print(f\"  Sentences processed: {len(pipeline_data)}\")\n",
    "print(f\"  Total tokens: {total_tokens}\")\n",
    "print(f\"  HLLSets created: {len(hllsets_created)}\")\n",
    "print(f\"  Throughput: {total_tokens/elapsed:.1f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f86722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final DuckDB state\n",
    "if db_path.exists():\n",
    "    conn = duckdb.connect(str(db_path), read_only=True)\n",
    "    \n",
    "    print(\"Final DuckDB state:\")\n",
    "    tables = conn.execute(\"SHOW TABLES\").fetchall()\n",
    "    \n",
    "    for table_name, in tables:\n",
    "        count = conn.execute(f\"SELECT COUNT(*) FROM {table_name}\").fetchone()[0]\n",
    "        print(f\"  {table_name}: {count} rows\")\n",
    "    \n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947fde93",
   "metadata": {},
   "source": [
    "## 10. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7e8e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup temp directory (optional - uncomment to remove)\n",
    "# import shutil\n",
    "# shutil.rmtree(temp_dir)\n",
    "# print(f\"Cleaned up: {temp_dir}\")\n",
    "\n",
    "print(f\"\\nTest data preserved at: {temp_dir}\")\n",
    "print(\"Uncomment cleanup cell to remove.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3f6bc7",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Tested components:**\n",
    "- ✅ Kernel: Token absorption, union, intersection, Jaccard similarity\n",
    "- ✅ HLLSet: Content-addressed naming, cardinality estimation\n",
    "- ✅ Fractal Core: N-tokenization, overlap measure, scale tower\n",
    "- ✅ ManifoldOS: Initialization, ingestion, artifact storage\n",
    "- ✅ DuckDB: Persistent LUT storage, metadata queries\n",
    "- ✅ Entanglement: Artifact relationships\n",
    "\n",
    "**Next steps for Stage 2:**\n",
    "- Integrate fractal_core with HRT lattice construction\n",
    "- Implement edge→token loop for full fractal recursion\n",
    "- Add entanglement number computation across scales"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hllset-manifold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
