{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10525088",
   "metadata": {},
   "source": [
    "# Uninflected Sign Systems: Vocabulary-Based Identification\n",
    "\n",
    "This notebook demonstrates **VocabularyIdentifierScheme** for fixed sign systems.\n",
    "\n",
    "## Inflected vs Uninflected Sign Systems\n",
    "\n",
    "| Inflected (English, Russian) | Uninflected (Chinese, Music, Emoji) |\n",
    "|------------------------------|-------------------------------------|\n",
    "| Open vocabulary (infinite forms) | Fixed vocabulary (finite signs) |\n",
    "| Words change form (`runâ†’running`) | Signs are atomic, immutable |\n",
    "| Typos create different hashes | Each sign is unambiguous |\n",
    "| Hash-based identification | Direct vocabulary lookup |\n",
    "\n",
    "**Examples of uninflected sign systems:**\n",
    "- Chinese/Japanese/Korean characters\n",
    "- Musical notation (â™©â™ªâ™«â™¬)\n",
    "- Mathematical symbols\n",
    "- Emoji sequences\n",
    "- Traffic signs\n",
    "\n",
    "## Key Insight: HRT is Universal\n",
    "\n",
    "The **structure** (AM, W, HRT) doesn't change - only the **identification** scheme:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    HashIdentifierScheme (inflected)                     â”‚\n",
    "â”‚               content â†’ SHA1 â†’ (reg, zeros) â†’ index                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                 VocabularyIdentifierScheme (uninflected)                â”‚\n",
    "â”‚               sign â†’ vocabulary lookup â†’ index                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                          AM / W / HRT                                   â”‚\n",
    "â”‚               (don't care HOW indices are computed)                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbeb888",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f74ff38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/alexmy/SGS/SGS_lib/fractal_manifold/fractal_manifold\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure core is importable\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f6fbfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Core modules imported\n"
     ]
    }
   ],
   "source": [
    "# Import core modules\n",
    "from core.mf_algebra import (\n",
    "    HashIdentifierScheme,\n",
    "    VocabularyIdentifierScheme,\n",
    "    LookupTable,\n",
    "    START, END,\n",
    "    build_w_from_am,\n",
    "    LayerHLLSets,\n",
    "    cascading_disambiguate,\n",
    ")\n",
    "from core.sparse_hrt_3d import Sparse3DConfig, SparseAM3D, Edge3D\n",
    "\n",
    "# Reload sign_tokenizer to pick up changes\n",
    "import importlib\n",
    "import core.sign_tokenizer\n",
    "importlib.reload(core.sign_tokenizer)\n",
    "\n",
    "# Import sign tokenizer (NEW - from module)\n",
    "from core.sign_tokenizer import (\n",
    "    tokenize_signs,\n",
    "    generate_sign_ngrams,\n",
    "    build_cover_from_w,\n",
    "    query_signs,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Core modules imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a2f17b",
   "metadata": {},
   "source": [
    "## 2. Compare Hash vs Vocabulary Schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d89883a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash-based indices (English):\n",
      "  'neural' â†’ reg=279, zeros=3 â†’ idx=6420\n",
      "  'network' â†’ reg=129, zeros=1 â†’ idx=2968\n",
      "  'learning' â†’ reg=878, zeros=1 â†’ idx=20195\n",
      "  'deep' â†’ reg=511, zeros=1 â†’ idx=11754\n",
      "\n",
      "Index range: 0 to 23552\n"
     ]
    }
   ],
   "source": [
    "# Hash-based scheme (default for English)\n",
    "hash_scheme = HashIdentifierScheme(p_bits=10, h_bits=32)\n",
    "\n",
    "# Test with English words\n",
    "english_words = [\"neural\", \"network\", \"learning\", \"deep\"]\n",
    "print(\"Hash-based indices (English):\")\n",
    "for word in english_words:\n",
    "    idx = hash_scheme.to_index(word)\n",
    "    reg, zeros = hash_scheme.to_reg_zeros(word)\n",
    "    print(f\"  '{word}' â†’ reg={reg}, zeros={zeros} â†’ idx={idx}\")\n",
    "\n",
    "print(f\"\\nIndex range: 0 to {hash_scheme.index_range()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ad7b970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary-based indices (Chinese):\n",
      "  'ä½ ' â†’ idx=1\n",
      "  'å¥½' â†’ idx=2\n",
      "  'æˆ‘' â†’ idx=3\n",
      "  'æ˜¯' â†’ idx=4\n",
      "  'äºº' â†’ idx=5\n",
      "  'æœº' â†’ idx=6\n",
      "  'å™¨' â†’ idx=7\n",
      "  'å­¦' â†’ idx=8\n",
      "  'ä¹ ' â†’ idx=9\n",
      "  'ç¥' â†’ idx=10\n",
      "\n",
      "Vocabulary size: 20\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary-based scheme for Chinese\n",
    "# Build a sample vocabulary\n",
    "chinese_chars = [\n",
    "    \"ä½ \", \"å¥½\", \"æˆ‘\", \"æ˜¯\", \"äºº\",  # Common characters\n",
    "    \"æœº\", \"å™¨\", \"å­¦\", \"ä¹ \",        # Machine learning\n",
    "    \"ç¥\", \"ç»\", \"ç½‘\", \"ç»œ\",        # Neural network\n",
    "    \"æ·±\", \"åº¦\",                    # Deep\n",
    "    \"æ•°\", \"æ®\", \"å¤„\", \"ç†\",        # Data processing\n",
    "]\n",
    "\n",
    "vocab_scheme = VocabularyIdentifierScheme()\n",
    "for char in chinese_chars:\n",
    "    vocab_scheme.add_sign(char)\n",
    "\n",
    "print(\"Vocabulary-based indices (Chinese):\")\n",
    "for char in chinese_chars[:10]:\n",
    "    idx = vocab_scheme.to_index(char)\n",
    "    print(f\"  '{char}' â†’ idx={idx}\")\n",
    "\n",
    "print(f\"\\nVocabulary size: {vocab_scheme.index_range()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5733e4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash scheme - same content ALWAYS same index:\n",
      "  'neural' â†’ 6420\n",
      "  'neural' â†’ 6420  (same)\n",
      "  'Neural' â†’ 7475  (different! case sensitive)\n",
      "\n",
      "Vocabulary scheme - direct lookup:\n",
      "  'ç¥' â†’ 10\n",
      "  'ç¥' â†’ 10  (same)\n",
      "  'æœªçŸ¥' â†’ -1  (unknown = -1)\n"
     ]
    }
   ],
   "source": [
    "# Key difference: Hash collisions vs direct lookup\n",
    "print(\"Hash scheme - same content ALWAYS same index:\")\n",
    "print(f\"  'neural' â†’ {hash_scheme.to_index('neural')}\")\n",
    "print(f\"  'neural' â†’ {hash_scheme.to_index('neural')}  (same)\")\n",
    "print(f\"  'Neural' â†’ {hash_scheme.to_index('Neural')}  (different! case sensitive)\")\n",
    "\n",
    "print(\"\\nVocabulary scheme - direct lookup:\")\n",
    "print(f\"  'ç¥' â†’ {vocab_scheme.to_index('ç¥')}\")\n",
    "print(f\"  'ç¥' â†’ {vocab_scheme.to_index('ç¥')}  (same)\")\n",
    "print(f\"  'æœªçŸ¥' â†’ {vocab_scheme.to_index('æœªçŸ¥')}  (unknown = -1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb64448",
   "metadata": {},
   "source": [
    "## 3. Create LookupTable with Vocabulary Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84f3e953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Chinese n-grams to LUT:\n",
      "  'æœº' (n=1) â†’ idx=6\n",
      "  'å™¨' (n=1) â†’ idx=7\n",
      "  'å­¦' (n=1) â†’ idx=8\n",
      "  'ä¹ ' (n=1) â†’ idx=9\n",
      "  'æœºå™¨' (n=2) â†’ idx=138\n",
      "  'å­¦ä¹ ' (n=2) â†’ idx=202\n",
      "  'æœºå™¨å­¦' (n=3) â†’ idx=5739\n",
      "\n",
      "LUT entries: 7\n"
     ]
    }
   ],
   "source": [
    "# Create config and LUT with vocabulary scheme\n",
    "config = Sparse3DConfig(p_bits=10, max_n=3)\n",
    "\n",
    "# LUT with vocabulary-based identification\n",
    "lut_vocab = LookupTable(config, identifier_scheme=vocab_scheme)\n",
    "\n",
    "# Add some Chinese n-grams\n",
    "chinese_ngrams = [\n",
    "    (\"æœº\",),              # 1-gram: machine\n",
    "    (\"å™¨\",),              # 1-gram: device  \n",
    "    (\"å­¦\",),              # 1-gram: learn\n",
    "    (\"ä¹ \",),              # 1-gram: practice\n",
    "    (\"æœº\", \"å™¨\"),         # 2-gram: machine\n",
    "    (\"å­¦\", \"ä¹ \"),         # 2-gram: learning\n",
    "    (\"æœº\", \"å™¨\", \"å­¦\"),   # 3-gram: machine learn\n",
    "]\n",
    "\n",
    "print(\"Adding Chinese n-grams to LUT:\")\n",
    "for ngram in chinese_ngrams:\n",
    "    idx = lut_vocab.add_ntoken(ngram)\n",
    "    display = ''.join(ngram)\n",
    "    print(f\"  '{display}' (n={len(ngram)}) â†’ idx={idx}\")\n",
    "\n",
    "print(f\"\\nLUT entries: {len(lut_vocab.ntoken_to_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69d5fad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same n-grams with hash-based LUT:\n",
      "  'æœº' (n=1) â†’ idx=3037\n",
      "  'å™¨' (n=1) â†’ idx=2004\n",
      "  'å­¦' (n=1) â†’ idx=17296\n",
      "  'ä¹ ' (n=1) â†’ idx=5060\n",
      "  'æœºå™¨' (n=2) â†’ idx=10557\n",
      "  'å­¦ä¹ ' (n=2) â†’ idx=8695\n",
      "  'æœºå™¨å­¦' (n=3) â†’ idx=4077\n",
      "\n",
      "Note: Hash indices are much larger (sparse)\n",
      "Vocabulary indices are compact (dense)\n"
     ]
    }
   ],
   "source": [
    "# Compare with hash-based LUT\n",
    "lut_hash = LookupTable(config)  # Uses default hash scheme\n",
    "\n",
    "print(\"Same n-grams with hash-based LUT:\")\n",
    "for ngram in chinese_ngrams:\n",
    "    idx = lut_hash.add_ntoken(ngram)\n",
    "    display = ''.join(ngram)\n",
    "    print(f\"  '{display}' (n={len(ngram)}) â†’ idx={idx}\")\n",
    "\n",
    "print(\"\\nNote: Hash indices are much larger (sparse)\")\n",
    "print(\"Vocabulary indices are compact (dense)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b247077",
   "metadata": {},
   "source": [
    "## 4. Sign Tokenizer (Using Module)\n",
    "\n",
    "Uninflected sign systems (Chinese, musical notation, emoji, etc.) don't use spaces - each character/symbol is a semantic unit.\n",
    "The `tokenize_signs()` and `generate_sign_ngrams()` functions handle this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8748ac48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: æœºå™¨å­¦ä¹ å¾ˆæœ‰è¶£\n",
      "Characters: ['æœº', 'å™¨', 'å­¦', 'ä¹ ', 'å¾ˆ', 'æœ‰', 'è¶£']\n",
      "\n",
      "N-grams (20 total):\n",
      "  ('<START>',)\n",
      "  æœº (n=1)\n",
      "  æœºå™¨ (n=2)\n",
      "  æœºå™¨å­¦ (n=3)\n",
      "  å™¨ (n=1)\n",
      "  å™¨å­¦ (n=2)\n",
      "  å™¨å­¦ä¹  (n=3)\n",
      "  å­¦ (n=1)\n",
      "  å­¦ä¹  (n=2)\n",
      "  å­¦ä¹ å¾ˆ (n=3)\n",
      "  ä¹  (n=1)\n",
      "  ä¹ å¾ˆ (n=2)\n",
      "  ä¹ å¾ˆæœ‰ (n=3)\n",
      "  å¾ˆ (n=1)\n",
      "  å¾ˆæœ‰ (n=2)\n",
      "  å¾ˆæœ‰è¶£ (n=3)\n",
      "  æœ‰ (n=1)\n",
      "  æœ‰è¶£ (n=2)\n",
      "  è¶£ (n=1)\n",
      "  ('<END>',)\n"
     ]
    }
   ],
   "source": [
    "# Tokenization uses module functions (not inline definitions)\n",
    "# tokenize_signs() and generate_sign_ngrams() are imported from core.sign_tokenizer\n",
    "\n",
    "# Test with Chinese sentence\n",
    "chinese_text = \"æœºå™¨å­¦ä¹ å¾ˆæœ‰è¶£\"\n",
    "chars = tokenize_signs(chinese_text)\n",
    "ngrams = generate_sign_ngrams(chars, max_n=3)\n",
    "\n",
    "print(f\"Text: {chinese_text}\")\n",
    "print(f\"Characters: {chars}\")\n",
    "print(f\"\\nN-grams ({len(ngrams)} total):\")\n",
    "for ng in ngrams:\n",
    "    if ng in (START, END):\n",
    "        print(f\"  {ng}\")\n",
    "    else:\n",
    "        print(f\"  {''.join(ng)} (n={len(ng)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e34e54",
   "metadata": {},
   "source": [
    "## 5. Build Chinese Vocabulary from Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37dad501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique signs in corpus: 36\n",
      "Signs: ä¸–ä¹ äººä»¥ä½¿åˆ†å˜å¯å™¨åŸºå¤å­¦å·¥åº¦å¼æ®æ”¯æ”¹æ•°æ˜¯æ™ºæœºæ‚æ¨¡æ·±ç”¨ç•Œçš„ç¡€ç¥ç»ç»œç½‘èƒ½è¦é‡\n"
     ]
    }
   ],
   "source": [
    "# Sample corpus - Chinese is just one example of uninflected sign systems\n",
    "# (Also works with: musical notation, emoji, mathematical symbols, etc.)\n",
    "chinese_corpus = [\n",
    "    \"æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯\",\n",
    "    \"æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ\",\n",
    "    \"æ•°æ®æ˜¯æœºå™¨å­¦ä¹ çš„åŸºç¡€\",\n",
    "    \"ç¥ç»ç½‘ç»œå¯ä»¥å­¦ä¹ å¤æ‚æ¨¡å¼\",\n",
    "    \"äººå·¥æ™ºèƒ½æ”¹å˜ä¸–ç•Œ\",\n",
    "]\n",
    "\n",
    "# Build vocabulary from corpus using module functions\n",
    "all_chars = set()\n",
    "for text in chinese_corpus:\n",
    "    chars = tokenize_signs(text)  # Using module function\n",
    "    all_chars.update(chars)\n",
    "\n",
    "print(f\"Unique signs in corpus: {len(all_chars)}\")\n",
    "print(f\"Signs: {''.join(sorted(all_chars))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87c86111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 37\n",
      "\n",
      "Sample mappings:\n",
      "  'èƒ½' â†’ 34\n",
      "  'ç•Œ' â†’ 27\n",
      "  'é‡' â†’ 36\n",
      "  'çš„' â†’ 28\n",
      "  'æ®' â†’ 16\n",
      "  'å¤' â†’ 11\n",
      "  'åº¦' â†’ 14\n",
      "  'æ¨¡' â†’ 24\n",
      "  'æ˜¯' â†’ 20\n",
      "  'ç¡€' â†’ 29\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary scheme from corpus\n",
    "corpus_vocab = VocabularyIdentifierScheme()\n",
    "for char in sorted(all_chars):\n",
    "    corpus_vocab.add_sign(char)\n",
    "\n",
    "print(f\"Vocabulary size: {corpus_vocab.index_range()}\")\n",
    "print(\"\\nSample mappings:\")\n",
    "for char in list(all_chars)[:10]:\n",
    "    print(f\"  '{char}' â†’ {corpus_vocab.to_index(char)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d395e9b9",
   "metadata": {},
   "source": [
    "## 6. ManifoldOS with Vocabulary Scheme\n",
    "\n",
    "Now let's integrate the vocabulary scheme with ManifoldOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fc0e11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 37 characters\n",
      "LUT ready with vocabulary scheme\n"
     ]
    }
   ],
   "source": [
    "# Create config\n",
    "config = Sparse3DConfig(p_bits=10, max_n=3)\n",
    "\n",
    "# Build vocabulary from corpus\n",
    "vocab = VocabularyIdentifierScheme()\n",
    "for text in chinese_corpus:\n",
    "    for char in tokenize_signs(text):  # Using module function\n",
    "        vocab.add_sign(char)\n",
    "\n",
    "# Create LUT with vocabulary scheme\n",
    "lut = LookupTable(config, identifier_scheme=vocab)\n",
    "\n",
    "print(f\"Vocabulary: {vocab.index_range()} characters\")\n",
    "print(f\"LUT ready with vocabulary scheme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe8c7a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: æ·±åº¦å­¦ä¹ \n",
      "\n",
      "N-grams with vocabulary indices:\n",
      "  '('<START>',)' â†’ idx=-1\n",
      "  'æ·±' â†’ idx=15\n",
      "  'æ·±åº¦' â†’ idx=586\n",
      "  'æ·±åº¦å­¦' â†’ idx=132984\n",
      "  'åº¦' â†’ idx=16\n",
      "  'åº¦å­¦' â†’ idx=267\n",
      "  'åº¦å­¦ä¹ ' â†’ idx=19618\n",
      "  'å­¦' â†’ idx=3\n",
      "  'å­¦ä¹ ' â†’ idx=106\n",
      "  'ä¹ ' â†’ idx=4\n",
      "  '('<END>',)' â†’ idx=-1\n",
      "\n",
      "LUT now has 11 entries\n"
     ]
    }
   ],
   "source": [
    "# Process sign text manually to see the indices\n",
    "test_text = \"æ·±åº¦å­¦ä¹ \"\n",
    "chars = tokenize_signs(test_text)  # Using module function\n",
    "ngrams = generate_sign_ngrams(chars, max_n=3)  # Using module function\n",
    "\n",
    "print(f\"Text: {test_text}\")\n",
    "print(f\"\\nN-grams with vocabulary indices:\")\n",
    "for ng in ngrams:\n",
    "    idx = lut.add_ntoken(ng)\n",
    "    if ng in (START, END):\n",
    "        display = str(ng)\n",
    "    else:\n",
    "        display = ''.join(ng)\n",
    "    print(f\"  '{display}' â†’ idx={idx}\")\n",
    "\n",
    "print(f\"\\nLUT now has {len(lut.ntoken_to_index)} entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc9ca46",
   "metadata": {},
   "source": [
    "## 7. Build AM/W with Vocabulary Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64f3514e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total edges: 152\n",
      "LUT entries: 114\n"
     ]
    }
   ],
   "source": [
    "# Create edges from n-gram sequence\n",
    "def create_edges_from_ngrams(ngrams: list, lut: LookupTable) -> list:\n",
    "    \"\"\"Create edges from sequential n-grams.\"\"\"\n",
    "    edges = []\n",
    "    for i in range(len(ngrams) - 1):\n",
    "        row_ng = ngrams[i]\n",
    "        col_ng = ngrams[i + 1]\n",
    "        \n",
    "        row_idx = lut.get_ntoken_index(row_ng)\n",
    "        col_idx = lut.get_ntoken_index(col_ng)\n",
    "        \n",
    "        if row_idx is not None and col_idx is not None:\n",
    "            # Layer is determined by the target n-gram\n",
    "            layer = 0 if col_ng in (START, END) else len(col_ng) - 1\n",
    "            layer = min(layer, config.max_n - 1)\n",
    "            edges.append(Edge3D(n=layer, row=row_idx, col=col_idx, value=1.0))\n",
    "    \n",
    "    return edges\n",
    "\n",
    "# Process all corpus using module functions\n",
    "all_edges = []\n",
    "for text in chinese_corpus:\n",
    "    chars = tokenize_signs(text)  # Module function\n",
    "    ngrams = generate_sign_ngrams(chars, max_n=3)  # Module function\n",
    "    \n",
    "    # Add all n-grams to LUT first\n",
    "    for ng in ngrams:\n",
    "        lut.add_ntoken(ng)\n",
    "    \n",
    "    # Create edges\n",
    "    edges = create_edges_from_ngrams(ngrams, lut)\n",
    "    all_edges.extend(edges)\n",
    "\n",
    "print(f\"Total edges: {len(all_edges)}\")\n",
    "print(f\"LUT entries: {len(lut.ntoken_to_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8467f1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexmy/SGS/SGS_lib/fractal_manifold/fractal_manifold/.venv/lib/python3.13/site-packages/torch/cuda/__init__.py:435: UserWarning: \n",
      "    Found GPU1 Quadro M1200 which is of cuda capability 5.0.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (7.0) - (12.0)\n",
      "    \n",
      "  queued_call()\n",
      "/home/alexmy/SGS/SGS_lib/fractal_manifold/fractal_manifold/.venv/lib/python3.13/site-packages/torch/cuda/__init__.py:435: UserWarning: \n",
      "    Please install PyTorch with a following CUDA\n",
      "    configurations:  12.6 following instructions at\n",
      "    https://pytorch.org/get-started/locally/\n",
      "    \n",
      "  queued_call()\n",
      "/home/alexmy/SGS/SGS_lib/fractal_manifold/fractal_manifold/.venv/lib/python3.13/site-packages/torch/cuda/__init__.py:435: UserWarning: \n",
      "Quadro M1200 with CUDA capability sm_50 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_70 sm_75 sm_80 sm_86 sm_90 sm_100 sm_120.\n",
      "If you want to use the Quadro M1200 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  queued_call()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AM built with vocabulary indices:\n",
      "  Total edges (nnz): 152\n",
      "  Layers: 3\n",
      "  L0: 49 rows, 37 cols\n",
      "  L1: 32 rows, 38 cols\n",
      "  L2: 34 rows, 38 cols\n"
     ]
    }
   ],
   "source": [
    "# Build AM from edges using from_edges (efficient batch construction)\n",
    "am = SparseAM3D.from_edges(config, all_edges)\n",
    "\n",
    "print(f\"AM built with vocabulary indices:\")\n",
    "print(f\"  Total edges (nnz): {am.nnz}\")\n",
    "print(f\"  Layers: {am.config.max_n}\")\n",
    "\n",
    "# Check layer statistics\n",
    "for n in range(config.max_n):\n",
    "    rows, cols = am.layer_active(n)\n",
    "    print(f\"  L{n}: {len(rows)} rows, {len(cols)} cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "294370fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W matrix built:\n",
      "  Total transitions: 129\n",
      "  Source indices: 3\n"
     ]
    }
   ],
   "source": [
    "# Build W transition matrix\n",
    "from core.mf_algebra import build_w_from_am\n",
    "\n",
    "W = build_w_from_am(am, config)\n",
    "\n",
    "# Count transitions\n",
    "total_transitions = sum(\n",
    "    sum(len(targets) for targets in layer.values())\n",
    "    for layer in W.values()\n",
    ")\n",
    "\n",
    "print(f\"W matrix built:\")\n",
    "print(f\"  Total transitions: {total_transitions}\")\n",
    "print(f\"  Source indices: {len(W)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ee38625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample transitions from W:\n"
     ]
    }
   ],
   "source": [
    "# Trace a path through W\n",
    "print(\"Sample transitions from W:\")\n",
    "\n",
    "# Find a starting point\n",
    "start_idx = lut.get_ntoken_index(START)\n",
    "if start_idx and start_idx in W:\n",
    "    print(f\"\\nFrom START ({start_idx}):\")\n",
    "    for layer, targets in W[start_idx].items():\n",
    "        for target_idx, prob in list(targets.items())[:5]:\n",
    "            # Recover n-token from index\n",
    "            ntokens = lut.get_ntokens_at_index(target_idx)\n",
    "            for _, nt in ntokens:\n",
    "                if nt not in (START, END):\n",
    "                    display = ''.join(nt)\n",
    "                    print(f\"  â†’ '{display}' (L{layer}, prob={prob:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b7d25c",
   "metadata": {},
   "source": [
    "## 8. Query with Vocabulary Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bc68d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Corpus Ingested ===\n",
      "AM edges: 152\n",
      "LUT entries: 113\n",
      "W source nodes: 115\n",
      "\n",
      "START index in LUT: -1\n",
      "Layer 0: 49 unique rows, 37 unique cols\n",
      "Layer 1: 32 unique rows, 38 unique cols\n",
      "Layer 2: 34 unique rows, 38 unique cols\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PROPER QUERY WORKFLOW:\n",
    "# 1. Query MUST be ingested first (or use existing corpus indices)\n",
    "# 2. Build query HLLSet from query indices\n",
    "# 3. Build cover HLLSet from W transitions \n",
    "# 4. Disambiguate cover to get relevant indices\n",
    "# 5. LUT lookup to recover tokens\n",
    "# 6. Order by AM layer structure\n",
    "# =============================================================================\n",
    "\n",
    "from core.mf_algebra import cascading_disambiguate, LayerHLLSets\n",
    "\n",
    "# First, let's verify what's in our structures\n",
    "print(\"=== Corpus Ingested ===\")\n",
    "print(f\"AM edges: {am.nnz}\")\n",
    "print(f\"LUT entries: {len(lut.index_to_ntokens)}\")\n",
    "print(f\"W source nodes: {sum(len(layer) for layer in W.values())}\")\n",
    "\n",
    "# Build LayerHLLSets from our AM (needed for disambiguation)\n",
    "layer_hllsets = LayerHLLSets.from_am(am, p_bits=config.p_bits)\n",
    "\n",
    "# Get START index for proper traversal\n",
    "start_idx = lut.get_ntoken_index(START)\n",
    "print(f\"\\nSTART index in LUT: {start_idx}\")\n",
    "\n",
    "# Show layer distribution\n",
    "for n in range(config.max_n):\n",
    "    rows, cols = am.layer_active(n)\n",
    "    print(f\"Layer {n}: {len(rows)} unique rows, {len(cols)} unique cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "164a99b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cover Building Strategies ===\n",
      "  rows_only: 21 indices\n",
      "  cols_only: 25 indices\n",
      "  union: 33 indices\n",
      "  intersection: 13 indices\n",
      "\n",
      "All indices in AM: 113\n",
      "\n",
      "Key insight: INTERSECTION gives smaller, more precise cover!\n",
      "(Indices that are BOTH reachable from AND can reach the query)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Build cover from W using INTERSECTION of row and column covers\n",
    "# Using module functions: build_cover_from_w()\n",
    "# =============================================================================\n",
    "\n",
    "# Test with different strategies (functions imported from core.sign_tokenizer)\n",
    "print(\"=== Cover Building Strategies ===\")\n",
    "test_query = {1, 2, 3, 4}  # æœº, å™¨, å­¦, ä¹ \n",
    "\n",
    "for strategy in ['rows_only', 'cols_only', 'union', 'intersection']:\n",
    "    cover = build_cover_from_w(test_query, W, max_depth=2, strategy=strategy)\n",
    "    print(f\"  {strategy}: {len(cover)} indices\")\n",
    "\n",
    "print(f\"\\nAll indices in AM: {len(am.all_active_rows | am.all_active_cols)}\")\n",
    "print(\"\\nKey insight: INTERSECTION gives smaller, more precise cover!\")\n",
    "print(\"(Indices that are BOTH reachable from AND can reach the query)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2c5d556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing query_signs() from module:\n",
      "Query: æœºå™¨å­¦ä¹  (strategy=intersection)\n",
      "  Signs: 4, Indices: 9 (from 9 n-grams)\n",
      "  Cover: rows=21, cols=25, intersection=13, using=13\n",
      "  Results: 10 tokens\n",
      "    L2: ['å™¨å­¦ä¹ ', 'å­¦ä¹ æ˜¯', 'å­¦ä¹ çš„']...\n",
      "    L1: ['æœºå™¨']\n",
      "    L0: ['æœº', 'å™¨', 'å­¦']...\n",
      "\n",
      "Returned 10 results\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPLETE QUERY PIPELINE (Using module function: query_signs())\n",
    "# =============================================================================\n",
    "\n",
    "# The query_signs() function from core.sign_tokenizer implements:\n",
    "# 1. Tokenize query\n",
    "# 2. Generate ALL n-grams (1-gram, 2-gram, 3-gram) from query\n",
    "# 3. Look up indices for ALL n-grams\n",
    "# 4. Build cover from W using row/col INTERSECTION\n",
    "# 5. Disambiguate cover\n",
    "# 6. Recover tokens and order by layer\n",
    "\n",
    "print(\"Testing query_signs() from module:\")\n",
    "results = query_signs(\n",
    "    query_text=\"æœºå™¨å­¦ä¹ \",\n",
    "    lut=lut,\n",
    "    am=am,\n",
    "    W=W,\n",
    "    layer_hllsets=layer_hllsets,\n",
    "    top_k=10,\n",
    "    cover_strategy='intersection',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nReturned {len(results)} results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e593e5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPARISON: Intersection vs Union Cover Strategies\n",
      "======================================================================\n",
      "\n",
      "Query: æœºå™¨å­¦ä¹  with INTERSECTION (precise)\n",
      "Query: æœºå™¨å­¦ä¹  (strategy=intersection)\n",
      "  Signs: 4, Indices: 9 (from 9 n-grams)\n",
      "  Cover: rows=21, cols=25, intersection=13, using=13\n",
      "  Results: 10 tokens\n",
      "    L2: ['å™¨å­¦ä¹ ', 'å­¦ä¹ æ˜¯', 'å­¦ä¹ çš„']...\n",
      "    L1: ['æœºå™¨']\n",
      "    L0: ['æœº', 'å™¨', 'å­¦']...\n",
      "\n",
      "======================================================================\n",
      "Query: æœºå™¨å­¦ä¹  with UNION (broad)\n",
      "Query: æœºå™¨å­¦ä¹  (strategy=union)\n",
      "  Signs: 4, Indices: 9 (from 9 n-grams)\n",
      "  Cover: rows=21, cols=25, intersection=13, using=33\n",
      "  Results: 22 tokens\n",
      "    L2: ['æ˜¯æœºå™¨', 'å­¦ä¹ æ˜¯', 'å­¦ä¹ çš„']...\n",
      "    L1: ['æœºå™¨', 'å™¨å­¦', 'åº¦å­¦']...\n",
      "    L0: ['æœº', 'å™¨', 'å­¦']...\n",
      "\n",
      "======================================================================\n",
      "SUMMARY:\n",
      "  Intersection: 10 results (more precise)\n",
      "  Union: 10 results (broader recall)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPARISON: Intersection vs Union Cover Strategies\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPARISON: Intersection vs Union Cover Strategies\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "test_query = \"æœºå™¨å­¦ä¹ \"\n",
    "\n",
    "print(f\"\\nQuery: {test_query} with INTERSECTION (precise)\")\n",
    "result_int = query_signs(test_query, lut, am, W, layer_hllsets, \n",
    "                         top_k=10, cover_strategy='intersection', verbose=True)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Query: {test_query} with UNION (broad)\")\n",
    "result_uni = query_signs(test_query, lut, am, W, layer_hllsets, \n",
    "                         top_k=10, cover_strategy='union', verbose=True)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SUMMARY:\")\n",
    "print(f\"  Intersection: {len(result_int)} results (more precise)\")\n",
    "print(f\"  Union: {len(result_uni)} results (broader recall)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfeaedd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 36 signs\n",
      "Sample: [('æœº', 1), ('å™¨', 2), ('å­¦', 3), ('ä¹ ', 4), ('æ˜¯', 5)]\n",
      "LUT: 113 unique indices\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# REBUILD CLEAN: LUT, AM, W with ordered vocabulary\n",
    "# =============================================================================\n",
    "\n",
    "# Build vocabulary in corpus order (deterministic)\n",
    "chars_in_order = []\n",
    "for text in chinese_corpus:\n",
    "    for char in tokenize_signs(text):\n",
    "        if char not in chars_in_order:\n",
    "            chars_in_order.append(char)\n",
    "\n",
    "vocab_dict = {char: idx + 1 for idx, char in enumerate(chars_in_order)}\n",
    "corpus_vocab = VocabularyIdentifierScheme(vocabulary=vocab_dict)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab_dict)} signs\")\n",
    "print(f\"Sample: {list(vocab_dict.items())[:5]}\")\n",
    "\n",
    "# Rebuild LUT\n",
    "new_config = Sparse3DConfig(p_bits=10, max_n=3, h_bits=32)\n",
    "lut = LookupTable(config=new_config, identifier_scheme=corpus_vocab)\n",
    "\n",
    "# Add all n-grams\n",
    "for text in chinese_corpus:\n",
    "    chars = tokenize_signs(text)\n",
    "    ngrams = generate_sign_ngrams(chars)\n",
    "    for ng in ngrams:\n",
    "        lut.add_ntoken(ng)\n",
    "\n",
    "print(f\"LUT: {len(lut.index_to_ntokens)} unique indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "785ec523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AM: 137 edges\n",
      "  L0: 33 rows, 36 cols\n",
      "  L1: 34 rows, 36 cols\n",
      "  L2: 34 rows, 36 cols\n"
     ]
    }
   ],
   "source": [
    "# Build edges between consecutive n-grams\n",
    "all_edges = []\n",
    "\n",
    "for text in chinese_corpus:\n",
    "    chars = tokenize_signs(text)\n",
    "    ngrams = generate_sign_ngrams(chars)\n",
    "    \n",
    "    prev_by_layer = {0: None, 1: None, 2: None}\n",
    "    for ng in ngrams:\n",
    "        if ng in (START, END):\n",
    "            continue\n",
    "        layer = len(ng) - 1\n",
    "        idx = lut.get_ntoken_index(ng)\n",
    "        \n",
    "        if prev_by_layer[layer] is not None:\n",
    "            prev_idx = prev_by_layer[layer]\n",
    "            all_edges.append(Edge3D(n=layer, row=prev_idx, col=idx, value=1.0))\n",
    "        prev_by_layer[layer] = idx\n",
    "\n",
    "# Add START edges\n",
    "for text in chinese_corpus:\n",
    "    chars = tokenize_signs(text)\n",
    "    if chars:\n",
    "        first_idx = lut.get_ntoken_index((chars[0],))\n",
    "        start_idx = lut.get_ntoken_index(START)\n",
    "        all_edges.append(Edge3D(n=0, row=start_idx, col=first_idx, value=1.0))\n",
    "\n",
    "# Build AM\n",
    "am = SparseAM3D.from_edges(new_config, all_edges)\n",
    "print(f\"AM: {am.nnz} edges\")\n",
    "for n in range(new_config.max_n):\n",
    "    rows, cols = am.layer_active(n)\n",
    "    print(f\"  L{n}: {len(rows)} rows, {len(cols)} cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd87ad88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: 101 sources\n",
      "LayerHLLSets ready for disambiguation\n"
     ]
    }
   ],
   "source": [
    "# Build W and LayerHLLSets\n",
    "W = build_w_from_am(am, new_config)\n",
    "layer_hllsets = LayerHLLSets.from_am(am, p_bits=new_config.p_bits)\n",
    "\n",
    "print(f\"W: {sum(len(l) for l in W.values())} sources\")\n",
    "print(f\"LayerHLLSets ready for disambiguation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b5e9137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FINAL TEST: Query Pipeline with Different Sign Sequences\n",
      "======================================================================\n",
      "Query: å­¦ä¹  (strategy=intersection)\n",
      "  Signs: 2, Indices: 3 (from 3 n-grams)\n",
      "  Cover: rows=21, cols=15, intersection=4, using=4\n",
      "  Results: 4 tokens\n",
      "    L1: ['å­¦ä¹ ']\n",
      "    L0: ['æœº', 'å­¦', 'ä¹ ']\n",
      "\n",
      "======================================================================\n",
      "Query: æœºå™¨å­¦ä¹  (strategy=intersection)\n",
      "  Signs: 4, Indices: 9 (from 9 n-grams)\n",
      "  Cover: rows=30, cols=24, intersection=10, using=10\n",
      "  Results: 8 tokens\n",
      "    L2: ['å™¨å­¦ä¹ ']\n",
      "    L1: ['æœºå™¨', 'å­¦ä¹ ']\n",
      "    L0: ['æœº', 'å™¨', 'å­¦']...\n",
      "\n",
      "======================================================================\n",
      "Query: ç¥ç»ç½‘ç»œ (strategy=intersection)\n",
      "  Signs: 4, Indices: 9 (from 9 n-grams)\n",
      "  Cover: rows=15, cols=16, intersection=9, using=9\n",
      "  Results: 7 tokens\n",
      "    L2: ['ç»ç½‘ç»œ']\n",
      "    L1: ['ç¥ç»', 'ç½‘ç»œ']\n",
      "    L0: ['ç¥', 'ç»', 'ç½‘']...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FINAL TEST: Multiple queries\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL TEST: Query Pipeline with Different Sign Sequences\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test 1: å­¦ä¹  (learning)\n",
    "result1 = query_signs(\"å­¦ä¹ \", lut, am, W, layer_hllsets, top_k=10, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Test 2: æœºå™¨å­¦ä¹  (machine learning)\n",
    "result2 = query_signs(\"æœºå™¨å­¦ä¹ \", lut, am, W, layer_hllsets, top_k=10, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Test 3: ç¥ç»ç½‘ç»œ (neural network)\n",
    "result3 = query_signs(\"ç¥ç»ç½‘ç»œ\", lut, am, W, layer_hllsets, top_k=10, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47bf4d8",
   "metadata": {},
   "source": [
    "## 9. Summary: Hash vs Vocabulary Schemes\n",
    "\n",
    "| Aspect | HashIdentifierScheme | VocabularyIdentifierScheme |\n",
    "|--------|---------------------|---------------------------|\n",
    "| **Use case** | Inflected languages (English, Russian, German) | Uninflected sign systems |\n",
    "| **Index range** | Large (sparse) | Compact (dense) |\n",
    "| **Collisions** | Possible (rare) | None (direct lookup) |\n",
    "| **Unknown tokens** | Hash to some index | Return -1 (explicit unknown) |\n",
    "| **Examples** | English words, sentences | Chinese, Musical notes (â™©â™ªâ™«â™¬), Emoji (ğŸµğŸ¶), Math symbols |\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "The **AM and W structures don't care** how indices are computed. They just need:\n",
    "1. Consistent mapping: same content â†’ same index\n",
    "2. Deterministic: reproducible across sessions\n",
    "\n",
    "This enables the fractal manifold to work with **ANY sign system**!\n",
    "\n",
    "### Module Functions Used\n",
    "\n",
    "From `core.sign_tokenizer`:\n",
    "- `tokenize_signs()` - Tokenize uninflected sign systems (any Unicode)\n",
    "- `generate_sign_ngrams()` - Generate 1-grams, 2-grams, 3-grams\n",
    "- `build_cover_from_w()` - Build cover using row/col intersection\n",
    "- `query_signs()` - Complete query pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3c8b934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "SCHEME COMPARISON: Hash vs Vocabulary\n",
      "==================================================\n",
      "\n",
      "Content: æœºå™¨å­¦ä¹ \n",
      "Signs: ['æœº', 'å™¨', 'å­¦', 'ä¹ ']\n",
      "\n",
      "Hash scheme indices (sparse):\n",
      "  'æœº' â†’ 3037\n",
      "  'å™¨' â†’ 2004\n",
      "  'å­¦' â†’ 17296\n",
      "  'ä¹ ' â†’ 5060\n",
      "\n",
      "Vocabulary scheme indices (dense):\n",
      "  'æœº' â†’ 1\n",
      "  'å™¨' â†’ 2\n",
      "  'å­¦' â†’ 3\n",
      "  'ä¹ ' â†’ 4\n",
      "\n",
      "Hash range: 0-23552 (2^32 * 1024)\n",
      "Vocab range: 0-37 (vocabulary size)\n",
      "\n",
      "âœ“ Same HRT structure, different identification schemes!\n"
     ]
    }
   ],
   "source": [
    "# Final comparison: Hash vs Vocabulary schemes\n",
    "print(\"=\" * 50)\n",
    "print(\"SCHEME COMPARISON: Hash vs Vocabulary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_content = \"æœºå™¨å­¦ä¹ \"\n",
    "chars = tokenize_signs(test_content)\n",
    "\n",
    "print(f\"\\nContent: {test_content}\")\n",
    "print(f\"Signs: {chars}\")\n",
    "\n",
    "print(f\"\\nHash scheme indices (sparse):\")\n",
    "for char in chars:\n",
    "    idx = hash_scheme.to_index(char)\n",
    "    print(f\"  '{char}' â†’ {idx}\")\n",
    "\n",
    "print(f\"\\nVocabulary scheme indices (dense):\")\n",
    "for char in chars:\n",
    "    idx = corpus_vocab.to_index(char)\n",
    "    print(f\"  '{char}' â†’ {idx}\")\n",
    "\n",
    "print(f\"\\nHash range: 0-{hash_scheme.index_range()} (2^32 * 1024)\")\n",
    "print(f\"Vocab range: 0-{corpus_vocab.index_range()} (vocabulary size)\")\n",
    "print(\"\\nâœ“ Same HRT structure, different identification schemes!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a16b4a",
   "metadata": {},
   "source": [
    "## 11. HashVocabularyScheme: Precise Hash-Based Identification\n",
    "\n",
    "**NEW**: `HashVocabularyScheme` combines benefits of both schemes:\n",
    "- Uses **full hash** as index (not (reg, zeros) compressed)\n",
    "- Each sign gets **exact** row/column in AM/W\n",
    "- Context unions are precise (neighbors of ä½  = exactly tokens seen with ä½ )\n",
    "- Still stores (reg, zeros) for HLLSet compatibility\n",
    "\n",
    "| Scheme | Pipeline | Index Range | AM/W Precision |\n",
    "|--------|----------|-------------|----------------|\n",
    "| `HashIdentifierScheme` | token â†’ hash â†’ (reg,zeros) â†’ index | ~32K | Cloud (compression) |\n",
    "| `HashVocabularyScheme` | sign â†’ hash â†’ index | ~4B | **Exact** (per-sign) |\n",
    "| `VocabularyIdentifierScheme` | sign â†’ sequential â†’ index | vocab size | Exact but arbitrary |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04e1a033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HashVocabularyScheme indices (precise):\n",
      "============================================================\n",
      "  'ä½ ' â†’ HVS: 1446033542 | HIS:  3083 | (reg=134, zeros=1)\n",
      "  'å¥½' â†’ HVS:  669318732 | HIS: 13524 | (reg=588, zeros=0)\n",
      "  'æˆ‘' â†’ HVS:  888275388 | HIS: 10219 | (reg=444, zeros=7)\n",
      "  'æ˜¯' â†’ HVS:  806750753 | HIS: 12536 | (reg=545, zeros=1)\n",
      "  'äºº' â†’ HVS: 1225946906 | HIS: 18262 | (reg=794, zeros=0)\n",
      "  'æœº' â†’ HVS:  225695876 | HIS:  3037 | (reg=132, zeros=1)\n",
      "  'å™¨' â†’ HVS: 2837930071 | HIS:  2004 | (reg=87, zeros=3)\n",
      "  'å­¦' â†’ HVS: 2088371952 | HIS: 17296 | (reg=752, zeros=0)\n",
      "\n",
      "HVS index range: 4,294,967,296 (full 32-bit hash)\n",
      "HIS index range: 23,552 (compressed)\n",
      "\n",
      "Collision report:\n",
      "  Vocabulary size: 19\n",
      "  Unique indices: 19\n",
      "  Collisions: 0\n"
     ]
    }
   ],
   "source": [
    "# Import HashVocabularyScheme\n",
    "from core.mf_algebra import HashVocabularyScheme\n",
    "\n",
    "# Build vocabulary with precise hash-based indices\n",
    "hvs = HashVocabularyScheme(p_bits=10, h_bits=32)\n",
    "\n",
    "# Add Chinese characters (lazy buildup)\n",
    "for char in chinese_chars:\n",
    "    hvs.add_sign(char)\n",
    "\n",
    "print(\"HashVocabularyScheme indices (precise):\")\n",
    "print(\"=\" * 60)\n",
    "for char in chinese_chars[:8]:\n",
    "    hvs_idx = hvs.to_index(char)\n",
    "    his_idx = hvs.to_his_index(char)  # HIS-compatible (compressed)\n",
    "    reg, zeros = hvs.to_reg_zeros(char)\n",
    "    print(f\"  '{char}' â†’ HVS: {hvs_idx:>10} | HIS: {his_idx:>5} | (reg={reg}, zeros={zeros})\")\n",
    "\n",
    "print(f\"\\nHVS index range: {hvs.index_range():,} (full 32-bit hash)\")\n",
    "print(f\"HIS index range: {hvs.his_index_range():,} (compressed)\")\n",
    "\n",
    "# Collision report\n",
    "report = hvs.collision_report()\n",
    "print(f\"\\nCollision report:\")\n",
    "print(f\"  Vocabulary size: {report['vocabulary_size']}\")\n",
    "print(f\"  Unique indices: {report['unique_indices']}\")\n",
    "print(f\"  Collisions: {report['collisions']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fda7f9",
   "metadata": {},
   "source": [
    "## 12. HVS â†” HIS Interoperability\n",
    "\n",
    "With same hash function, p_bits, and seed, HVS and HIS are fully compatible:\n",
    "\n",
    "```\n",
    "HVS: sign â†’ hash â†’ index (precise, ~4B range)\n",
    "       â†“\n",
    "      P(hash), Zeros(hash)\n",
    "       â†“\n",
    "HIS: sign â†’ hash â†’ (reg, zeros) â†’ index (compressed, ~32K range)\n",
    "```\n",
    "\n",
    "This enables **mixing precise and compressed data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e67d04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HVS â†’ HIS Projection Map\n",
      "============================================================\n",
      "HVS indices mapped to HIS: 19 entries\n",
      "Unique HIS indices: 19 entries\n",
      "\n",
      "Sample mappings (HVS â†’ HIS):\n",
      "  'ä½ ': 1446033542 â†’ 3083\n",
      "  'å¥½': 669318732 â†’ 13524\n",
      "  'æˆ‘': 888275388 â†’ 10219\n",
      "  'æ˜¯': 806750753 â†’ 12536\n",
      "  'äºº': 1225946906 â†’ 18262\n",
      "\n",
      "âœ“ No HIS collisions (vocabulary is small enough)\n"
     ]
    }
   ],
   "source": [
    "# HVS â†’ HIS projection maps\n",
    "print(\"HVS â†’ HIS Projection Map\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Build projection map\n",
    "hvs_to_his = hvs.hvs_to_his_index_map()\n",
    "his_to_hvs = hvs.his_to_hvs_index_map()\n",
    "\n",
    "print(f\"HVS indices mapped to HIS: {len(hvs_to_his)} entries\")\n",
    "print(f\"Unique HIS indices: {len(his_to_hvs)} entries\")\n",
    "\n",
    "# Show some mappings\n",
    "print(\"\\nSample mappings (HVS â†’ HIS):\")\n",
    "for char in chinese_chars[:5]:\n",
    "    hvs_idx = hvs.to_index(char)\n",
    "    his_idx = hvs_to_his[hvs_idx]\n",
    "    print(f\"  '{char}': {hvs_idx} â†’ {his_idx}\")\n",
    "\n",
    "# Check for HIS index collisions (multiple HVS indices â†’ same HIS index)\n",
    "collisions = {k: v for k, v in his_to_hvs.items() if len(v) > 1}\n",
    "if collisions:\n",
    "    print(f\"\\nâš ï¸ HIS collisions found: {len(collisions)}\")\n",
    "    for his_idx, hvs_set in list(collisions.items())[:3]:\n",
    "        signs = [hvs.get_sign(idx) for idx in hvs_set]\n",
    "        print(f\"  HIS {his_idx} â† signs: {signs}\")\n",
    "else:\n",
    "    print(\"\\nâœ“ No HIS collisions (vocabulary is small enough)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1eba3e",
   "metadata": {},
   "source": [
    "## 13. Vocabulary as HLLSet Fingerprint\n",
    "\n",
    "Vocabulary can be represented as an HLLSet for:\n",
    "- O(1) vocabulary comparison via tau similarity\n",
    "- Cross-installation vocabulary matching\n",
    "- Social/domain profiling by vocabulary overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ea900f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary HLLSet Fingerprint\n",
      "============================================================\n",
      "Fingerprint shape: (1024,)\n",
      "Non-zero registers: 19\n",
      "Estimated cardinality: 19\n",
      "Actual vocabulary size: 19\n",
      "\n",
      "Vocabulary comparison:\n",
      "  Tech vocab size: 8\n",
      "  Social vocab size: 8\n",
      "\n",
      "Tau similarity (O(1) via HLLSet):\n",
      "  Main â†” Tech: 0.750\n",
      "  Main â†” Social: 0.625\n",
      "  Tech â†” Social: 0.000\n"
     ]
    }
   ],
   "source": [
    "# Get vocabulary as HLLSet fingerprint\n",
    "vocab_hllset = hvs.to_hllset()\n",
    "print(\"Vocabulary HLLSet Fingerprint\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Fingerprint shape: {vocab_hllset.shape}\")\n",
    "print(f\"Non-zero registers: {(vocab_hllset > 0).sum()}\")\n",
    "print(f\"Estimated cardinality: {hvs.vocabulary_cardinality():.0f}\")\n",
    "print(f\"Actual vocabulary size: {len(hvs.known_signs)}\")\n",
    "\n",
    "# Create another vocabulary for comparison\n",
    "hvs_tech = HashVocabularyScheme(p_bits=10, h_bits=32)\n",
    "for char in [\"æœº\", \"å™¨\", \"å­¦\", \"ä¹ \", \"æ•°\", \"æ®\", \"ç®—\", \"æ³•\"]:  # Tech vocab\n",
    "    hvs_tech.add_sign(char)\n",
    "\n",
    "hvs_social = HashVocabularyScheme(p_bits=10, h_bits=32)\n",
    "for char in [\"ä½ \", \"å¥½\", \"æˆ‘\", \"æ˜¯\", \"äºº\", \"è¯´\", \"è¯\", \"å¬\"]:  # Social vocab\n",
    "    hvs_social.add_sign(char)\n",
    "\n",
    "# Compare vocabularies\n",
    "print(f\"\\nVocabulary comparison:\")\n",
    "print(f\"  Tech vocab size: {len(hvs_tech.known_signs)}\")\n",
    "print(f\"  Social vocab size: {len(hvs_social.known_signs)}\")\n",
    "\n",
    "# Tau similarity (O(1) via HLLSet)\n",
    "tau_hvs_tech = hvs.tau(hvs_tech)\n",
    "tau_hvs_social = hvs.tau(hvs_social)\n",
    "tau_tech_social = hvs_tech.tau(hvs_social)\n",
    "\n",
    "print(f\"\\nTau similarity (O(1) via HLLSet):\")\n",
    "print(f\"  Main â†” Tech: {tau_hvs_tech:.3f}\")\n",
    "print(f\"  Main â†” Social: {tau_hvs_social:.3f}\")\n",
    "print(f\"  Tech â†” Social: {tau_tech_social:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37d0d92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Set Operations\n",
      "============================================================\n",
      "Main âˆ© Tech: {'æ•°', 'å™¨', 'æ®', 'æœº', 'ä¹ ', 'å­¦'}\n",
      "Main âˆ© Social: {'æˆ‘', 'ä½ ', 'å¥½', 'äºº', 'æ˜¯'}\n",
      "\n",
      "Main \\ Tech: 13 signs\n",
      "Main \\ Social: 14 signs\n",
      "\n",
      "Merged (Tech âˆª Social): 16 signs\n",
      "  Signs: ['æœº', 'å™¨', 'å­¦', 'ä¹ ', 'æ•°', 'æ®', 'ç®—', 'æ³•', 'ä½ ', 'å¥½', 'æˆ‘', 'æ˜¯', 'äºº', 'è¯´', 'è¯', 'å¬']\n",
      "\n",
      "Jaccard(Tech, Social): 0.000 (|Aâˆ©B| / |AâˆªB|)\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary operations\n",
    "print(\"Vocabulary Set Operations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Intersection - signs in both\n",
    "shared_tech = hvs.vocabulary_intersection(hvs_tech)\n",
    "shared_social = hvs.vocabulary_intersection(hvs_social)\n",
    "\n",
    "print(f\"Main âˆ© Tech: {shared_tech}\")\n",
    "print(f\"Main âˆ© Social: {shared_social}\")\n",
    "\n",
    "# Difference - signs in main but not in other\n",
    "unique_from_tech = hvs.vocabulary_diff(hvs_tech)\n",
    "unique_from_social = hvs.vocabulary_diff(hvs_social)\n",
    "\n",
    "print(f\"\\nMain \\\\ Tech: {len(unique_from_tech)} signs\")\n",
    "print(f\"Main \\\\ Social: {len(unique_from_social)} signs\")\n",
    "\n",
    "# Merge vocabularies\n",
    "merged = hvs_tech.merge_vocabulary(hvs_social)\n",
    "print(f\"\\nMerged (Tech âˆª Social): {len(merged.known_signs)} signs\")\n",
    "print(f\"  Signs: {list(merged.known_signs.keys())}\")\n",
    "\n",
    "# Jaccard similarity\n",
    "jaccard_tech_social = hvs_tech.jaccard(hvs_social)\n",
    "print(f\"\\nJaccard(Tech, Social): {jaccard_tech_social:.3f} (|Aâˆ©B| / |AâˆªB|)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96741a8d",
   "metadata": {},
   "source": [
    "## 14. Summary: Three Identifier Schemes\n",
    "\n",
    "| Scheme | Pipeline | Index Range | Use Case |\n",
    "|--------|----------|-------------|----------|\n",
    "| `HashIdentifierScheme` | token â†’ hash â†’ (reg,zeros) â†’ index | ~32K | Default, open vocabularies, HLLSet-native |\n",
    "| `HashVocabularyScheme` | sign â†’ hash â†’ index | ~4B | Compact vocabularies, exact AM/W addressing |\n",
    "| `VocabularyIdentifierScheme` | sign â†’ sequential â†’ index | vocab size | Custom index assignments |\n",
    "\n",
    "**Key features of HashVocabularyScheme:**\n",
    "- âœ“ Lazy vocabulary buildup\n",
    "- âœ“ Vocabulary as HLLSet fingerprint (O(1) comparison)\n",
    "- âœ“ $\\text{HVS} â†” \\text{HIS}$ interoperability (same hash, compatible addressing)\n",
    "- âœ“ Cross-installation vocabulary matching\n",
    "- âœ“ Social/domain profiling via tau similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fractal_manifold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
