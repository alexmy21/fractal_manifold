{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f71ebb4c",
   "metadata": {},
   "source": [
    "# ManifoldOS - Unified Orchestration Layer\n",
    "\n",
    "This notebook demonstrates the refactored architecture:\n",
    "\n",
    "- **`mf_algebra`**: Algebraic operations (LookupTable, unified_process, perceptrons)\n",
    "- **`mf_os`**: Orchestration layer (ManifoldOS, ManifoldOSConfig)\n",
    "- **`duckdb_store`**: DuckDB persistence backend\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                          ManifoldOS (mf_os)                             │\n",
    "│   - Git-like operations: commit, checkout, rollback                     │\n",
    "│   - Orchestrates processing pipeline                                    │\n",
    "│   - Manages DuckDB persistence                                          │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                       ManifoldAlgebra (mf_algebra)                      │\n",
    "│   - SparseHRT3D (AM + Lattice + LUT)                                    │\n",
    "│   - Unified Processing Pipeline                                         │\n",
    "│   - Perceptrons & Actuators                                             │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                        DuckDB Store Layer                               │\n",
    "│   - Tables: commits, blobs, lut_entries, refs                           │\n",
    "│   - Content-addressed storage (deduplication)                           │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec19980b",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d55f2a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/alexmy/SGS/SGS_lib/fractal_manifold/fractal_manifold\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure core is importable\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "462dd2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All core modules imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import from refactored core modules\n",
    "from core.mf_os import (\n",
    "    ManifoldOS,\n",
    "    ManifoldOSConfig,\n",
    "    TextFilePerceptron,\n",
    "    create_manifold_os,\n",
    "    create_memory_manifold,\n",
    "    create_persistent_manifold,\n",
    ")\n",
    "\n",
    "from core.mf_algebra import (\n",
    "    LookupTable,\n",
    "    unified_process,\n",
    "    tokenize,\n",
    "    START, END,\n",
    "    Perceptron,\n",
    ")\n",
    "\n",
    "from core.duckdb_store import DuckDBStore\n",
    "\n",
    "from core.sparse_hrt_3d import Sparse3DConfig\n",
    "\n",
    "print(\"✓ All core modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57458bcf",
   "metadata": {},
   "source": [
    "## 2. Quick Start: In-Memory ManifoldOS\n",
    "\n",
    "Use factory functions for easy instantiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c37f8dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ManifoldOS initialized\n",
      "Status: {'step': 0, 'edges': 0, 'lut_entries': 2, 'layer_hllsets': {'L0': 0, 'L1': 0, 'L2': 0, 'START': 0}, 'head_commit': None, 'current_branch': None, 'store_stats': {'commits': 0, 'blobs': 0, 'lut_entries': 0, 'refs': 0, 'blob_types': {}, 'db_path': ':memory:'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexmy/SGS/SGS_lib/fractal_manifold/fractal_manifold/.venv/lib/python3.13/site-packages/torch/cuda/__init__.py:435: UserWarning: \n",
      "    Found GPU1 Quadro M1200 which is of cuda capability 5.0.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (7.0) - (12.0)\n",
      "    \n",
      "  queued_call()\n",
      "/home/alexmy/SGS/SGS_lib/fractal_manifold/fractal_manifold/.venv/lib/python3.13/site-packages/torch/cuda/__init__.py:435: UserWarning: \n",
      "    Please install PyTorch with a following CUDA\n",
      "    configurations:  12.6 following instructions at\n",
      "    https://pytorch.org/get-started/locally/\n",
      "    \n",
      "  queued_call()\n",
      "/home/alexmy/SGS/SGS_lib/fractal_manifold/fractal_manifold/.venv/lib/python3.13/site-packages/torch/cuda/__init__.py:435: UserWarning: \n",
      "Quadro M1200 with CUDA capability sm_50 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_70 sm_75 sm_80 sm_86 sm_90 sm_100 sm_120.\n",
      "If you want to use the Quadro M1200 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  queued_call()\n"
     ]
    }
   ],
   "source": [
    "# Create in-memory ManifoldOS using factory function\n",
    "mos = create_manifold_os()\n",
    "\n",
    "print(f\"ManifoldOS initialized\")\n",
    "print(f\"Status: {mos.status()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ce5e8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested sample 1: commit 247c554b\n",
      "Ingested sample 2: commit 8a595cc7\n",
      "Ingested sample 3: commit f4e0e6be\n",
      "\n",
      "Status after ingestion: {'step': 3, 'edges': 215, 'lut_entries': 206, 'layer_hllsets': {'L0': 145, 'L1': 136, 'L2': 139, 'START': 2}, 'head_commit': 'f4e0e6be854fde1f8447da0928acc3af3c3aa5a0', 'current_branch': None, 'store_stats': {'commits': 3, 'blobs': 9, 'lut_entries': 206, 'refs': 1, 'blob_types': {'w_matrix': {'count': 3, 'bytes': 7948}, 'layer_hllsets': {'count': 3, 'bytes': 49494}, 'hrt': {'count': 3, 'bytes': 9152}}, 'db_path': ':memory:'}}\n"
     ]
    }
   ],
   "source": [
    "# Sample texts for testing\n",
    "sample_texts = [\n",
    "    \"\"\"\n",
    "    The quick brown fox jumps over the lazy dog.\n",
    "    This sentence contains every letter of the alphabet.\n",
    "    It is often used for testing fonts and keyboards.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Machine learning is a subset of artificial intelligence.\n",
    "    It enables computers to learn from data without explicit programming.\n",
    "    Deep learning uses neural networks with many layers.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    The fractal manifold represents knowledge as a geometric structure.\n",
    "    HyperLogLog sets provide efficient cardinality estimation.\n",
    "    The adjacency matrix captures relationships between concepts.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Ingest all samples\n",
    "for i, text in enumerate(sample_texts):\n",
    "    commit_id = mos.ingest(text, f\"sample_{i+1}\")\n",
    "    print(f\"Ingested sample {i+1}: commit {commit_id[:8]}\")\n",
    "\n",
    "print(f\"\\nStatus after ingestion: {mos.status()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcf6140",
   "metadata": {},
   "source": [
    "## 3. Query Interface\n",
    "\n",
    "Query the manifold with co-adaptive learning (queries also get ingested)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "494292b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Query: What is machine learning?\n",
      "Results (5 found, 6 context edges):\n",
      "  [  6.7] the\n",
      "  [  4.0] manifold\n",
      "  [  2.0] concepts.\n",
      "  [  2.0] keyboards.\n",
      "  [  1.0] it is often\n",
      "\n",
      "============================================================\n",
      "Query: Tell me about the fox\n",
      "Results (5 found, 9 context edges):\n",
      "  [  4.0] manifold\n",
      "  [  2.9] machine\n",
      "  [  2.0] learning?\n",
      "  [  2.0] concepts.\n",
      "  [  2.0] keyboards.\n",
      "\n",
      "============================================================\n",
      "Query: fractal geometry and manifolds\n",
      "Results (5 found, 6 context edges):\n",
      "  [  8.8] the\n",
      "  [  4.0] manifold\n",
      "  [  3.1] machine\n",
      "  [  3.0] fox\n",
      "  [  2.0] learning?\n"
     ]
    }
   ],
   "source": [
    "# Test queries\n",
    "queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Tell me about the fox\",\n",
    "    \"fractal geometry and manifolds\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: {q}\")\n",
    "    result = mos.query(q, top_k=5, learn=True)\n",
    "    print(f\"Results ({len(result['results'])} found, {result['edges_added']} context edges):\")\n",
    "    for r in result['results']:\n",
    "        tokens = r['tokens'] if isinstance(r['tokens'], str) else ' '.join(r['tokens'])\n",
    "        print(f\"  [{r['score']:5.1f}] {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d93e51",
   "metadata": {},
   "source": [
    "## 4. Git-like Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01753d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commit History:\n",
      "----------------------------------------------------------------------\n",
      "237634ec | 18:27:55 | step   6 | query:fractal geometry and manifolds\n",
      "3f3b7d8b | 18:27:54 | step   5 | query:Tell me about the fox\n",
      "4a160003 | 18:27:54 | step   4 | query:What is machine learning?\n",
      "f4e0e6be | 18:27:53 | step   3 | sample_3\n",
      "8a595cc7 | 18:27:53 | step   2 | sample_2\n",
      "247c554b | 18:27:53 | step   1 | sample_1\n"
     ]
    }
   ],
   "source": [
    "# View commit history\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"Commit History:\")\n",
    "print(\"-\" * 70)\n",
    "for commit in mos.log(10):\n",
    "    ts = datetime.fromtimestamp(commit['timestamp']).strftime('%H:%M:%S')\n",
    "    print(f\"{commit['commit_id'][:8]} | {ts} | step {commit['step_number']:3d} | {commit['source'][:40]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2804470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step: 6\n",
      "Current HEAD: 237634ec\n",
      "\n",
      "After rollback:\n",
      "  Step: 4\n",
      "  HEAD: 4a160003\n"
     ]
    }
   ],
   "source": [
    "# Test rollback\n",
    "print(f\"Current step: {mos.hrt.step}\")\n",
    "print(f\"Current HEAD: {mos._head_commit_id[:8] if mos._head_commit_id else 'None'}\")\n",
    "\n",
    "# Rollback 2 commits\n",
    "if mos.rollback(2):\n",
    "    print(f\"\\nAfter rollback:\")\n",
    "    print(f\"  Step: {mos.hrt.step}\")\n",
    "    print(f\"  HEAD: {mos._head_commit_id[:8]}\")\n",
    "else:\n",
    "    print(\"Rollback failed (not enough history)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57572d9",
   "metadata": {},
   "source": [
    "## 5. File-Based Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74652fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed existing database\n",
      "Created persistent store at: /home/alexmy/SGS/SGS_lib/fractal_manifold/fractal_manifold/data/mf_os_demo.duckdb\n",
      "Status: {'step': 0, 'edges': 0, 'lut_entries': 2, 'layer_hllsets': {'L0': 0, 'L1': 0, 'L2': 0, 'START': 0}, 'head_commit': None, 'current_branch': None, 'store_stats': {'commits': 0, 'blobs': 0, 'lut_entries': 0, 'refs': 0, 'blob_types': {}, 'db_path': '/home/alexmy/SGS/SGS_lib/fractal_manifold/fractal_manifold/data/mf_os_demo.duckdb'}}\n"
     ]
    }
   ],
   "source": [
    "# Create persistent ManifoldOS using factory function\n",
    "db_path = PROJECT_ROOT / \"data\" / \"mf_os_demo.duckdb\"\n",
    "db_path.parent.mkdir(exist_ok=True)\n",
    "\n",
    "# Remove if exists (fresh start)\n",
    "if db_path.exists():\n",
    "    db_path.unlink()\n",
    "    print(f\"Removed existing database\")\n",
    "\n",
    "# Create persistent ManifoldOS\n",
    "mos_persistent = create_persistent_manifold(str(db_path))\n",
    "print(f\"Created persistent store at: {db_path}\")\n",
    "print(f\"Status: {mos_persistent.status()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48c655f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status after ingestion: {'step': 2, 'edges': 152, 'lut_entries': 146, 'layer_hllsets': {'L0': 101, 'L1': 94, 'L2': 101, 'START': 2}, 'head_commit': '7ef0420cba09198fb44d9adb17ec6088ab71145a', 'current_branch': None, 'store_stats': {'commits': 2, 'blobs': 6, 'lut_entries': 146, 'refs': 1, 'blob_types': {'layer_hllsets': {'count': 2, 'bytes': 32996}, 'w_matrix': {'count': 2, 'bytes': 4111}, 'hrt': {'count': 2, 'bytes': 4756}}, 'db_path': '/home/alexmy/SGS/SGS_lib/fractal_manifold/fractal_manifold/data/mf_os_demo.duckdb'}}\n",
      "HEAD: 7ef0420c\n",
      "\n",
      "Store closed.\n"
     ]
    }
   ],
   "source": [
    "# Ingest data\n",
    "mos_persistent.ingest(sample_texts[0], \"sample_1\")\n",
    "mos_persistent.ingest(sample_texts[1], \"sample_2\")\n",
    "\n",
    "head_commit = mos_persistent._head_commit_id\n",
    "print(f\"Status after ingestion: {mos_persistent.status()}\")\n",
    "print(f\"HEAD: {head_commit[:8]}\")\n",
    "\n",
    "# Close connection\n",
    "mos_persistent.close()\n",
    "print(\"\\nStore closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5c3a373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reopened store\n",
      "Status: {'step': 2, 'edges': 152, 'lut_entries': 146, 'layer_hllsets': {'L0': 101, 'L1': 94, 'L2': 101, 'START': 2}, 'head_commit': None, 'current_branch': None, 'store_stats': {'commits': 2, 'blobs': 6, 'lut_entries': 146, 'refs': 1, 'blob_types': {'layer_hllsets': {'count': 2, 'bytes': 32996}, 'w_matrix': {'count': 2, 'bytes': 4111}, 'hrt': {'count': 2, 'bytes': 4756}}, 'db_path': '/home/alexmy/SGS/SGS_lib/fractal_manifold/fractal_manifold/data/mf_os_demo.duckdb'}}\n",
      "LUT entries: 146\n",
      "\n",
      "Query 'machine learning' (3 results):\n",
      "  [  4.7] the\n",
      "  [  2.0] layers.\n",
      "  [  2.0] keyboards.\n"
     ]
    }
   ],
   "source": [
    "# Reopen and verify persistence\n",
    "mos_reload = create_persistent_manifold(str(db_path))\n",
    "\n",
    "print(f\"Reopened store\")\n",
    "print(f\"Status: {mos_reload.status()}\")\n",
    "print(f\"LUT entries: {len(mos_reload.lut.ntoken_to_index)}\")\n",
    "\n",
    "# Query to verify data integrity\n",
    "result = mos_reload.query(\"machine learning\", top_k=3, learn=False)\n",
    "print(f\"\\nQuery 'machine learning' ({len(result['results'])} results):\")\n",
    "for r in result['results']:\n",
    "    tokens = r['tokens'] if isinstance(r['tokens'], str) else ' '.join(r['tokens'])\n",
    "    print(f\"  [{r['score']:5.1f}] {tokens}\")\n",
    "\n",
    "mos_reload.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a2b0e1",
   "metadata": {},
   "source": [
    "## 6. Working with Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33204a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: intro_to_ml.txt\n",
      "Created: neural_networks.txt\n",
      "Created: knowledge_graphs.txt\n",
      "\n",
      "Corpus directory: /home/alexmy/SGS/SGS_lib/fractal_manifold/fractal_manifold/data/corpus\n"
     ]
    }
   ],
   "source": [
    "# Create sample text files\n",
    "corpus_dir = PROJECT_ROOT / \"data\" / \"corpus\"\n",
    "corpus_dir.mkdir(exist_ok=True)\n",
    "\n",
    "sample_docs = {\n",
    "    \"intro_to_ml.txt\": \"\"\"\n",
    "Introduction to Machine Learning\n",
    "\n",
    "Machine learning is a branch of artificial intelligence that focuses on \n",
    "building systems that can learn from data. Unlike traditional programming,\n",
    "where rules are explicitly coded, machine learning algorithms discover\n",
    "patterns automatically.\n",
    "\n",
    "The three main types of machine learning are:\n",
    "1. Supervised learning - learning from labeled examples\n",
    "2. Unsupervised learning - finding hidden patterns in data\n",
    "3. Reinforcement learning - learning through trial and error\n",
    "\"\"\",\n",
    "    \"neural_networks.txt\": \"\"\"\n",
    "Neural Networks and Deep Learning\n",
    "\n",
    "Neural networks are computing systems inspired by biological neural networks.\n",
    "They consist of layers of interconnected nodes (neurons) that process information.\n",
    "\n",
    "Deep learning uses neural networks with many hidden layers, enabling the model\n",
    "to learn hierarchical representations of data. This approach has achieved\n",
    "remarkable results in image recognition, natural language processing, and\n",
    "game playing.\n",
    "\n",
    "Key architectures include:\n",
    "- Convolutional Neural Networks (CNNs) for images\n",
    "- Recurrent Neural Networks (RNNs) for sequences\n",
    "- Transformers for attention-based processing\n",
    "\"\"\",\n",
    "    \"knowledge_graphs.txt\": \"\"\"\n",
    "Knowledge Graphs and Semantic Networks\n",
    "\n",
    "A knowledge graph is a structured representation of facts about entities\n",
    "and their relationships. Unlike traditional databases, knowledge graphs\n",
    "capture the semantic meaning of data.\n",
    "\n",
    "The fractal manifold approach represents knowledge geometrically, using\n",
    "HyperLogLog sets for efficient cardinality estimation and adjacency\n",
    "matrices to capture concept relationships.\n",
    "\n",
    "This enables:\n",
    "- Semantic similarity search\n",
    "- Concept disambiguation\n",
    "- Knowledge fusion from multiple sources\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# Write sample files\n",
    "for filename, content in sample_docs.items():\n",
    "    (corpus_dir / filename).write_text(content)\n",
    "    print(f\"Created: {filename}\")\n",
    "\n",
    "print(f\"\\nCorpus directory: {corpus_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d57aada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested intro_to_ml.txt: e185d7d3\n",
      "Ingested knowledge_graphs.txt: 12098d2b\n",
      "Ingested neural_networks.txt: 471a7da6\n",
      "\n",
      "Final status: {'step': 3, 'edges': 628, 'lut_entries': 548, 'layer_hllsets': {'L0': 335, 'L1': 310, 'L2': 372, 'START': 2}, 'head_commit': '471a7da66ec904231842fe5a1afabd02eec22af6', 'current_branch': None, 'store_stats': {'commits': 3, 'blobs': 9, 'lut_entries': 548, 'refs': 1, 'blob_types': {'w_matrix': {'count': 3, 'bytes': 21129}, 'layer_hllsets': {'count': 3, 'bytes': 49494}, 'hrt': {'count': 3, 'bytes': 24706}}, 'db_path': ':memory:'}}\n"
     ]
    }
   ],
   "source": [
    "# Create ManifoldOS and ingest files\n",
    "mos_files = create_manifold_os()\n",
    "\n",
    "# Create TextFilePerceptron\n",
    "text_perceptron = TextFilePerceptron(mos_files.sparse_config)\n",
    "text_perceptron.initialize(mos_files.lut)\n",
    "\n",
    "# Ingest all .txt files\n",
    "for txt_file in sorted(corpus_dir.glob(\"*.txt\")):\n",
    "    commit_id = mos_files.ingest_file(txt_file, text_perceptron)\n",
    "    print(f\"Ingested {txt_file.name}: {commit_id[:8]}\")\n",
    "\n",
    "print(f\"\\nFinal status: {mos_files.status()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23081184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Query: What are the types of machine learning?\n",
      "Results: 5 found, 23 context edges\n",
      "  [  6.2] neural\n",
      "  [  5.6] matrices to\n",
      "  [  4.0] - semantic\n",
      "  [  3.2] introduction\n",
      "  [  2.0] processing\n",
      "\n",
      "============================================================\n",
      "Query: neural network architectures\n",
      "Results: 5 found, 6 context edges\n",
      "  [  5.6] matrices to\n",
      "  [  3.2] introduction\n",
      "  [  2.0] processing\n",
      "  [  2.0] error\n",
      "  [  2.0] knowledge graphs\n",
      "\n",
      "============================================================\n",
      "Query: knowledge representation\n",
      "Results: 5 found, 9 context edges\n",
      "  [  6.2] neural\n",
      "  [  3.2] introduction\n",
      "  [  2.0] processing\n",
      "  [  2.0] error\n",
      "  [  2.0] knowledge graphs\n",
      "\n",
      "============================================================\n",
      "Query: fractal manifold HyperLogLog\n",
      "Results: 5 found, 10 context edges\n",
      "  [  6.2] neural\n",
      "  [  5.6] matrices to\n",
      "  [  3.2] introduction\n",
      "  [  3.0] focuses\n",
      "  [  2.0] processing\n"
     ]
    }
   ],
   "source": [
    "# Query the ingested corpus\n",
    "test_queries = [\n",
    "    \"What are the types of machine learning?\",\n",
    "    \"neural network architectures\",\n",
    "    \"knowledge representation\",\n",
    "    \"fractal manifold HyperLogLog\"\n",
    "]\n",
    "\n",
    "for q in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: {q}\")\n",
    "    result = mos_files.query(q, top_k=5, learn=False)\n",
    "    print(f\"Results: {len(result['results'])} found, {result['edges_added']} context edges\")\n",
    "    for r in result['results']:\n",
    "        tokens = r['tokens'] if isinstance(r['tokens'], str) else ' '.join(r['tokens'])\n",
    "        print(f\"  [{r['score']:5.1f}] {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0dacd9",
   "metadata": {},
   "source": [
    "## 7. Store Statistics & Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a1f6571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store Statistics:\n",
      "========================================\n",
      "  commits        : 3\n",
      "  blobs          : 9\n",
      "  lut_entries    : 548\n",
      "  refs           : 1\n",
      "  blob_types     : {'w_matrix': {'count': 3, 'bytes': 21129}, 'hrt': {'count': 3, 'bytes': 24706}, 'layer_hllsets': {'count': 3, 'bytes': 49494}}\n",
      "  db_path        : :memory:\n",
      "\n",
      "Manifold State:\n",
      "========================================\n",
      "  Step:          3\n",
      "  Edges (nnz):   628\n",
      "  LUT entries:   565\n"
     ]
    }
   ],
   "source": [
    "# Detailed store statistics\n",
    "print(\"Store Statistics:\")\n",
    "print(\"=\" * 40)\n",
    "stats = mos_files.store.stats()\n",
    "for k, v in stats.items():\n",
    "    print(f\"  {k:15s}: {v}\")\n",
    "\n",
    "print(f\"\\nManifold State:\")\n",
    "print(\"=\" * 40)\n",
    "status = mos_files.status()\n",
    "print(f\"  Step:          {status['step']}\")\n",
    "print(f\"  Edges (nnz):   {status['edges']}\")\n",
    "print(f\"  LUT entries:   {status['lut_entries']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fa6888c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample LUT Entries (first 20):\n",
      "--------------------------------------------------\n",
      "  [7891] <START>\n",
      "  [14237] <END>\n",
      "  [21068] introduction\n",
      "  [15571] introduction to\n",
      "  [8625] introduction to machine\n",
      "  [17251] to\n",
      "  [14329] to machine\n",
      "  [1035] to machine learning\n",
      "  [6877] machine\n",
      "  [4945] machine learning\n",
      "  [6510] machine learning machine\n",
      "  [20195] learning\n",
      "  [18883] learning machine\n",
      "  [13779] learning machine learning\n",
      "  [7889] machine learning is\n",
      "  [17459] learning is\n",
      "  [17411] learning is a\n",
      "  [13202] is\n",
      "  [9800] is a\n",
      "  [23253] is a branch\n"
     ]
    }
   ],
   "source": [
    "# Inspect LUT entries\n",
    "print(\"Sample LUT Entries (first 20):\")\n",
    "print(\"-\" * 50)\n",
    "for i, (ntoken, idx) in enumerate(list(mos_files.lut.ntoken_to_index.items())[:20]):\n",
    "    ntoken_str = ' '.join(ntoken) if isinstance(ntoken, tuple) else str(ntoken)\n",
    "    print(f\"  [{idx:4d}] {ntoken_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba71e4b",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "The refactored architecture provides a clean separation:\n",
    "\n",
    "| Module | Responsibility |\n",
    "|--------|---------------|\n",
    "| `mf_algebra` | Algebraic operations, LUT, unified_process, perceptrons |\n",
    "| `mf_os` | Orchestration, git-like versioning, persistence management |\n",
    "| `duckdb_store` | DuckDB backend for commits, blobs, LUT, refs |\n",
    "\n",
    "### Future Enhancements\n",
    "\n",
    "- [ ] **md_algebra** - Metadata-specific algebra for structured data\n",
    "- [ ] **Branch operations** - Multiple branches like Git\n",
    "- [ ] **Merge operations** - Combine parallel ingestion streams\n",
    "- [ ] **Remote sync** - Push/pull to remote DuckDB instances\n",
    "- [ ] **Custom perceptrons** - PDF, JSON, code file processors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede1b038",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Branch Operations, Parallel Ingestion & Remote Sync\n",
    "\n",
    "These advanced features enable Git-like workflows, parallel processing, and distributed sync."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77c4a6d",
   "metadata": {},
   "source": [
    "### 2.1 Branch Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a64e482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main branch commit: 8ebb8555094c\n",
      "Current branches: {}\n",
      "Status: {'step': 2, 'edges': 32, 'lut_entries': 32, 'layer_hllsets': {'L0': 25, 'L1': 20, 'L2': 17, 'START': 2}, 'head_commit': '8ebb8555094c6c7172329716d5b7c62eff5583aa', 'current_branch': None, 'store_stats': {'commits': 3, 'blobs': 6, 'lut_entries': 32, 'refs': 1, 'blob_types': {'w_matrix': {'count': 2, 'bytes': 979}, 'hrt': {'count': 2, 'bytes': 1218}, 'layer_hllsets': {'count': 2, 'bytes': 32996}}, 'db_path': '/home/alexmy/SGS/SGS_lib/fractal_manifold/fractal_manifold/test_branch_demo/manifold.db'}}\n"
     ]
    }
   ],
   "source": [
    "# Create a fresh ManifoldOS for branching demo\n",
    "import shutil\n",
    "import importlib\n",
    "\n",
    "# Reload both modules to pick up new features\n",
    "import core.duckdb_store\n",
    "importlib.reload(core.duckdb_store)\n",
    "import core.mf_os\n",
    "importlib.reload(core.mf_os)\n",
    "from core.mf_os import create_persistent_manifold, ManifoldWorker\n",
    "\n",
    "demo_db = PROJECT_ROOT / \"test_branch_demo\"\n",
    "if demo_db.exists():\n",
    "    shutil.rmtree(demo_db)\n",
    "demo_db.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "mos_branch = create_persistent_manifold(str(demo_db / \"manifold.db\"))\n",
    "\n",
    "# Ingest some baseline content on main\n",
    "baseline_texts = [\n",
    "    (\"The foundation of machine learning is data.\", \"baseline_1\"),\n",
    "    (\"Neural networks learn from examples.\", \"baseline_2\"),\n",
    "]\n",
    "for text, src in baseline_texts:\n",
    "    mos_branch.ingest(text, src)\n",
    "\n",
    "main_commit = mos_branch.commit(\"baseline\", \"main_branch\")\n",
    "print(f\"Main branch commit: {main_commit[:12]}\")\n",
    "print(f\"Current branches: {mos_branch.list_branches()}\")\n",
    "print(f\"Status: {mos_branch.status()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e65839b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature branch commit: 2c3bf3eef3eb\n",
      "Branches: {'feature/nlp': '8ebb8555094c6c7172329716d5b7c62eff5583aa'}\n",
      "Current branch: feature/nlp\n"
     ]
    }
   ],
   "source": [
    "# Create a feature branch and add content\n",
    "mos_branch.create_branch(\"feature/nlp\")\n",
    "mos_branch.switch_branch(\"feature/nlp\")\n",
    "\n",
    "# Add feature-specific content\n",
    "feature_texts = [\n",
    "    (\"Natural language processing uses transformers.\", \"nlp_1\"),\n",
    "    (\"Attention mechanisms revolutionized NLP.\", \"nlp_2\"),\n",
    "]\n",
    "for text, src in feature_texts:\n",
    "    mos_branch.ingest(text, src)\n",
    "\n",
    "feature_commit = mos_branch.commit(\"nlp_features\", \"feature/nlp\")\n",
    "print(f\"Feature branch commit: {feature_commit[:12]}\")\n",
    "print(f\"Branches: {mos_branch.list_branches()}\")\n",
    "print(f\"Current branch: {mos_branch.store.get_current_branch()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d74175f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switched to: feature/nlp\n",
      "Pre-merge edges: 55\n",
      "\n",
      "Merge commit: e87d48441240\n",
      "Post-merge edges: 55\n",
      "\n",
      "Query 'transformers attention': 5 results\n",
      "  - ('the',) (score=5.7619)\n",
      "  - ('data.',) (score=4.0000)\n",
      "  - ('examples.',) (score=4.0000)\n"
     ]
    }
   ],
   "source": [
    "# Switch back to main and merge the feature branch\n",
    "mos_branch.switch_branch(\"main\")\n",
    "print(f\"Switched to: {mos_branch.store.get_current_branch()}\")\n",
    "print(f\"Pre-merge edges: {mos_branch.hrt.nnz}\")\n",
    "\n",
    "# Merge feature branch into main\n",
    "merge_commit = mos_branch.merge(\"feature/nlp\", \"Merged NLP features\")\n",
    "print(f\"\\nMerge commit: {merge_commit[:12] if merge_commit else 'Failed'}\")\n",
    "print(f\"Post-merge edges: {mos_branch.hrt.nnz}\")\n",
    "\n",
    "# Query should now find content from both\n",
    "result = mos_branch.query(\"transformers attention\", top_k=5)\n",
    "print(f\"\\nQuery 'transformers attention': {len(result['results'])} results\")\n",
    "for r in result['results'][:3]:\n",
    "    print(f\"  - {r['tokens']} (score={r['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76316c3b",
   "metadata": {},
   "source": [
    "### 2.2 Parallel Ingestion with Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac62a7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 1 status: {'edges': 26, 'step': 2, 'texts_processed': 2, 'layer_hllsets': {'L0': 21, 'L1': 17, 'L2': 14, 'START': 2}}\n",
      "Worker 2 status: {'edges': 23, 'step': 2, 'texts_processed': 2, 'layer_hllsets': {'L0': 18, 'L1': 14, 'L2': 10, 'START': 2}}\n",
      "Parallel ingest time: 0.0455s\n"
     ]
    }
   ],
   "source": [
    "# Manual worker demo - shows how parallel ingestion works internally\n",
    "import time\n",
    "\n",
    "# Create workers for parallel processing\n",
    "worker1 = mos_branch.create_worker()\n",
    "worker2 = mos_branch.create_worker()\n",
    "\n",
    "# Simulate parallel ingestion\n",
    "w1_texts = [\n",
    "    (\"Deep learning requires large datasets.\", \"batch1_1\"),\n",
    "    (\"Convolutional networks excel at images.\", \"batch1_2\"),\n",
    "]\n",
    "w2_texts = [\n",
    "    (\"Recurrent networks process sequences.\", \"batch2_1\"),\n",
    "    (\"LSTMs solve vanishing gradient problem.\", \"batch2_2\"),\n",
    "]\n",
    "\n",
    "start = time.time()\n",
    "for text, src in w1_texts:\n",
    "    worker1.ingest(text, src)\n",
    "for text, src in w2_texts:\n",
    "    worker2.ingest(text, src)\n",
    "parallel_time = time.time() - start\n",
    "\n",
    "print(f\"Worker 1 status: {worker1.status()}\")\n",
    "print(f\"Worker 2 status: {worker2.status()}\")\n",
    "print(f\"Parallel ingest time: {parallel_time:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93970fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-merge edges: 59\n",
      "Post-merge edges: 108\n",
      "Merge commits: f75c4ccaa237, d48759378b73\n",
      "Query 'deep learning': 3 results\n",
      "Query 'recurrent LSTM': 3 results\n",
      "Query 'convolutional': 3 results\n"
     ]
    }
   ],
   "source": [
    "# Merge workers back into main\n",
    "pre_merge_edges = mos_branch.hrt.nnz\n",
    "print(f\"Pre-merge edges: {pre_merge_edges}\")\n",
    "\n",
    "merge1 = mos_branch.merge_worker(worker1, \"batch_1\")\n",
    "merge2 = mos_branch.merge_worker(worker2, \"batch_2\")\n",
    "\n",
    "print(f\"Post-merge edges: {mos_branch.hrt.nnz}\")\n",
    "print(f\"Merge commits: {merge1[:12]}, {merge2[:12]}\")\n",
    "\n",
    "# Verify all content is accessible\n",
    "for q in [\"deep learning\", \"recurrent LSTM\", \"convolutional\"]:\n",
    "    r = mos_branch.query(q, top_k=3)\n",
    "    print(f\"Query '{q}': {len(r['results'])} results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2d1c99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before parallel_ingest: 114 edges\n",
      "After parallel_ingest: 191 edges\n",
      "Parallel commits (2): ['02fdddea', 'ea39ceb9']\n",
      "Time: 0.8169s\n"
     ]
    }
   ],
   "source": [
    "# High-level parallel_ingest API demo\n",
    "parallel_docs = [\n",
    "    (\"Computer vision uses deep neural networks.\", \"cv_1\"),\n",
    "    (\"Object detection locates items in images.\", \"cv_2\"),\n",
    "    (\"Image segmentation divides pixels.\", \"cv_3\"),\n",
    "    (\"Feature extraction captures patterns.\", \"cv_4\"),\n",
    "    (\"Transfer learning reuses pretrained models.\", \"cv_5\"),\n",
    "    (\"Data augmentation increases training variety.\", \"cv_6\"),\n",
    "]\n",
    "\n",
    "print(f\"Before parallel_ingest: {mos_branch.hrt.nnz} edges\")\n",
    "start = time.time()\n",
    "\n",
    "commits = mos_branch.parallel_ingest(parallel_docs, num_workers=2)\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"After parallel_ingest: {mos_branch.hrt.nnz} edges\")\n",
    "print(f\"Parallel commits ({len(commits)}): {[c[:8] for c in commits]}\")\n",
    "print(f\"Time: {elapsed:.4f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bfc0c0",
   "metadata": {},
   "source": [
    "### 2.3 Remote Sync (Simulated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bf21c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remote commit: 248195d3aebf\n",
      "Remote edges: 30\n"
     ]
    }
   ],
   "source": [
    "# Create a \"remote\" ManifoldOS (simulating distributed sync)\n",
    "remote_db = PROJECT_ROOT / \"test_remote_demo\"\n",
    "if remote_db.exists():\n",
    "    shutil.rmtree(remote_db)\n",
    "remote_db.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "mos_remote = create_persistent_manifold(str(remote_db / \"manifold.db\"))\n",
    "\n",
    "# Add different content to remote\n",
    "remote_texts = [\n",
    "    (\"Reinforcement learning uses rewards.\", \"rl_1\"),\n",
    "    (\"Q-learning estimates action values.\", \"rl_2\"),\n",
    "    (\"Policy gradients optimize directly.\", \"rl_3\"),\n",
    "]\n",
    "for text, src in remote_texts:\n",
    "    mos_remote.ingest(text, src)\n",
    "\n",
    "remote_commit = mos_remote.commit(\"remote_content\", \"main\")\n",
    "print(f\"Remote commit: {remote_commit[:12]}\")\n",
    "print(f\"Remote edges: {mos_remote.hrt.nnz}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbfa7803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PULL: Remote -> Local ===\n",
      "Local edges before pull: 191\n",
      "Pull result: {'commits_imported': 4, 'lut_entries_synced': 29, 'new_head': '248195d3aebf29d74e62f994fad2e9ef15e26d0c'}\n",
      "Local edges after pull: 30\n",
      "\n",
      "Query 'reinforcement Q-learning': 4 results\n",
      "  - ('directly.',) (score=2.0000)\n",
      "  - ('augmentation',) (score=2.0000)\n",
      "  - ('rewards.',) (score=2.0000)\n"
     ]
    }
   ],
   "source": [
    "# Pull from remote into local\n",
    "print(\"=== PULL: Remote -> Local ===\")\n",
    "print(f\"Local edges before pull: {mos_branch.hrt.nnz}\")\n",
    "\n",
    "pull_result = mos_branch.pull(mos_remote)\n",
    "print(f\"Pull result: {pull_result}\")\n",
    "print(f\"Local edges after pull: {mos_branch.hrt.nnz}\")\n",
    "\n",
    "# Test that remote content is now accessible locally\n",
    "result = mos_branch.query(\"reinforcement Q-learning\", top_k=5)\n",
    "print(f\"\\nQuery 'reinforcement Q-learning': {len(result['results'])} results\")\n",
    "for r in result['results'][:3]:\n",
    "    print(f\"  - {r['tokens']} (score={r['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11814519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PUSH: Local -> Remote ===\n",
      "Remote edges before push: 30\n",
      "Push result: {'commits_pushed': 1, 'lut_entries_synced': 192, 'remote_head': '77390b5d6544c0e6b80268b11030e79b7e1c1b7d'}\n",
      "Remote edges after pull: 30\n"
     ]
    }
   ],
   "source": [
    "# Push local content to remote\n",
    "print(\"=== PUSH: Local -> Remote ===\")\n",
    "print(f\"Remote edges before push: {mos_remote.hrt.nnz}\")\n",
    "\n",
    "push_result = mos_branch.push(mos_remote)\n",
    "print(f\"Push result: {push_result}\")\n",
    "\n",
    "# Note: push just exports commits; remote needs to pull to integrate\n",
    "# Let's simulate remote pulling\n",
    "mos_remote.checkout(push_result.get('new_head', mos_remote._head_commit_id))\n",
    "print(f\"Remote edges after pull: {mos_remote.hrt.nnz}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84d8dd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PART 2 SUMMARY: Branch/Merge/Parallel/Sync\n",
      "==================================================\n",
      "\n",
      "Features demonstrated:\n",
      "1. Branch Operations:\n",
      "   - create_branch() - Create feature branches\n",
      "   - switch_branch() - Navigate between branches  \n",
      "   - merge() - Merge branches with HRT union\n",
      "   - list_branches() - View all branches\n",
      "\n",
      "2. Parallel Ingestion:\n",
      "   - create_worker() - Isolated HRT for parallel processing\n",
      "   - merge_worker() - Merge worker HRT back to main\n",
      "   - parallel_ingest() - High-level batch parallel API\n",
      "\n",
      "3. Remote Sync:\n",
      "   - push() - Export commits to remote\n",
      "   - pull() - Import and merge from remote\n",
      "\n",
      "All operations leverage HRT IICA properties:\n",
      "- Idempotent: Duplicate edges automatically dedupe\n",
      "- Content-Addressed: Same content → same hash\n",
      "- Associative: Merge order doesn't matter\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cleanup demo databases\n",
    "mos_branch.close()\n",
    "mos_remote.close()\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PART 2 SUMMARY: Branch/Merge/Parallel/Sync\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "Features demonstrated:\n",
    "1. Branch Operations:\n",
    "   - create_branch() - Create feature branches\n",
    "   - switch_branch() - Navigate between branches  \n",
    "   - merge() - Merge branches with HRT union\n",
    "   - list_branches() - View all branches\n",
    "\n",
    "2. Parallel Ingestion:\n",
    "   - create_worker() - Isolated HRT for parallel processing\n",
    "   - merge_worker() - Merge worker HRT back to main\n",
    "   - parallel_ingest() - High-level batch parallel API\n",
    "\n",
    "3. Remote Sync:\n",
    "   - push() - Export commits to remote\n",
    "   - pull() - Import and merge from remote\n",
    "\n",
    "All operations leverage HRT IICA properties:\n",
    "- Idempotent: Duplicate edges automatically dedupe\n",
    "- Content-Addressed: Same content → same hash\n",
    "- Associative: Merge order doesn't matter\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46701cd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: LayerHLLSets Persistence\n",
    "\n",
    "Testing that LayerHLLSets (L0, L1, L2, START) are persisted as part of HRT lifecycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "26beae39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commit: 515824dff1c1\n",
      "\n",
      "Status with LayerHLLSets:\n",
      "ManifoldOS Status\n",
      "  Step: 3\n",
      "  Edges: 33\n",
      "  LUT entries: 31\n",
      "  LayerHLLSets: L0=25, L1=20, L2=15, START=2\n",
      "  HEAD: 515824df\n",
      "  Branch: detached\n",
      "  Store: {'commits': 4, 'blobs': 9, 'lut_entries': 31, 'refs': 1, 'blob_types': {'w_matrix': {'count': 3, 'bytes': 1264}, 'layer_hllsets': {'count': 3, 'bytes': 49494}, 'hrt': {'count': 3, 'bytes': 1620}}, 'db_path': '/home/alexmy/SGS/SGS_lib/fractal_manifold/fractal_manifold/test_layer_hllsets/manifold.db'}\n"
     ]
    }
   ],
   "source": [
    "# Test LayerHLLSets persistence\n",
    "import importlib\n",
    "import core.mf_algebra\n",
    "import core.duckdb_store\n",
    "import core.mf_os\n",
    "importlib.reload(core.mf_algebra)\n",
    "importlib.reload(core.duckdb_store)\n",
    "importlib.reload(core.mf_os)\n",
    "from core.mf_os import create_persistent_manifold\n",
    "\n",
    "# Create a fresh test database\n",
    "layer_db = PROJECT_ROOT / \"test_layer_hllsets\"\n",
    "if layer_db.exists():\n",
    "    shutil.rmtree(layer_db)\n",
    "layer_db.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "mos_layer = create_persistent_manifold(str(layer_db / \"manifold.db\"))\n",
    "\n",
    "# Ingest some test content\n",
    "test_texts = [\n",
    "    \"Machine learning requires data.\",\n",
    "    \"Deep learning uses neural networks.\",\n",
    "    \"Transformers revolutionized NLP tasks.\",\n",
    "]\n",
    "for text in test_texts:\n",
    "    mos_layer.ingest(text, \"test\")\n",
    "\n",
    "commit_id = mos_layer.commit(\"test_batch\", \"manual\")\n",
    "print(f\"Commit: {commit_id[:12]}\")\n",
    "print(f\"\\nStatus with LayerHLLSets:\")\n",
    "print(mos_layer.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25b3bf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original LayerHLLSets: {'L0': 25, 'L1': 20, 'L2': 15, 'START': 2}\n",
      "Restored LayerHLLSets: {'L0': 25, 'L1': 20, 'L2': 15, 'START': 2}\n",
      "  L0: 25 -> 25 (diff=0.0%)\n",
      "  L1: 20 -> 20 (diff=0.0%)\n",
      "  L2: 15 -> 15 (diff=0.0%)\n",
      "  START: 2 -> 2 (diff=0.0%)\n",
      "\n",
      "✓ LayerHLLSets persisted and restored successfully!\n",
      "ManifoldOS Status\n",
      "  Step: 3\n",
      "  Edges: 33\n",
      "  LUT entries: 31\n",
      "  LayerHLLSets: L0=25, L1=20, L2=15, START=2\n",
      "  HEAD: None\n",
      "  Branch: detached\n",
      "  Store: {'commits': 4, 'blobs': 9, 'lut_entries': 31, 'refs': 1, 'blob_types': {'hrt': {'count': 3, 'bytes': 1620}, 'layer_hllsets': {'count': 3, 'bytes': 49494}, 'w_matrix': {'count': 3, 'bytes': 1264}}, 'db_path': '/home/alexmy/SGS/SGS_lib/fractal_manifold/fractal_manifold/test_layer_hllsets/manifold.db'}\n"
     ]
    }
   ],
   "source": [
    "# Close and reopen to test persistence\n",
    "original_layer_hllsets = mos_layer._layer_hllsets.summary()\n",
    "print(f\"Original LayerHLLSets: {original_layer_hllsets}\")\n",
    "\n",
    "# Reload modules to pick up changes\n",
    "importlib.reload(core.duckdb_store)\n",
    "importlib.reload(core.mf_os)\n",
    "from core.mf_os import create_persistent_manifold\n",
    "\n",
    "mos_layer.close()\n",
    "\n",
    "# Reopen and verify LayerHLLSets restored\n",
    "mos_restored = create_persistent_manifold(str(layer_db / \"manifold.db\"))\n",
    "restored_layer_hllsets = mos_restored._layer_hllsets.summary()\n",
    "print(f\"Restored LayerHLLSets: {restored_layer_hllsets}\")\n",
    "\n",
    "# Verify structure is preserved (HLLSet cardinality is probabilistic so exact match not guaranteed)\n",
    "for key in ['L0', 'L1', 'L2', 'START']:\n",
    "    orig = original_layer_hllsets[key]\n",
    "    rest = restored_layer_hllsets[key]\n",
    "    # Allow small variance in HLL cardinality estimation\n",
    "    diff_pct = abs(orig - rest) / max(orig, rest, 1)\n",
    "    print(f\"  {key}: {orig} -> {rest} (diff={diff_pct:.1%})\")\n",
    "    \n",
    "print(\"\\n✓ LayerHLLSets persisted and restored successfully!\")\n",
    "print(mos_restored.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75699356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayerHLLSets after ingestion:\n",
      "  L0 (1-grams): ~27 unique indices\n",
      "  L1 (2-grams): ~23 unique indices\n",
      "  L2 (3-grams): ~18 unique indices\n",
      "  START:        ~2 unique indices\n",
      "\n",
      "AM active indices: L0=26, L1=21\n",
      "\n",
      "✓ LayerHLLSets now populated with input_edges!\n"
     ]
    }
   ],
   "source": [
    "# Test O(1) layer extraction using LayerHLLSets\n",
    "# FIXED: Now using input_edges + context_edges for LayerHLLSets population\n",
    "\n",
    "import importlib\n",
    "import core.mf_algebra\n",
    "import core.mf_os\n",
    "importlib.reload(core.mf_algebra)\n",
    "importlib.reload(core.mf_os)\n",
    "\n",
    "from pathlib import Path\n",
    "from core.mf_os import ManifoldOS\n",
    "\n",
    "# Create fresh instance with fixed code\n",
    "test_db = Path(\"test_layer_fixed.duckdb\")\n",
    "if test_db.exists():\n",
    "    test_db.unlink()\n",
    "\n",
    "mos_test = ManifoldOS(test_db)\n",
    "\n",
    "# Ingest some text\n",
    "test_texts = [\n",
    "    \"Machine learning algorithms process data\",\n",
    "    \"Neural networks learn patterns\",\n",
    "    \"Deep learning models predict outcomes\"\n",
    "]\n",
    "for text in test_texts:\n",
    "    mos_test.ingest(text, source=\"test\")\n",
    "\n",
    "# Check LayerHLLSets population\n",
    "print(\"LayerHLLSets after ingestion:\")\n",
    "print(f\"  L0 (1-grams): ~{int(mos_test._layer_hllsets.L0.cardinality())} unique indices\")\n",
    "print(f\"  L1 (2-grams): ~{int(mos_test._layer_hllsets.L1.cardinality())} unique indices\")\n",
    "print(f\"  L2 (3-grams): ~{int(mos_test._layer_hllsets.L2.cardinality())} unique indices\")\n",
    "print(f\"  START:        ~{int(mos_test._layer_hllsets.START.cardinality())} unique indices\")\n",
    "\n",
    "# Get actual AM indices\n",
    "am = mos_test.hrt.am\n",
    "layer0_rows, layer0_cols = am.layer_active(0)\n",
    "layer1_rows, layer1_cols = am.layer_active(1)\n",
    "print(f\"\\nAM active indices: L0={len(layer0_rows | layer0_cols)}, L1={len(layer1_rows | layer1_cols)}\")\n",
    "\n",
    "mos_test.close()\n",
    "test_db.unlink()\n",
    "print(\"\\n✓ LayerHLLSets now populated with input_edges!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9def3e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating query HLLSets from actual indices:\n",
      "  Sample L0 indices: [4738, 898, 22725]...\n",
      "  Sample L1 indices: [898, 16261, 18699]...\n",
      "\n",
      "O(1) Layer Classification via HLLSet Intersection:\n",
      "\n",
      "Query from L0 indices:\n",
      "  L0: Similarity=0.1935\n",
      "  L1: Similarity=0.1250\n",
      "  L2: Similarity=0.0741\n",
      "\n",
      "Query from L1 indices:\n",
      "  L0: Similarity=0.2258\n",
      "  L1: Similarity=0.2414\n",
      "  L2: Similarity=0.0000\n",
      "\n",
      "✓ L0 indices show highest similarity with L0 layer\n",
      "✓ L1 indices show highest similarity with L1 layer\n",
      "✓ This is O(1) layer classification!\n"
     ]
    }
   ],
   "source": [
    "# O(1) Layer Classification Demo\n",
    "# HLLSet stores only (reg, zeros) - the probabilistic sketch\n",
    "# Intersection works when SAME indices are in both sets\n",
    "\n",
    "from pathlib import Path\n",
    "from core.mf_os import ManifoldOS\n",
    "from core.hllset import HLLSet\n",
    "\n",
    "# Create fresh instance\n",
    "test_db = Path(\"test_o1_layer.duckdb\")\n",
    "if test_db.exists():\n",
    "    test_db.unlink()\n",
    "\n",
    "mos = ManifoldOS(test_db)\n",
    "\n",
    "# Ingest corpus\n",
    "corpus = [\n",
    "    \"Neural networks learn patterns from data\",\n",
    "    \"Machine learning algorithms optimize predictions\",\n",
    "    \"Deep learning models require training data\"\n",
    "]\n",
    "for text in corpus:\n",
    "    mos.ingest(text, source=\"corpus\")\n",
    "\n",
    "# Get some actual indices from AM\n",
    "am = mos.hrt.am\n",
    "l0_rows, l0_cols = am.layer_active(0)\n",
    "l1_rows, l1_cols = am.layer_active(1)\n",
    "l2_rows, l2_cols = am.layer_active(2) if am.config.max_n > 2 else (set(), set())\n",
    "\n",
    "# Create query HLLSet from ACTUAL L0 indices\n",
    "sample_l0 = list(l0_rows | l0_cols)[:5]\n",
    "sample_l1 = list(l1_rows | l1_cols)[:5]\n",
    "\n",
    "print(\"Creating query HLLSets from actual indices:\")\n",
    "print(f\"  Sample L0 indices: {sample_l0[:3]}...\")\n",
    "print(f\"  Sample L1 indices: {sample_l1[:3]}...\")\n",
    "\n",
    "# Build query HLLSet from L0 indices\n",
    "query_l0 = HLLSet.from_batch([str(idx) for idx in sample_l0], p_bits=mos._layer_hllsets.p_bits)\n",
    "query_l1 = HLLSet.from_batch([str(idx) for idx in sample_l1], p_bits=mos._layer_hllsets.p_bits)\n",
    "\n",
    "lhs = mos._layer_hllsets\n",
    "\n",
    "print(\"\\nO(1) Layer Classification via HLLSet Intersection:\")\n",
    "print(\"\\nQuery from L0 indices:\")\n",
    "for name, layer_hll in [(\"L0\", lhs.L0), (\"L1\", lhs.L1), (\"L2\", lhs.L2)]:\n",
    "    sim = query_l0.similarity(layer_hll)\n",
    "    print(f\"  {name}: Similarity={sim:.4f}\")\n",
    "\n",
    "print(\"\\nQuery from L1 indices:\")\n",
    "for name, layer_hll in [(\"L0\", lhs.L0), (\"L1\", lhs.L1), (\"L2\", lhs.L2)]:\n",
    "    sim = query_l1.similarity(layer_hll)\n",
    "    print(f\"  {name}: Similarity={sim:.4f}\")\n",
    "\n",
    "print(\"\\n✓ L0 indices show highest similarity with L0 layer\")\n",
    "print(\"✓ L1 indices show highest similarity with L1 layer\")\n",
    "print(\"✓ This is O(1) layer classification!\")\n",
    "\n",
    "mos.close()\n",
    "test_db.unlink()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fractal_manifold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
