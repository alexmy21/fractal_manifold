{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9d34707",
   "metadata": {},
   "source": [
    "# Manifold Algebra + Manifold OS Integration\n",
    "\n",
    "## Goals\n",
    "\n",
    "1. **Sync mf_algebra and mf_os**: Create a unified architecture where:\n",
    "   - `ManifoldOS` is the orchestration layer (git-like operations, persistence)\n",
    "   - `mf_algebra` provides the algebraic operations on HRT structures\n",
    "\n",
    "2. **DuckDB Backend**: Use DuckDB as the persistent store for:\n",
    "   - HRT state (AM, Lattice)\n",
    "   - LUT (Lookup Table)\n",
    "   - Commits (versioning)\n",
    "\n",
    "3. **Simple .txt Input**: Work with normal text documents\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                          ManifoldOS (mf_os)                             │\n",
    "│   - Git-like operations: commit, push, pull, rollback                  │\n",
    "│   - Orchestrates processing pipeline                                    │\n",
    "│   - Manages DuckDB persistence                                          │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                       ManifoldAlgebra (mf_algebra)                      │\n",
    "│   - SparseHRT3D (AM + Lattice + LUT)                                   │\n",
    "│   - Unified Processing Pipeline                                        │\n",
    "│   - Perceptrons & Actuators                                            │\n",
    "│   - QueryContext & ask()                                               │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                        DuckDB Store Layer                               │\n",
    "│   - Tables: commits, blobs, lut_entries, refs                          │\n",
    "│   - Content-addressed storage                                          │\n",
    "│   - Efficient querying                                                 │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c3eb7d",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4607668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure core is importable\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "\n",
    "# Check DuckDB availability\n",
    "try:\n",
    "    import duckdb\n",
    "    print(f\"DuckDB version: {duckdb.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Installing DuckDB...\")\n",
    "    !pip install duckdb\n",
    "    import duckdb\n",
    "    print(f\"DuckDB version: {duckdb.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba727c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core modules\n",
    "from core.sparse_hrt_3d import (\n",
    "    Sparse3DConfig,\n",
    "    SparseHRT3D,\n",
    "    SparseAM3D,\n",
    "    SparseLattice3D,\n",
    "    Edge3D,\n",
    ")\n",
    "from core.mf_algebra import (\n",
    "    # LUT\n",
    "    LookupTable,\n",
    "    START, END,\n",
    "    \n",
    "    # Processing pipeline\n",
    "    unified_process,\n",
    "    build_w_from_am,\n",
    "    tokenize,\n",
    "    generate_ntokens,\n",
    "    \n",
    "    # Query interface\n",
    "    QueryContext,\n",
    "    create_query_context,\n",
    "    ask,\n",
    "    \n",
    "    # Perceptrons & Actuators\n",
    "    Perceptron,\n",
    "    PromptPerceptron,\n",
    "    ResponseActuator,\n",
    "    \n",
    "    # Commit store\n",
    "    Commit,\n",
    "    CommitStore,\n",
    ")\n",
    "from core.hllset import HLLSet\n",
    "from core.constants import P_BITS\n",
    "\n",
    "print(\"Core modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e15dce",
   "metadata": {},
   "source": [
    "## 2. DuckDB Store Implementation\n",
    "\n",
    "We'll create a DuckDB-backed store that replaces the in-memory `CommitStore` and provides persistent LUT storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e2ae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "from typing import Dict, List, Optional, Set, Tuple, Any\n",
    "from dataclasses import dataclass\n",
    "from core.hllset import compute_sha1\n",
    "\n",
    "\n",
    "class DuckDBStore:\n",
    "    \"\"\"\n",
    "    DuckDB-backed persistent store for ManifoldOS.\n",
    "    \n",
    "    Tables:\n",
    "    - commits: Version history with HRT snapshots\n",
    "    - lut_entries: Lookup table entries (token → index mapping)\n",
    "    - blobs: Content-addressed binary storage\n",
    "    - refs: Branch references (like Git refs)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = \":memory:\"):\n",
    "        \"\"\"\n",
    "        Initialize DuckDB store.\n",
    "        \n",
    "        Args:\n",
    "            db_path: Path to database file, or \":memory:\" for in-memory\n",
    "        \"\"\"\n",
    "        self.db_path = db_path\n",
    "        self.conn = duckdb.connect(db_path)\n",
    "        self._init_schema()\n",
    "    \n",
    "    def _init_schema(self):\n",
    "        \"\"\"Create tables if they don't exist.\"\"\"\n",
    "        self.conn.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS commits (\n",
    "                commit_id VARCHAR PRIMARY KEY,\n",
    "                parent_id VARCHAR,\n",
    "                timestamp DOUBLE,\n",
    "                source VARCHAR,\n",
    "                perceptron VARCHAR,\n",
    "                step_number INTEGER,\n",
    "                hrt_blob_id VARCHAR,\n",
    "                w_blob_id VARCHAR,\n",
    "                metadata JSON\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        self.conn.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS lut_entries (\n",
    "                idx INTEGER,\n",
    "                layer INTEGER,\n",
    "                ntoken VARCHAR,\n",
    "                ntoken_hash VARCHAR,\n",
    "                PRIMARY KEY (idx, layer, ntoken)\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        self.conn.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS blobs (\n",
    "                blob_id VARCHAR PRIMARY KEY,\n",
    "                blob_type VARCHAR,\n",
    "                data BLOB,\n",
    "                created_at DOUBLE\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        self.conn.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS refs (\n",
    "                ref_name VARCHAR PRIMARY KEY,\n",
    "                commit_id VARCHAR,\n",
    "                updated_at DOUBLE\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create indexes for faster lookups\n",
    "        self.conn.execute(\"CREATE INDEX IF NOT EXISTS idx_commits_parent ON commits(parent_id)\")\n",
    "        self.conn.execute(\"CREATE INDEX IF NOT EXISTS idx_lut_ntoken ON lut_entries(ntoken)\")\n",
    "        self.conn.execute(\"CREATE INDEX IF NOT EXISTS idx_lut_idx ON lut_entries(idx)\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Blob Operations (Content-Addressed Storage)\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    def store_blob(self, data: bytes, blob_type: str) -> str:\n",
    "        \"\"\"Store binary data, return content hash (deduplicated).\"\"\"\n",
    "        blob_id = compute_sha1(data)\n",
    "        \n",
    "        # Check if already exists\n",
    "        exists = self.conn.execute(\n",
    "            \"SELECT 1 FROM blobs WHERE blob_id = ?\", [blob_id]\n",
    "        ).fetchone()\n",
    "        \n",
    "        if not exists:\n",
    "            self.conn.execute(\n",
    "                \"INSERT INTO blobs (blob_id, blob_type, data, created_at) VALUES (?, ?, ?, ?)\",\n",
    "                [blob_id, blob_type, data, time.time()]\n",
    "            )\n",
    "        \n",
    "        return blob_id\n",
    "    \n",
    "    def fetch_blob(self, blob_id: str) -> Optional[bytes]:\n",
    "        \"\"\"Fetch blob data by ID.\"\"\"\n",
    "        result = self.conn.execute(\n",
    "            \"SELECT data FROM blobs WHERE blob_id = ?\", [blob_id]\n",
    "        ).fetchone()\n",
    "        return result[0] if result else None\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # HRT Serialization\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    def store_hrt(self, hrt: SparseHRT3D) -> str:\n",
    "        \"\"\"Serialize and store HRT, return blob ID.\"\"\"\n",
    "        # Get all edges from the tensor - use edges() method which returns List[Edge3D]\n",
    "        all_edges = hrt.am.tensor.edges()\n",
    "        # Convert Edge3D to tuples for serialization\n",
    "        edge_tuples = [(e.n, e.row, e.col, e.value) for e in all_edges]\n",
    "        \n",
    "        data = pickle.dumps({\n",
    "            'am_edges': edge_tuples,\n",
    "            'config': {\n",
    "                'p_bits': hrt.config.p_bits,\n",
    "                'h_bits': hrt.config.h_bits,\n",
    "                'max_n': hrt.config.max_n,\n",
    "                'dimension': hrt.config.dimension,\n",
    "            },\n",
    "            'step': hrt.step,\n",
    "        })\n",
    "        return self.store_blob(data, 'hrt')\n",
    "    \n",
    "    def fetch_hrt(self, blob_id: str, config: Sparse3DConfig) -> Optional[SparseHRT3D]:\n",
    "        \"\"\"Fetch and deserialize HRT.\"\"\"\n",
    "        data = self.fetch_blob(blob_id)\n",
    "        if not data:\n",
    "            return None\n",
    "        \n",
    "        obj = pickle.loads(data)\n",
    "        # Edge tuples are (n, row, col, value)\n",
    "        edges = [Edge3D(n=e[0], row=e[1], col=e[2], value=e[3]) for e in obj['am_edges']]\n",
    "        am = SparseAM3D.from_edges(config, edges)\n",
    "        lattice = SparseLattice3D.from_sparse_am(am)\n",
    "        \n",
    "        return SparseHRT3D(\n",
    "            am=am,\n",
    "            lattice=lattice,\n",
    "            config=config,\n",
    "            lut=frozenset(),\n",
    "            step=obj['step']\n",
    "        )\n",
    "    \n",
    "    def store_w(self, W: Dict[int, Dict[int, Dict[int, float]]]) -> str:\n",
    "        \"\"\"Serialize and store W matrix, return blob ID.\"\"\"\n",
    "        data = pickle.dumps(W)\n",
    "        return self.store_blob(data, 'w_matrix')\n",
    "    \n",
    "    def fetch_w(self, blob_id: str) -> Optional[Dict[int, Dict[int, Dict[int, float]]]]:\n",
    "        \"\"\"Fetch and deserialize W matrix.\"\"\"\n",
    "        data = self.fetch_blob(blob_id)\n",
    "        return pickle.loads(data) if data else None\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Commit Operations\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    def commit(\n",
    "        self,\n",
    "        hrt: SparseHRT3D,\n",
    "        W: Dict[int, Dict[int, Dict[int, float]]],\n",
    "        source: str,\n",
    "        perceptron: str,\n",
    "        parent_id: Optional[str] = None,\n",
    "        metadata: Optional[Dict] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Create a new commit with HRT and W state.\"\"\"\n",
    "        hrt_blob_id = self.store_hrt(hrt)\n",
    "        w_blob_id = self.store_w(W)\n",
    "        \n",
    "        # Generate commit ID from content\n",
    "        content = f\"{hrt_blob_id}:{w_blob_id}:{time.time()}:{source}\"\n",
    "        commit_id = compute_sha1(content)\n",
    "        \n",
    "        self.conn.execute(\"\"\"\n",
    "            INSERT INTO commits \n",
    "            (commit_id, parent_id, timestamp, source, perceptron, step_number, hrt_blob_id, w_blob_id, metadata)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\", [\n",
    "            commit_id,\n",
    "            parent_id,\n",
    "            time.time(),\n",
    "            source,\n",
    "            perceptron,\n",
    "            hrt.step,\n",
    "            hrt_blob_id,\n",
    "            w_blob_id,\n",
    "            json.dumps(metadata or {})\n",
    "        ])\n",
    "        \n",
    "        return commit_id\n",
    "    \n",
    "    def get_commit(self, commit_id: str) -> Optional[Dict]:\n",
    "        \"\"\"Get commit metadata by ID.\"\"\"\n",
    "        result = self.conn.execute(\n",
    "            \"SELECT * FROM commits WHERE commit_id = ?\", [commit_id]\n",
    "        ).fetchone()\n",
    "        \n",
    "        if not result:\n",
    "            return None\n",
    "        \n",
    "        return {\n",
    "            'commit_id': result[0],\n",
    "            'parent_id': result[1],\n",
    "            'timestamp': result[2],\n",
    "            'source': result[3],\n",
    "            'perceptron': result[4],\n",
    "            'step_number': result[5],\n",
    "            'hrt_blob_id': result[6],\n",
    "            'w_blob_id': result[7],\n",
    "            'metadata': json.loads(result[8]) if result[8] else {}\n",
    "        }\n",
    "    \n",
    "    def checkout(self, commit_id: str, config: Sparse3DConfig) -> Tuple[Optional[SparseHRT3D], Optional[Dict]]:\n",
    "        \"\"\"Checkout HRT and W from a commit.\"\"\"\n",
    "        commit = self.get_commit(commit_id)\n",
    "        if not commit:\n",
    "            return None, None\n",
    "        \n",
    "        hrt = self.fetch_hrt(commit['hrt_blob_id'], config)\n",
    "        W = self.fetch_w(commit['w_blob_id'])\n",
    "        \n",
    "        return hrt, W\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Refs (Branch Management)\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    def set_ref(self, ref_name: str, commit_id: str):\n",
    "        \"\"\"Set a ref to point to a commit.\"\"\"\n",
    "        self.conn.execute(\"\"\"\n",
    "            INSERT OR REPLACE INTO refs (ref_name, commit_id, updated_at)\n",
    "            VALUES (?, ?, ?)\n",
    "        \"\"\", [ref_name, commit_id, time.time()])\n",
    "    \n",
    "    def get_ref(self, ref_name: str) -> Optional[str]:\n",
    "        \"\"\"Get commit ID for a ref.\"\"\"\n",
    "        result = self.conn.execute(\n",
    "            \"SELECT commit_id FROM refs WHERE ref_name = ?\", [ref_name]\n",
    "        ).fetchone()\n",
    "        return result[0] if result else None\n",
    "    \n",
    "    def get_head(self, config: Sparse3DConfig) -> Tuple[Optional[SparseHRT3D], Optional[Dict]]:\n",
    "        \"\"\"Get the current HEAD state.\"\"\"\n",
    "        head_commit = self.get_ref('HEAD')\n",
    "        if not head_commit:\n",
    "            return None, None\n",
    "        return self.checkout(head_commit, config)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # LUT Operations\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    def store_lut_entry(self, idx: int, layer: int, ntoken: Tuple[str, ...]):\n",
    "        \"\"\"Store a LUT entry.\"\"\"\n",
    "        ntoken_str = \" \".join(ntoken)\n",
    "        ntoken_hash = compute_sha1(ntoken_str)\n",
    "        \n",
    "        self.conn.execute(\"\"\"\n",
    "            INSERT OR IGNORE INTO lut_entries (idx, layer, ntoken, ntoken_hash)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "        \"\"\", [idx, layer, ntoken_str, ntoken_hash])\n",
    "    \n",
    "    def get_ntokens_at_index(self, idx: int) -> Set[Tuple[int, Tuple[str, ...]]]:\n",
    "        \"\"\"Get all (layer, ntoken) pairs at an index.\"\"\"\n",
    "        results = self.conn.execute(\n",
    "            \"SELECT layer, ntoken FROM lut_entries WHERE idx = ?\", [idx]\n",
    "        ).fetchall()\n",
    "        \n",
    "        return {(row[0], tuple(row[1].split())) for row in results}\n",
    "    \n",
    "    def get_ntoken_index(self, ntoken: Tuple[str, ...]) -> Optional[int]:\n",
    "        \"\"\"Get index for an ntoken.\"\"\"\n",
    "        ntoken_str = \" \".join(ntoken)\n",
    "        result = self.conn.execute(\n",
    "            \"SELECT idx FROM lut_entries WHERE ntoken = ? LIMIT 1\", [ntoken_str]\n",
    "        ).fetchone()\n",
    "        return result[0] if result else None\n",
    "    \n",
    "    def sync_lut_from_memory(self, lut: LookupTable):\n",
    "        \"\"\"Sync in-memory LUT to DuckDB.\"\"\"\n",
    "        for idx, entries in lut.index_to_ntokens.items():\n",
    "            for layer, ntoken in entries:\n",
    "                self.store_lut_entry(idx, layer, ntoken)\n",
    "    \n",
    "    def load_lut_to_memory(self, config: Sparse3DConfig) -> LookupTable:\n",
    "        \"\"\"Load LUT from DuckDB to memory.\"\"\"\n",
    "        lut = LookupTable(config=config)\n",
    "        \n",
    "        results = self.conn.execute(\n",
    "            \"SELECT idx, layer, ntoken FROM lut_entries\"\n",
    "        ).fetchall()\n",
    "        \n",
    "        for idx, layer, ntoken_str in results:\n",
    "            ntoken = tuple(ntoken_str.split())\n",
    "            lut.index_to_ntokens[idx].add((layer, ntoken))\n",
    "            lut.ntoken_to_index[ntoken] = idx\n",
    "        \n",
    "        return lut\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # History & Stats\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    def log(self, limit: int = 10) -> List[Dict]:\n",
    "        \"\"\"Get recent commits.\"\"\"\n",
    "        results = self.conn.execute(\"\"\"\n",
    "            SELECT commit_id, parent_id, timestamp, source, perceptron, step_number\n",
    "            FROM commits\n",
    "            ORDER BY timestamp DESC\n",
    "            LIMIT ?\n",
    "        \"\"\", [limit]).fetchall()\n",
    "        \n",
    "        return [{\n",
    "            'commit_id': r[0],\n",
    "            'parent_id': r[1],\n",
    "            'timestamp': r[2],\n",
    "            'source': r[3],\n",
    "            'perceptron': r[4],\n",
    "            'step_number': r[5]\n",
    "        } for r in results]\n",
    "    \n",
    "    def stats(self) -> Dict[str, int]:\n",
    "        \"\"\"Get store statistics.\"\"\"\n",
    "        commits = self.conn.execute(\"SELECT COUNT(*) FROM commits\").fetchone()[0]\n",
    "        blobs = self.conn.execute(\"SELECT COUNT(*) FROM blobs\").fetchone()[0]\n",
    "        lut_entries = self.conn.execute(\"SELECT COUNT(*) FROM lut_entries\").fetchone()[0]\n",
    "        \n",
    "        return {\n",
    "            'commits': commits,\n",
    "            'blobs': blobs,\n",
    "            'lut_entries': lut_entries\n",
    "        }\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the database connection.\"\"\"\n",
    "        self.conn.close()\n",
    "\n",
    "\n",
    "# Test DuckDB store\n",
    "store = DuckDBStore(\":memory:\")\n",
    "print(f\"DuckDB store initialized: {store.stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d81b72",
   "metadata": {},
   "source": [
    "## 3. Text File Perceptron\n",
    "\n",
    "A perceptron for processing plain .txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98729901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Iterator\n",
    "\n",
    "\n",
    "class TextFilePerceptron(Perceptron):\n",
    "    \"\"\"\n",
    "    Perceptron for plain text files (.txt).\n",
    "    \n",
    "    Simple and effective for normal documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Sparse3DConfig):\n",
    "        super().__init__(\"p_text\", [\".txt\"], config)\n",
    "    \n",
    "    def extract_text(self, path: Path) -> str:\n",
    "        \"\"\"Read text file content.\"\"\"\n",
    "        try:\n",
    "            return path.read_text(encoding='utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            # Fallback to latin-1 if UTF-8 fails\n",
    "            return path.read_text(encoding='latin-1')\n",
    "\n",
    "\n",
    "# Test\n",
    "config = Sparse3DConfig(p_bits=P_BITS, h_bits=32, max_n=3)\n",
    "text_perceptron = TextFilePerceptron(config)\n",
    "print(f\"TextFilePerceptron initialized: {text_perceptron.name}\")\n",
    "print(f\"Extensions: {text_perceptron.extensions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40a2ecc",
   "metadata": {},
   "source": [
    "## 4. ManifoldOS with DuckDB Backend\n",
    "\n",
    "The orchestration layer that combines mf_algebra with DuckDB persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e71d4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, List, Dict, Tuple, Any\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ManifoldOSConfig:\n",
    "    \"\"\"Configuration for ManifoldOS.\"\"\"\n",
    "    p_bits: int = 10\n",
    "    h_bits: int = 32\n",
    "    max_n: int = 3\n",
    "    auto_commit: bool = True\n",
    "    auto_sync_lut: bool = True\n",
    "    \n",
    "    @property\n",
    "    def sparse_config(self) -> Sparse3DConfig:\n",
    "        return Sparse3DConfig(\n",
    "            p_bits=self.p_bits,\n",
    "            h_bits=self.h_bits,\n",
    "            max_n=self.max_n\n",
    "        )\n",
    "\n",
    "\n",
    "class ManifoldOS:\n",
    "    \"\"\"\n",
    "    ManifoldOS - Orchestration layer for Manifold operations.\n",
    "    \n",
    "    Features:\n",
    "    - Git-like versioning (commit, checkout, rollback)\n",
    "    - DuckDB persistence for HRT, W, and LUT\n",
    "    - Unified processing pipeline from mf_algebra\n",
    "    - Perceptron/Actuator architecture\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        db_path: str = \":memory:\",\n",
    "        config: Optional[ManifoldOSConfig] = None\n",
    "    ):\n",
    "        self.config = config or ManifoldOSConfig()\n",
    "        self.sparse_config = self.config.sparse_config\n",
    "        \n",
    "        # Initialize DuckDB store\n",
    "        self.store = DuckDBStore(db_path)\n",
    "        \n",
    "        # Initialize in-memory state\n",
    "        self._init_state()\n",
    "        \n",
    "        # Track current commit\n",
    "        self._head_commit_id: Optional[str] = None\n",
    "    \n",
    "    def _init_state(self):\n",
    "        \"\"\"Initialize empty state or load from store.\"\"\"\n",
    "        # Try to load HEAD from store\n",
    "        hrt, W = self.store.get_head(self.sparse_config)\n",
    "        \n",
    "        if hrt is not None and W is not None:\n",
    "            self.hrt = hrt\n",
    "            self.W = W\n",
    "            self.lut = self.store.load_lut_to_memory(self.sparse_config)\n",
    "            print(f\"Loaded existing state from store (step {self.hrt.step})\")\n",
    "        else:\n",
    "            # Create empty state\n",
    "            empty_am = SparseAM3D.from_edges(self.sparse_config, [])\n",
    "            empty_lattice = SparseLattice3D.from_sparse_am(empty_am)\n",
    "            self.hrt = SparseHRT3D(\n",
    "                am=empty_am,\n",
    "                lattice=empty_lattice,\n",
    "                config=self.sparse_config,\n",
    "                lut=frozenset(),\n",
    "                step=0\n",
    "            )\n",
    "            self.W: Dict[int, Dict[int, Dict[int, float]]] = {n: {} for n in range(self.config.max_n)}\n",
    "            self.lut = LookupTable(config=self.sparse_config)\n",
    "            self.lut.add_ntoken(START)\n",
    "            self.lut.add_ntoken(END)\n",
    "            print(\"Initialized empty state\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Ingestion\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    def ingest(self, text: str, source: str = \"input\", perceptron: str = \"manual\") -> str:\n",
    "        \"\"\"\n",
    "        Ingest text into the manifold.\n",
    "        \n",
    "        Args:\n",
    "            text: Text content to ingest\n",
    "            source: Source identifier (e.g., filename)\n",
    "            perceptron: Perceptron name that processed this\n",
    "        \n",
    "        Returns:\n",
    "            commit_id if auto_commit is True\n",
    "        \"\"\"\n",
    "        if not text.strip():\n",
    "            return \"\"\n",
    "        \n",
    "        # Process through unified pipeline\n",
    "        result = unified_process(\n",
    "            text,\n",
    "            self.hrt,\n",
    "            self.W,\n",
    "            self.sparse_config,\n",
    "            self.lut,\n",
    "            self.config.max_n\n",
    "        )\n",
    "        \n",
    "        # Update state\n",
    "        self.hrt = result.merged_hrt\n",
    "        self.W = build_w_from_am(self.hrt.am, self.sparse_config)\n",
    "        \n",
    "        # Auto-commit\n",
    "        commit_id = \"\"\n",
    "        if self.config.auto_commit:\n",
    "            commit_id = self.commit(source, perceptron)\n",
    "        \n",
    "        return commit_id\n",
    "    \n",
    "    def ingest_file(self, path: Path, perceptron: Optional[Perceptron] = None) -> str:\n",
    "        \"\"\"\n",
    "        Ingest a file using appropriate perceptron.\n",
    "        \"\"\"\n",
    "        if perceptron is None:\n",
    "            # Use TextFilePerceptron as default\n",
    "            perceptron = TextFilePerceptron(self.sparse_config)\n",
    "            perceptron.initialize(self.lut)\n",
    "        \n",
    "        text = perceptron.extract_text(path)\n",
    "        return self.ingest(text, str(path), perceptron.name)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Git-like Operations\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    def commit(self, source: str, perceptron: str, message: str = \"\") -> str:\n",
    "        \"\"\"\n",
    "        Create a new commit with current state.\n",
    "        \"\"\"\n",
    "        commit_id = self.store.commit(\n",
    "            self.hrt,\n",
    "            self.W,\n",
    "            source,\n",
    "            perceptron,\n",
    "            parent_id=self._head_commit_id,\n",
    "            metadata={'message': message} if message else None\n",
    "        )\n",
    "        \n",
    "        self._head_commit_id = commit_id\n",
    "        self.store.set_ref('HEAD', commit_id)\n",
    "        \n",
    "        # Sync LUT to store\n",
    "        if self.config.auto_sync_lut:\n",
    "            self.store.sync_lut_from_memory(self.lut)\n",
    "        \n",
    "        return commit_id\n",
    "    \n",
    "    def checkout(self, commit_id: str) -> bool:\n",
    "        \"\"\"\n",
    "        Checkout a specific commit.\n",
    "        \"\"\"\n",
    "        hrt, W = self.store.checkout(commit_id, self.sparse_config)\n",
    "        if hrt is None:\n",
    "            return False\n",
    "        \n",
    "        self.hrt = hrt\n",
    "        self.W = W\n",
    "        self._head_commit_id = commit_id\n",
    "        return True\n",
    "    \n",
    "    def rollback(self, steps: int = 1) -> bool:\n",
    "        \"\"\"\n",
    "        Rollback to a previous commit.\n",
    "        \"\"\"\n",
    "        commit = self.store.get_commit(self._head_commit_id)\n",
    "        \n",
    "        for _ in range(steps):\n",
    "            if commit is None or commit['parent_id'] is None:\n",
    "                return False\n",
    "            commit = self.store.get_commit(commit['parent_id'])\n",
    "        \n",
    "        if commit:\n",
    "            return self.checkout(commit['commit_id'])\n",
    "        return False\n",
    "    \n",
    "    def log(self, limit: int = 10) -> List[Dict]:\n",
    "        \"\"\"Get commit history.\"\"\"\n",
    "        return self.store.log(limit)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Query Interface\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    def query(self, prompt: str, top_k: int = 10, learn: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Query the manifold.\n",
    "        \n",
    "        Args:\n",
    "            prompt: Query text\n",
    "            top_k: Number of results to return\n",
    "            learn: If True, the query is also ingested (co-adaptive learning)\n",
    "        \n",
    "        Returns:\n",
    "            Query results with tokens and scores\n",
    "        \"\"\"\n",
    "        from core.manifold_algebra import (\n",
    "            reachable_from, project_layer, Sparse3DMatrix,\n",
    "            cascading_disambiguate, LayerHLLSets\n",
    "        )\n",
    "        \n",
    "        # Process query (same as ingestion!)\n",
    "        result = unified_process(\n",
    "            prompt,\n",
    "            self.hrt,\n",
    "            self.W,\n",
    "            self.sparse_config,\n",
    "            self.lut,\n",
    "            self.config.max_n\n",
    "        )\n",
    "        \n",
    "        # Get query indices\n",
    "        query_indices = set()\n",
    "        for basic in result.input_basics:\n",
    "            query_indices.add(basic.to_index(self.sparse_config))\n",
    "        \n",
    "        # Find reachable concepts\n",
    "        AM = Sparse3DMatrix.from_am(self.hrt.am, self.sparse_config)\n",
    "        layer0 = project_layer(AM, 0)\n",
    "        reachable = reachable_from(layer0, query_indices, hops=1)\n",
    "        \n",
    "        # Score by connectivity\n",
    "        layer0_dict = layer0.to_dict()\n",
    "        scores = {}\n",
    "        for idx in reachable:\n",
    "            if idx in layer0_dict:\n",
    "                scores[idx] = sum(layer0_dict[idx].values())\n",
    "        \n",
    "        # Get top-k results with token resolution\n",
    "        top = sorted(scores.items(), key=lambda x: -x[1])[:top_k]\n",
    "        results = []\n",
    "        for idx, score in top:\n",
    "            ntokens = self.lut.index_to_ntokens.get(idx, set())\n",
    "            if ntokens:\n",
    "                _, ntoken = next(iter(ntokens))\n",
    "                results.append({'index': idx, 'tokens': ntoken, 'score': score})\n",
    "            else:\n",
    "                results.append({'index': idx, 'tokens': f'<idx:{idx}>', 'score': score})\n",
    "        \n",
    "        # Learn from query if enabled\n",
    "        if learn:\n",
    "            self.hrt = result.merged_hrt\n",
    "            self.W = build_w_from_am(self.hrt.am, self.sparse_config)\n",
    "            if self.config.auto_commit:\n",
    "                self.commit(f\"query:{prompt[:50]}\", \"p_query\")\n",
    "        \n",
    "        return {\n",
    "            'prompt': prompt,\n",
    "            'query_indices': list(query_indices),\n",
    "            'results': results,\n",
    "            'edges_added': len(result.context_edges)\n",
    "        }\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # Status & Info\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    def status(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get current status.\"\"\"\n",
    "        return {\n",
    "            'step': self.hrt.step,\n",
    "            'edges': self.hrt.nnz,\n",
    "            'lut_entries': len(self.lut.ntoken_to_index),\n",
    "            'head_commit': self._head_commit_id,\n",
    "            'store_stats': self.store.stats()\n",
    "        }\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close store connection.\"\"\"\n",
    "        self.store.close()\n",
    "    \n",
    "    def __enter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        self.close()\n",
    "\n",
    "\n",
    "# Test ManifoldOS\n",
    "mos = ManifoldOS(\":memory:\")\n",
    "print(f\"ManifoldOS status: {mos.status()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea687c91",
   "metadata": {},
   "source": [
    "## 5. Test with Sample Text\n",
    "\n",
    "Let's test the integrated system with some sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ca4eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text for testing\n",
    "sample_texts = [\n",
    "    \"\"\"\n",
    "    The quick brown fox jumps over the lazy dog.\n",
    "    This sentence contains every letter of the alphabet.\n",
    "    It is often used for testing fonts and keyboards.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Machine learning is a subset of artificial intelligence.\n",
    "    It enables computers to learn from data without explicit programming.\n",
    "    Deep learning uses neural networks with many layers.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    The fractal manifold represents knowledge as a geometric structure.\n",
    "    HyperLogLog sets provide efficient cardinality estimation.\n",
    "    The adjacency matrix captures relationships between concepts.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Ingest all samples\n",
    "for i, text in enumerate(sample_texts):\n",
    "    commit_id = mos.ingest(text, f\"sample_{i+1}\")\n",
    "    print(f\"Ingested sample {i+1}: commit {commit_id[:8]}\")\n",
    "\n",
    "print(f\"\\nStatus after ingestion: {mos.status()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e250c4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test queries\n",
    "queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Tell me about the fox\",\n",
    "    \"fractal geometry\"\n",
    "]\n",
    "\n",
    "for q in queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: {q}\")\n",
    "    result = mos.query(q, top_k=5, learn=True)\n",
    "    print(f\"Results ({len(result['results'])} found):\")\n",
    "    for r in result['results']:\n",
    "        print(f\"  [{r['score']:5.1f}] {r['tokens']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e03a358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View commit history\n",
    "print(\"Commit History:\")\n",
    "print(\"-\" * 60)\n",
    "for commit in mos.log(10):\n",
    "    from datetime import datetime\n",
    "    ts = datetime.fromtimestamp(commit['timestamp']).strftime('%H:%M:%S')\n",
    "    print(f\"{commit['commit_id'][:8]} | {ts} | step {commit['step_number']:3d} | {commit['source'][:40]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836885f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store stats\n",
    "print(f\"\\nStore Statistics:\")\n",
    "stats = mos.store.stats()\n",
    "for k, v in stats.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab64d9d",
   "metadata": {},
   "source": [
    "## 6. File-Based Persistence Test\n",
    "\n",
    "Test with a file-based DuckDB database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dfb3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a file-based store\n",
    "db_file = PROJECT_ROOT / \"data\" / \"manifold.duckdb\"\n",
    "db_file.parent.mkdir(exist_ok=True)\n",
    "\n",
    "# Remove if exists (fresh start)\n",
    "if db_file.exists():\n",
    "    db_file.unlink()\n",
    "\n",
    "print(f\"Creating persistent store at: {db_file}\")\n",
    "\n",
    "# Create ManifoldOS with file store\n",
    "mos_persistent = ManifoldOS(str(db_file))\n",
    "\n",
    "# Ingest sample data\n",
    "mos_persistent.ingest(sample_texts[0], \"sample_1\")\n",
    "mos_persistent.ingest(sample_texts[1], \"sample_2\")\n",
    "\n",
    "print(f\"Status: {mos_persistent.status()}\")\n",
    "\n",
    "# Close and reopen to test persistence\n",
    "head_before = mos_persistent._head_commit_id\n",
    "mos_persistent.close()\n",
    "print(f\"\\nClosed store. HEAD was: {head_before[:8]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962d8e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reopen and verify state\n",
    "mos_reload = ManifoldOS(str(db_file))\n",
    "print(f\"Reopened store. Status: {mos_reload.status()}\")\n",
    "\n",
    "# Verify LUT was persisted\n",
    "print(f\"\\nLUT entries loaded: {len(mos_reload.lut.ntoken_to_index)}\")\n",
    "\n",
    "# Query to verify data integrity\n",
    "result = mos_reload.query(\"machine learning\", top_k=3, learn=False)\n",
    "print(f\"\\nQuery results: {result['results']}\")\n",
    "\n",
    "mos_reload.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961ed615",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "We've successfully synced `mf_algebra` and `mf_os` with the following architecture:\n",
    "\n",
    "1. **DuckDBStore**: Persistent storage backend\n",
    "   - Content-addressed blob storage (deduplicated)\n",
    "   - Commit history with parent tracking\n",
    "   - LUT persistence\n",
    "   - Ref management (HEAD, branches)\n",
    "\n",
    "2. **ManifoldOS**: Orchestration layer\n",
    "   - Uses `unified_process` from `mf_algebra`\n",
    "   - Git-like versioning (commit, checkout, rollback)\n",
    "   - Query interface with co-adaptive learning\n",
    "   - File ingestion via Perceptrons\n",
    "\n",
    "3. **TextFilePerceptron**: Simple .txt file processing\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- [ ] Move DuckDBStore to `core/duckdb_store.py`\n",
    "- [ ] Update `manifold_os_iica.py` to use new architecture (or replace)\n",
    "- [ ] Add branch operations\n",
    "- [ ] Add merge operations for parallel ingestion\n",
    "- [ ] Performance optimization for large documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fractal_manifold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
