{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b188643",
   "metadata": {},
   "source": [
    "# N-Token HLLSet Retrieval Simulation (v0.6.2)\n",
    "\n",
    "## The N-Token Model\n",
    "\n",
    "### Key Insight: N-Tokens Bootstrap Vocabulary\n",
    "\n",
    "For limited vocabularies (Chinese ~80K, low-res images), we need more variation.\n",
    "N-tokens artificially expand the element space.\n",
    "\n",
    "### Generation Pattern\n",
    "\n",
    "```\n",
    "Original sequence: (a, b, c, d)\n",
    "\n",
    "Full n-token chain with boundaries:\n",
    "\n",
    "  (START) → (a) → (a,b) → (a,b,c) → (b) → (b,c) → (b,c,d) → (c) → ... → (END)\n",
    "     ↓       ↓      ↓        ↓       ↓      ↓        ↓       ↓           ↓\n",
    "    HLL    HLL    HLL      HLL     HLL    HLL      HLL     HLL         HLL\n",
    "```\n",
    "\n",
    "### AM Stores the Chain (Explicit Order)\n",
    "\n",
    "```\n",
    "AM edges (row → col):\n",
    "  START      →  (a)          # Sequence begins\n",
    "  (a)        →  (a,b)        \n",
    "  (a,b)      →  (a,b,c)      \n",
    "  (a,b,c)    →  (b)          # Transition to next position\n",
    "  (b)        →  (b,c)        \n",
    "  ...\n",
    "  (c,d)      →  END          # Sequence ends\n",
    "```\n",
    "\n",
    "### AM Structure\n",
    "\n",
    "| | START | ... n-tokens ... | END |\n",
    "|---|---|---|---|\n",
    "| **START** | - | first n-token | - |\n",
    "| **... n-tokens ...** | (backprop) | chain links | last n-token |\n",
    "| **END** | - | (backprop) | - |\n",
    "\n",
    "- **Forward**: START row → first token; last token → END column\n",
    "- **Backprop**: Transpose AM, traverse END → START\n",
    "\n",
    "### What HLLSet Provides\n",
    "\n",
    "1. **Unordered result \"all at once\"** via set operations (union, intersection)\n",
    "2. **Implicit local order** via n-token composition (e.g., `(a,b)` implies `a` before `b`)\n",
    "3. **NO duplicates** in HLLSet\n",
    "\n",
    "### What AM Provides\n",
    "\n",
    "1. **Explicit global order** via edge chain\n",
    "2. **Duplicates** when needed (same token appears multiple times)\n",
    "3. **START/END boundaries** for sequence detection\n",
    "\n",
    "### 3D Layers Prevent Collisions\n",
    "\n",
    "| Layer | N-token type | Example |\n",
    "|-------|--------------|---------|\n",
    "| 0 | 1-tokens + START/END | `(a)`, `(b)`, `START`, `END` |\n",
    "| 1 | 2-tokens | `(a,b)`, `(b,c)` |\n",
    "| 2 | 3-tokens | `(a,b,c)`, `(b,c,d)` |\n",
    "\n",
    "Each n-token has its own `(reg, zeros)` signature.\n",
    "Layers prevent hash collisions between different-sized n-tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec353e29",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29f3dc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fractal Manifold Core v0.6.0\n",
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "import os\n",
    "import hashlib\n",
    "from typing import List, Dict, Set, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "# Suppress GPU warnings\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"0\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*cuda capability.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Quadro.*\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 3D Sparse Architecture (v0.6.0)\n",
    "from core import (\n",
    "    SparseHRT3D,\n",
    "    Sparse3DConfig,\n",
    "    SparseAM3D,\n",
    "    SparseLattice3D,\n",
    "    ImmutableSparseTensor3D,\n",
    "    BasicHLLSet3D,\n",
    "    Edge3D,\n",
    "    create_sparse_hrt_3d,\n",
    "    HLLSet,\n",
    "    get_device,\n",
    "    __version__\n",
    ")\n",
    "\n",
    "print(f\"Fractal Manifold Core v{__version__}\")\n",
    "print(f\"Device: {get_device()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dcb24d",
   "metadata": {},
   "source": [
    "## 2. Configuration and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73bfa63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== System Configuration ===\n",
      "N-gram size: 3\n",
      "AM dimension: 32,770\n",
      "Shape: (3, 32770, 32770)\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# System configuration\n",
    "N_GRAM_SIZE = 3  # Fixed n for the system (trigrams)\n",
    "P_BITS = 10      # HLL precision\n",
    "H_BITS = 32      # Hash bits\n",
    "\n",
    "# Special tokens\n",
    "START = \"<START>\"\n",
    "END = \"<END>\"\n",
    "\n",
    "# Create config\n",
    "config = Sparse3DConfig(\n",
    "    p_bits=P_BITS,\n",
    "    h_bits=H_BITS,\n",
    "    max_n=N_GRAM_SIZE  # Layers 0,1,2 for 1,2,3-grams\n",
    ")\n",
    "\n",
    "print(f\"=== System Configuration ===\")\n",
    "print(f\"N-gram size: {N_GRAM_SIZE}\")\n",
    "print(f\"AM dimension: {config.dimension:,}\")\n",
    "print(f\"Shape: {config.shape}\")\n",
    "print(f\"Device: {config.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bde6f4",
   "metadata": {},
   "source": [
    "## 3. Text Processing Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b80e9343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== N-Token Generation with START/END ===\n",
      "Text: 'The cat sat'\n",
      "Tokens: ['the', 'cat', 'sat']\n",
      "\n",
      "N-tokens (8 total, including START/END):\n",
      "  [ 0] L0: ('<START>',)\n",
      "  [ 1] L0: ('the',)\n",
      "  [ 2] L1: ('the', 'cat')\n",
      "  [ 3] L2: ('the', 'cat', 'sat')\n",
      "  [ 4] L0: ('cat',)\n",
      "  [ 5] L1: ('cat', 'sat')\n",
      "  [ 6] L0: ('sat',)\n",
      "  [ 7] L0: ('<END>',)\n",
      "\n",
      "AM Edges (7 total):\n",
      "  row=('<START>',)         → col=('the',)\n",
      "  row=('the',)             → col=('the', 'cat')\n",
      "  row=('the', 'cat')       → col=('the', 'cat', 'sat')\n",
      "  row=('the', 'cat', 'sat') → col=('cat',)\n",
      "  row=('cat',)             → col=('cat', 'sat')\n",
      "  row=('cat', 'sat')       → col=('sat',)\n",
      "  row=('sat',)             → col=('<END>',)\n",
      "\n",
      "Note: AM can be transposed for backpropagation (END → START)\n"
     ]
    }
   ],
   "source": [
    "# Special boundary tokens (treated as 1-tokens in layer 0)\n",
    "START = (\"<START>\",)  # Tuple form for consistency\n",
    "END = (\"<END>\",)\n",
    "\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Simple tokenization: lowercase and split on whitespace.\"\"\"\n",
    "    return text.lower().strip().split()\n",
    "\n",
    "\n",
    "def generate_ntokens_with_boundaries(tokens: List[str], max_n: int = 3) -> List[Tuple[str, ...]]:\n",
    "    \"\"\"\n",
    "    Generate all n-tokens from a token sequence WITH START/END boundaries.\n",
    "    \n",
    "    Pattern: (START) → (a) → (a,b) → (a,b,c) → (b) → ... → (y,z) → (END)\n",
    "    \n",
    "    Each n-token is a separate element that goes into HLLSet.\n",
    "    \"\"\"\n",
    "    ntokens = [START]  # Begin with START\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        # At position i, generate 1-token, 2-token, ..., up to max_n-token\n",
    "        for n in range(1, min(max_n + 1, len(tokens) - i + 1)):\n",
    "            ntoken = tuple(tokens[i:i + n])\n",
    "            ntokens.append(ntoken)\n",
    "    \n",
    "    ntokens.append(END)  # End with END\n",
    "    \n",
    "    return ntokens\n",
    "\n",
    "\n",
    "def generate_am_edges_with_boundaries(tokens: List[str], max_n: int = 3) -> List[Tuple[Tuple[str, ...], Tuple[str, ...]]]:\n",
    "    \"\"\"\n",
    "    Generate AM edges that link n-tokens in sequence.\n",
    "    \n",
    "    Pattern: row → col\n",
    "      START   → (a)\n",
    "      (a)     → (a,b)\n",
    "      (a,b)   → (a,b,c)\n",
    "      (a,b,c) → (b)        # Transition to next position\n",
    "      ...\n",
    "      (y,z)   → END\n",
    "    \n",
    "    These edges preserve explicit order.\n",
    "    Forward: START row points to first, last points to END column.\n",
    "    Backprop: Transpose AM to go END → START.\n",
    "    \"\"\"\n",
    "    ntokens = generate_ntokens_with_boundaries(tokens, max_n)\n",
    "    edges = []\n",
    "    \n",
    "    for i in range(len(ntokens) - 1):\n",
    "        edges.append((ntokens[i], ntokens[i + 1]))\n",
    "    \n",
    "    return edges\n",
    "\n",
    "\n",
    "def ntoken_to_hash(ntoken: Tuple[str, ...]) -> int:\n",
    "    \"\"\"Hash an n-token to integer.\"\"\"\n",
    "    text = \" \".join(ntoken)\n",
    "    h = hashlib.sha1(text.encode()).hexdigest()[:8]\n",
    "    return int(h, 16)\n",
    "\n",
    "\n",
    "def hash_to_index(hash_val: int, config: Sparse3DConfig) -> int:\n",
    "    \"\"\"Convert hash to AM index via BasicHLLSet.\"\"\"\n",
    "    basic = BasicHLLSet3D.from_hash(hash_val, n=0, p_bits=config.p_bits, h_bits=config.h_bits)\n",
    "    return basic.to_index(config)\n",
    "\n",
    "\n",
    "# Test the n-token generation\n",
    "test_text = \"The cat sat\"\n",
    "test_tokens = tokenize(test_text)\n",
    "\n",
    "print(f\"=== N-Token Generation with START/END ===\")\n",
    "print(f\"Text: '{test_text}'\")\n",
    "print(f\"Tokens: {test_tokens}\")\n",
    "print()\n",
    "\n",
    "# Generate n-tokens\n",
    "ntokens = generate_ntokens_with_boundaries(test_tokens, N_GRAM_SIZE)\n",
    "print(f\"N-tokens ({len(ntokens)} total, including START/END):\")\n",
    "for i, nt in enumerate(ntokens):\n",
    "    n = len(nt)\n",
    "    layer = 0 if nt in (START, END) else n - 1\n",
    "    print(f\"  [{i:2d}] L{layer}: {nt}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Generate AM edges\n",
    "edges = generate_am_edges_with_boundaries(test_tokens, N_GRAM_SIZE)\n",
    "print(f\"AM Edges ({len(edges)} total):\")\n",
    "for i, (row, col) in enumerate(edges):\n",
    "    print(f\"  row={str(row):20s} → col={col}\")\n",
    "\n",
    "print()\n",
    "print(\"Note: AM can be transposed for backpropagation (END → START)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fa465c",
   "metadata": {},
   "source": [
    "## 4. LUT (Lookup Table) for Token Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bdb94d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LUT initialized with boundary tokens:\n",
      "  START index: 7547\n",
      "  END index: 13621\n"
     ]
    }
   ],
   "source": [
    "class LookupTable:\n",
    "    \"\"\"\n",
    "    Lookup Table for n-token recovery.\n",
    "    \n",
    "    Maps:\n",
    "    - index → set of n-tokens at that index\n",
    "    - ntoken_hash → ntoken tuple\n",
    "    - ntoken → index\n",
    "    \n",
    "    START and END are treated as 1-tokens (layer 0).\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Sparse3DConfig):\n",
    "        self.config = config\n",
    "        # index → set of (layer, ntoken tuple)\n",
    "        self.index_to_ntokens: Dict[int, Set[Tuple[int, Tuple[str, ...]]]] = defaultdict(set)\n",
    "        # hash → ntoken tuple\n",
    "        self.hash_to_ntoken: Dict[int, Tuple[str, ...]] = {}\n",
    "        # ntoken → index\n",
    "        self.ntoken_to_index: Dict[Tuple[str, ...], int] = {}\n",
    "        \n",
    "    def add_ntoken(self, ntoken: Tuple[str, ...]) -> int:\n",
    "        \"\"\"Add an n-token to LUT. Returns its index.\"\"\"\n",
    "        # START and END go to layer 0 (like 1-tokens)\n",
    "        if ntoken in (START, END):\n",
    "            layer = 0\n",
    "        else:\n",
    "            layer = len(ntoken) - 1\n",
    "        \n",
    "        h = ntoken_to_hash(ntoken)\n",
    "        idx = hash_to_index(h, self.config)\n",
    "        \n",
    "        self.index_to_ntokens[idx].add((layer, ntoken))\n",
    "        self.hash_to_ntoken[h] = ntoken\n",
    "        self.ntoken_to_index[ntoken] = idx\n",
    "        \n",
    "        return idx\n",
    "    \n",
    "    def get_ntokens_at_index(self, idx: int) -> Set[Tuple[int, Tuple[str, ...]]]:\n",
    "        \"\"\"Get all (layer, ntoken) pairs at index.\"\"\"\n",
    "        return self.index_to_ntokens.get(idx, set())\n",
    "    \n",
    "    def get_1tokens_at_index(self, idx: int) -> Set[str]:\n",
    "        \"\"\"Get only 1-tokens (single words, not START/END) at index.\"\"\"\n",
    "        result = set()\n",
    "        for layer, nt in self.index_to_ntokens.get(idx, set()):\n",
    "            if layer == 0 and nt not in (START, END):\n",
    "                result.add(nt[0])  # Extract the single token string\n",
    "        return result\n",
    "    \n",
    "    def get_ntoken_index(self, ntoken: Tuple[str, ...]) -> Optional[int]:\n",
    "        \"\"\"Get index for an n-token if it was registered.\"\"\"\n",
    "        return self.ntoken_to_index.get(ntoken)\n",
    "    \n",
    "    def is_boundary(self, idx: int) -> bool:\n",
    "        \"\"\"Check if index contains START or END.\"\"\"\n",
    "        for _, nt in self.index_to_ntokens.get(idx, set()):\n",
    "            if nt in (START, END):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "# Create global LUT\n",
    "lut = LookupTable(config)\n",
    "\n",
    "# Pre-register START and END\n",
    "lut.add_ntoken(START)\n",
    "lut.add_ntoken(END)\n",
    "\n",
    "print(f\"LUT initialized with boundary tokens:\")\n",
    "print(f\"  START index: {lut.ntoken_to_index[START]}\")\n",
    "print(f\"  END index: {lut.ntoken_to_index[END]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1438374",
   "metadata": {},
   "source": [
    "## 5. Sample Texts Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2f9756c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 36 texts\n",
      "\n",
      "Sample texts:\n",
      "  1. The cat sat on the mat\n",
      "  2. The dog ran in the park\n",
      "  3. A bird flew over the house\n",
      "  4. The fish swam in the pond\n",
      "  5. The horse galloped across the field\n",
      "  ...\n"
     ]
    }
   ],
   "source": [
    "# Sample corpus of texts (3 dozen = 36 texts)\n",
    "CORPUS = [\n",
    "    # Animals\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog ran in the park\",\n",
    "    \"A bird flew over the house\",\n",
    "    \"The fish swam in the pond\",\n",
    "    \"The horse galloped across the field\",\n",
    "    \"A rabbit hopped through the garden\",\n",
    "    \n",
    "    # Actions\n",
    "    \"She walked to the store\",\n",
    "    \"He drove to work\",\n",
    "    \"They ran to school\",\n",
    "    \"We flew to Paris\",\n",
    "    \"I swam in the ocean\",\n",
    "    \"You jumped over the fence\",\n",
    "    \n",
    "    # Nature\n",
    "    \"The sun rose in the east\",\n",
    "    \"The moon shone at night\",\n",
    "    \"Stars twinkled in the sky\",\n",
    "    \"Rain fell on the ground\",\n",
    "    \"Snow covered the mountains\",\n",
    "    \"Wind blew through the trees\",\n",
    "    \n",
    "    # Objects\n",
    "    \"The book lay on the table\",\n",
    "    \"A cup sat on the shelf\",\n",
    "    \"The lamp lit the room\",\n",
    "    \"Keys hung by the door\",\n",
    "    \"Papers covered the desk\",\n",
    "    \"Photos lined the wall\",\n",
    "    \n",
    "    # Food\n",
    "    \"She ate an apple\",\n",
    "    \"He drank some coffee\",\n",
    "    \"They cooked dinner together\",\n",
    "    \"We baked a cake\",\n",
    "    \"I made some tea\",\n",
    "    \"You bought fresh bread\",\n",
    "    \n",
    "    # Time\n",
    "    \"Morning came with sunshine\",\n",
    "    \"Evening brought cool breeze\",\n",
    "    \"Night fell over the city\",\n",
    "    \"Dawn broke on the horizon\",\n",
    "    \"Dusk painted the sky red\",\n",
    "    \"Noon arrived with heat\",\n",
    "]\n",
    "\n",
    "print(f\"Corpus size: {len(CORPUS)} texts\")\n",
    "print(f\"\\nSample texts:\")\n",
    "for i, text in enumerate(CORPUS[:5]):\n",
    "    print(f\"  {i+1}. {text}\")\n",
    "print(f\"  ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a14244",
   "metadata": {},
   "source": [
    "## 6. Process Texts into 3D AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a7814d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processing Corpus with N-Token Model ===\n",
      "  Processed 10/36 texts...\n",
      "  Processed 20/36 texts...\n",
      "  Processed 30/36 texts...\n",
      "\n",
      "Processing complete!\n",
      "  Total edges: 450\n",
      "  LUT n-tokens: 336\n",
      "  Time: 3.0ms\n",
      "\n",
      "Edges by destination layer:\n",
      "  Layer 0: 210 edges\n",
      "  Layer 1: 138 edges\n",
      "  Layer 2: 102 edges\n"
     ]
    }
   ],
   "source": [
    "def process_text_to_ntoken_edges(\n",
    "    text: str,\n",
    "    config: Sparse3DConfig,\n",
    "    lut: LookupTable,\n",
    "    max_n: int = 3\n",
    ") -> List[Edge3D]:\n",
    "    \"\"\"\n",
    "    Process text into n-token edges for AM.\n",
    "    \n",
    "    N-token chain: (START) → (a) → (a,b) → (a,b,c) → (b) → ... → (END)\n",
    "    \n",
    "    AM edge: row=current_ntoken, col=next_ntoken\n",
    "    \n",
    "    Layer assignment:\n",
    "    - Use the DESTINATION n-token's layer\n",
    "    - START/END are layer 0\n",
    "    \"\"\"\n",
    "    tokens = tokenize(text)\n",
    "    ntokens = generate_ntokens_with_boundaries(tokens, max_n)\n",
    "    edges = []\n",
    "    \n",
    "    # Add all n-tokens to LUT\n",
    "    for ntoken in ntokens:\n",
    "        lut.add_ntoken(ntoken)\n",
    "    \n",
    "    # Create edges: each n-token links to the next\n",
    "    for i in range(len(ntokens) - 1):\n",
    "        row_ntoken = ntokens[i]\n",
    "        col_ntoken = ntokens[i + 1]\n",
    "        \n",
    "        row_idx = lut.get_ntoken_index(row_ntoken)\n",
    "        col_idx = lut.get_ntoken_index(col_ntoken)\n",
    "        \n",
    "        # Layer = destination n-token's layer\n",
    "        if col_ntoken in (START, END):\n",
    "            layer = 0\n",
    "        else:\n",
    "            layer = len(col_ntoken) - 1\n",
    "        \n",
    "        if row_idx is not None and col_idx is not None and layer < config.max_n:\n",
    "            edges.append(Edge3D(n=layer, row=row_idx, col=col_idx, value=1.0))\n",
    "    \n",
    "    return edges\n",
    "\n",
    "\n",
    "# Process all texts\n",
    "print(\"=== Processing Corpus with N-Token Model ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "all_edges = []\n",
    "for i, text in enumerate(CORPUS):\n",
    "    edges = process_text_to_ntoken_edges(text, config, lut, N_GRAM_SIZE)\n",
    "    all_edges.extend(edges)\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Processed {i+1}/{len(CORPUS)} texts...\")\n",
    "\n",
    "process_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nProcessing complete!\")\n",
    "print(f\"  Total edges: {len(all_edges):,}\")\n",
    "print(f\"  LUT n-tokens: {len(lut.ntoken_to_index):,}\")\n",
    "print(f\"  Time: {process_time*1000:.1f}ms\")\n",
    "\n",
    "# Show edge distribution by layer\n",
    "layer_counts = defaultdict(int)\n",
    "for edge in all_edges:\n",
    "    layer_counts[edge.n] += 1\n",
    "print(f\"\\nEdges by destination layer:\")\n",
    "for n in range(config.max_n):\n",
    "    print(f\"  Layer {n}: {layer_counts[n]:,} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64265f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Building 3D AM ===\n",
      "3D AM built!\n",
      "  Shape: (3, 32770, 32770)\n",
      "  Total edges: 416\n",
      "  Layer stats: {0: 193, 1: 123, 2: 100}\n",
      "  Memory: 0.01 MB\n",
      "  Time: 315.3ms\n"
     ]
    }
   ],
   "source": [
    "# Build 3D AM from edges\n",
    "print(\"=== Building 3D AM ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Aggregate edges (sum values for duplicates)\n",
    "edge_dict: Dict[Tuple[int, int, int], float] = {}\n",
    "for edge in all_edges:\n",
    "    key = (edge.n, edge.row, edge.col)\n",
    "    edge_dict[key] = edge_dict.get(key, 0.0) + edge.value\n",
    "\n",
    "aggregated_edges = [\n",
    "    Edge3D(n=k[0], row=k[1], col=k[2], value=v)\n",
    "    for k, v in edge_dict.items()\n",
    "]\n",
    "\n",
    "am = SparseAM3D.from_edges(config, aggregated_edges)\n",
    "lattice = SparseLattice3D.from_sparse_am(am)\n",
    "hrt = SparseHRT3D(am=am, lattice=lattice, config=config, lut=frozenset())\n",
    "\n",
    "build_time = time.time() - start_time\n",
    "\n",
    "print(f\"3D AM built!\")\n",
    "print(f\"  Shape: {hrt.shape}\")\n",
    "print(f\"  Total edges: {hrt.nnz:,}\")\n",
    "print(f\"  Layer stats: {hrt.layer_stats()}\")\n",
    "print(f\"  Memory: {hrt.memory_mb():.2f} MB\")\n",
    "print(f\"  Time: {build_time*1000:.1f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3233b538",
   "metadata": {},
   "source": [
    "## 7. Build W Matrix (Transition Probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54d9fed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Building W Matrix ===\n",
      "W matrix built!\n",
      "  Layer 0 (1-grams): 169 rows, 193 entries\n",
      "  Layer 1 (2-grams): 76 rows, 123 entries\n",
      "  Layer 2 (3-grams): 86 rows, 100 entries\n",
      "  Time: 3.8ms\n"
     ]
    }
   ],
   "source": [
    "def build_w_matrix_sparse(\n",
    "    am: SparseAM3D,\n",
    "    config: Sparse3DConfig\n",
    ") -> Dict[int, Dict[int, Dict[int, float]]]:\n",
    "    \"\"\"\n",
    "    Build W matrix (transition probabilities) from AM.\n",
    "    \n",
    "    W[n][row][col] = P(col | row, n-gram) = AM[n, row, col] / Σ_c AM[n, row, c]\n",
    "    \n",
    "    Returns sparse dict structure.\n",
    "    \"\"\"\n",
    "    W: Dict[int, Dict[int, Dict[int, float]]] = {}\n",
    "    \n",
    "    for n in range(config.max_n):\n",
    "        W[n] = {}\n",
    "        edges = am.tensor.layer_edges(n)\n",
    "        \n",
    "        # Group by row and compute sum\n",
    "        row_sums: Dict[int, float] = {}\n",
    "        row_edges: Dict[int, List[Tuple[int, float]]] = defaultdict(list)\n",
    "        \n",
    "        for row, col, val in edges:\n",
    "            row_sums[row] = row_sums.get(row, 0.0) + val\n",
    "            row_edges[row].append((col, val))\n",
    "        \n",
    "        # Normalize to get probabilities\n",
    "        for row, edges_list in row_edges.items():\n",
    "            W[n][row] = {}\n",
    "            row_sum = row_sums[row]\n",
    "            for col, val in edges_list:\n",
    "                W[n][row][col] = val / row_sum if row_sum > 0 else 0.0\n",
    "    \n",
    "    return W\n",
    "\n",
    "# Build W matrix\n",
    "print(\"=== Building W Matrix ===\")\n",
    "start_time = time.time()\n",
    "\n",
    "W = build_w_matrix_sparse(am, config)\n",
    "\n",
    "build_time = time.time() - start_time\n",
    "\n",
    "print(f\"W matrix built!\")\n",
    "for n in range(config.max_n):\n",
    "    n_rows = len(W[n])\n",
    "    n_entries = sum(len(cols) for cols in W[n].values())\n",
    "    print(f\"  Layer {n} ({n+1}-grams): {n_rows} rows, {n_entries} entries\")\n",
    "print(f\"  Time: {build_time*1000:.1f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe365a8",
   "metadata": {},
   "source": [
    "## 8. Create Query Text (Composition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe92781d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Query Text ===\n",
      "Query: 'The cat ran in the park'\n",
      "\n",
      "Source fragments:\n",
      "  From: 'The cat sat on the mat' → 'The cat'\n",
      "  From: 'The dog ran in the park' → 'ran in the park'\n",
      "\n",
      "Tokens: ['the', 'cat', 'ran', 'in', 'the', 'park']\n",
      "\n",
      "N-tokens (17 elements for HLLSet):\n",
      "  [ 0] L0: ('<START>',)\n",
      "  [ 1] L0: ('the',)\n",
      "  [ 2] L1: ('the', 'cat')\n",
      "  [ 3] L2: ('the', 'cat', 'ran')\n",
      "  [ 4] L0: ('cat',)\n",
      "  [ 5] L1: ('cat', 'ran')\n",
      "  [ 6] L2: ('cat', 'ran', 'in')\n",
      "  [ 7] L0: ('ran',)\n",
      "  [ 8] L1: ('ran', 'in')\n",
      "  [ 9] L2: ('ran', 'in', 'the')\n",
      "  [10] L0: ('in',)\n",
      "  [11] L1: ('in', 'the')\n",
      "  [12] L2: ('in', 'the', 'park')\n",
      "  [13] L0: ('the',)\n",
      "  [14] L1: ('the', 'park')\n",
      "  [15] L0: ('park',)\n",
      "  [16] L0: ('<END>',)\n"
     ]
    }
   ],
   "source": [
    "# Create a query text by combining fragments from different corpus texts\n",
    "# This simulates a \"noisy\" or \"mixed\" query\n",
    "\n",
    "# Let's compose: \"The cat ran in the park\" \n",
    "# (combines \"The cat sat on the mat\" + \"The dog ran in the park\")\n",
    "\n",
    "QUERY_TEXT = \"The cat ran in the park\"\n",
    "\n",
    "print(f\"=== Query Text ===\")\n",
    "print(f\"Query: '{QUERY_TEXT}'\")\n",
    "print()\n",
    "print(\"Source fragments:\")\n",
    "print(f\"  From: 'The cat sat on the mat' → 'The cat'\")\n",
    "print(f\"  From: 'The dog ran in the park' → 'ran in the park'\")\n",
    "\n",
    "# Generate n-tokens for query\n",
    "query_tokens = tokenize(QUERY_TEXT)\n",
    "query_ntokens = generate_ntokens_with_boundaries(query_tokens, N_GRAM_SIZE)\n",
    "\n",
    "print(f\"\\nTokens: {query_tokens}\")\n",
    "print(f\"\\nN-tokens ({len(query_ntokens)} elements for HLLSet):\")\n",
    "for i, nt in enumerate(query_ntokens):\n",
    "    layer = 0 if nt in (START, END) else len(nt) - 1\n",
    "    print(f\"  [{i:2d}] L{layer}: {nt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84658d56",
   "metadata": {},
   "source": [
    "## 9. Convert Query to HLLSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc700f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Query HLLSet (N-Token Model) ===\n",
      "Cardinality estimate: 16.0\n",
      "BasicHLLSet3D count: 17 (all n-tokens)\n",
      "\n",
      "By layer:\n",
      "  Layer 0: 8 n-tokens\n",
      "  Layer 1: 5 n-tokens\n",
      "  Layer 2: 4 n-tokens\n"
     ]
    }
   ],
   "source": [
    "def text_to_hllset_ntokens(\n",
    "    text: str,\n",
    "    config: Sparse3DConfig,\n",
    "    max_n: int = 3\n",
    ") -> Tuple[HLLSet, List[BasicHLLSet3D]]:\n",
    "    \"\"\"\n",
    "    Convert text to HLLSet using n-token model.\n",
    "    \n",
    "    All n-tokens go into HLLSet as separate elements.\n",
    "    Returns:\n",
    "        (hllset, list of BasicHLLSet3D for retrieval)\n",
    "    \"\"\"\n",
    "    tokens = tokenize(text)\n",
    "    ntokens = generate_ntokens_with_boundaries(tokens, max_n)\n",
    "    \n",
    "    hll = HLLSet(p_bits=config.p_bits)\n",
    "    basics: List[BasicHLLSet3D] = []\n",
    "    \n",
    "    # Add all n-tokens to HLLSet\n",
    "    for ntoken in ntokens:\n",
    "        ntoken_text = \" \".join(ntoken)\n",
    "        \n",
    "        # Add to HLLSet\n",
    "        hll = HLLSet.add(hll, ntoken_text)\n",
    "        \n",
    "        # Create BasicHLLSet3D for retrieval\n",
    "        h = ntoken_to_hash(ntoken)\n",
    "        \n",
    "        # Layer: START/END are layer 0, otherwise len-1\n",
    "        if ntoken in (START, END):\n",
    "            layer = 0\n",
    "        else:\n",
    "            layer = len(ntoken) - 1\n",
    "        \n",
    "        basic = BasicHLLSet3D.from_hash(\n",
    "            h, \n",
    "            n=layer,\n",
    "            p_bits=config.p_bits, \n",
    "            h_bits=config.h_bits\n",
    "        )\n",
    "        basics.append(basic)\n",
    "    \n",
    "    return hll, basics\n",
    "\n",
    "# Convert query to HLLSet\n",
    "query_hll, query_basics = text_to_hllset_ntokens(QUERY_TEXT, config, N_GRAM_SIZE)\n",
    "\n",
    "print(f\"=== Query HLLSet (N-Token Model) ===\")\n",
    "print(f\"Cardinality estimate: {query_hll.cardinality():.1f}\")\n",
    "print(f\"BasicHLLSet3D count: {len(query_basics)} (all n-tokens)\")\n",
    "print(f\"\\nBy layer:\")\n",
    "for n in range(config.max_n):\n",
    "    layer_basics = [b for b in query_basics if b.n == n]\n",
    "    print(f\"  Layer {n}: {len(layer_basics)} n-tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21433be1",
   "metadata": {},
   "source": [
    "## 10. Sheaf-Based Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35f41e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sheaf-Based Retrieval ===\n",
      "\n",
      "--- UNION MODE (any layer) ---\n",
      "Found 20 candidates in 21.9ms\n",
      "\n",
      "Top 10 candidates:\n",
      "  idx=17909: score=9.0 (L0=9.0) → [the]\n",
      "  idx=18835: score=5.0 (L1=5.0) → [<unknown>]\n",
      "  idx=1211: score=3.0 (L0=3.0) → [a]\n",
      "  idx=15490: score=2.0 (L0=2.0) → [she]\n",
      "  idx=18064: score=2.0 (L0=2.0) → [we]\n",
      "  idx=2201: score=2.0 (L1=1.0, L2=1.0) → [<unknown>]\n",
      "  idx=5151: score=2.0 (L0=2.0) → [he]\n",
      "  idx=10693: score=2.0 (L0=2.0) → [you]\n",
      "  idx=10715: score=2.0 (L0=1.0, L1=1.0) → [wind]\n",
      "  idx=2399: score=2.0 (L0=2.0) → [they]\n"
     ]
    }
   ],
   "source": [
    "# Use the retrieve method\n",
    "print(\"=== Sheaf-Based Retrieval ===\")\n",
    "print()\n",
    "\n",
    "# UNION MODE: All candidates from any layer\n",
    "print(\"--- UNION MODE (any layer) ---\")\n",
    "start_time = time.time()\n",
    "results_union = hrt.retrieve(query_basics, top_k=20, require_all_layers=False)\n",
    "retrieval_time = time.time() - start_time\n",
    "\n",
    "print(f\"Found {len(results_union)} candidates in {retrieval_time*1000:.1f}ms\")\n",
    "print(f\"\\nTop 10 candidates:\")\n",
    "for col, total, layers in results_union[:10]:\n",
    "    # Look up tokens at this index (use get_1tokens_at_index for 1-grams)\n",
    "    tokens_at_idx = lut.get_1tokens_at_index(col)\n",
    "    token_str = \", \".join(sorted(tokens_at_idx)[:3]) if tokens_at_idx else \"<unknown>\"\n",
    "    layer_str = \", \".join(f\"L{n}={v:.1f}\" for n, v in sorted(layers.items()))\n",
    "    print(f\"  idx={col}: score={total:.1f} ({layer_str}) → [{token_str}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e814adcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INTERSECTION MODE (global section) ---\n",
      "Found 0 candidates in 20.9ms\n",
      "\n",
      "Global section (tokens in ALL n-gram layers):\n"
     ]
    }
   ],
   "source": [
    "# INTERSECTION MODE: Only candidates in ALL layers (global section)\n",
    "print(\"--- INTERSECTION MODE (global section) ---\")\n",
    "start_time = time.time()\n",
    "results_inter = hrt.retrieve(query_basics, top_k=20, require_all_layers=True)\n",
    "retrieval_time = time.time() - start_time\n",
    "\n",
    "print(f\"Found {len(results_inter)} candidates in {retrieval_time*1000:.1f}ms\")\n",
    "print(f\"\\nGlobal section (tokens in ALL n-gram layers):\")\n",
    "for col, total, layers in results_inter[:10]:\n",
    "    tokens_at_idx = lut.get_tokens_at_index(col)\n",
    "    token_str = \", \".join(sorted(tokens_at_idx)[:3]) if tokens_at_idx else \"<unknown>\"\n",
    "    layer_str = \", \".join(f\"L{n}={v:.1f}\" for n, v in sorted(layers.items()))\n",
    "    print(f\"  idx={col}: score={total:.1f} ({layer_str}) → [{token_str}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c48ffe",
   "metadata": {},
   "source": [
    "## 11. COMMIT Query to HRT (The Key Step!)\n",
    "\n",
    "**Critical insight**: To interact with the system consistently, we must:\n",
    "\n",
    "1. **Create HLLSet** from our prompt\n",
    "2. **COMMIT** (merge) the HLLSet edges into HRT  \n",
    "3. **RETRIEVE** from the enhanced context\n",
    "\n",
    "By committing first, the system gains our query's n-gram relationships. The retrieval then happens against an **enriched** context that includes our prompt. The system returns an **enhanced response** that combines our query with existing knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06f01278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╔══════════════════════════════════════════════════════════╗\n",
      "║   N-TOKEN MODEL: COMMIT → RETRIEVE → REORDER             ║\n",
      "║   (HLLSet = all at once, AM = explicit order)            ║\n",
      "╚══════════════════════════════════════════════════════════╝\n",
      "\n",
      "Query: 'The cat ran in the park'\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "STEP 1: COMMIT Query N-Tokens to HRT\n",
      "────────────────────────────────────────────────────────────\n",
      "  Original HRT:  416 edges\n",
      "  Query edges:   16 (10x boosted)\n",
      "  Enhanced HRT:  421 edges\n",
      "\n",
      "  Query n-token chain:\n",
      "    ('<START>',) →\n",
      "    ('the',) →\n",
      "    ('the', 'cat') →\n",
      "    ('the', 'cat', 'ran') →\n",
      "    ('cat',) →\n",
      "    ('cat', 'ran') →\n",
      "    ('cat', 'ran', 'in') →\n",
      "    ('ran',) →\n",
      "    ... (9 more)\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "STEP 2: RETRIEVE N-Tokens (HLLSet = unordered)\n",
      "────────────────────────────────────────────────────────────\n",
      "  Retrieved 81 n-tokens (unordered set)\n",
      "  + Query n-tokens ensured: 17\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "STEP 3: REORDER via AM (explicit order + duplicates)\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "  Step 0: ('the',) (score=1010.0) ✓\n",
      "  Step 1: ('the', 'cat') (score=1010.0) ✓\n",
      "  Step 2: ('the', 'cat', 'ran') (score=1010.0) ✓\n",
      "  Step 3: ('cat',) (score=1010.0) ✓\n",
      "  Step 4: ('cat', 'ran') (score=1010.0) ✓\n",
      "  Step 5: ('cat', 'ran', 'in') (score=1010.0) ✓\n",
      "  Step 6: ('ran',) (score=1010.0) ✓\n",
      "  Step 7: ('ran', 'in') (score=1010.0) ✓\n",
      "  Step 8: ('ran', 'in', 'the') (score=1010.0) ✓\n",
      "  Step 9: ('in',) (score=1010.0) ✓\n",
      "  Step 10: ('in', 'the') (score=1010.0) ✓\n",
      "  Step 11: ('in', 'the', 'park') (score=1010.0) ✓\n",
      "  Step 12: ('the',) (score=1010.0) ✓\n",
      "  Step 13: ('the', 'park') (score=1010.0) ✓\n",
      "  Step 14: ('park',) (score=1010.0) ✓\n",
      "  Step 15: → END\n",
      "\n",
      "════════════════════════════════════════════════════════════\n",
      "RESULT\n",
      "════════════════════════════════════════════════════════════\n",
      "  Original:      'The cat ran in the park'\n",
      "  Reconstructed: 'the cat ran in the park'\n",
      "\n",
      "  Exact match:   ✓ Yes\n",
      "  Tokens OK:     ['cat', 'in', 'park', 'ran', 'the']\n",
      "\n",
      "  Recall:    100%\n",
      "  Precision: 100%\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# N-TOKEN MODEL: COMMIT → RETRIEVE → REORDER\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "#\n",
    "# Key differences from LLM approach:\n",
    "# 1. HLLSet gives result \"all at once\" (unordered set)\n",
    "# 2. AM provides explicit ordering + duplicates\n",
    "# 3. N-tokens bootstrap vocabulary variation\n",
    "# 4. START/END mark sequence boundaries\n",
    "#\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def commit_query_ntokens(\n",
    "    query_text: str,\n",
    "    hrt: SparseHRT3D,\n",
    "    lut: LookupTable,\n",
    "    config: Sparse3DConfig,\n",
    "    max_n: int = 3,\n",
    "    query_weight: float = 10.0\n",
    ") -> Tuple[SparseHRT3D, List[Edge3D], Set[Tuple[int, int, int]]]:\n",
    "    \"\"\"\n",
    "    COMMIT query n-tokens to HRT.\n",
    "    \n",
    "    Adds the n-token chain edges to AM with boosted weight.\n",
    "    \"\"\"\n",
    "    edges = process_text_to_ntoken_edges(query_text, config, lut, max_n)\n",
    "    \n",
    "    if not edges:\n",
    "        return hrt, [], set()\n",
    "    \n",
    "    edge_keys = set()\n",
    "    new_am = hrt.am\n",
    "    \n",
    "    for edge in edges:\n",
    "        boosted_value = edge.value * query_weight\n",
    "        new_am = new_am.with_edge(edge.n, edge.row, edge.col, boosted_value)\n",
    "        edge_keys.add((edge.n, edge.row, edge.col))\n",
    "    \n",
    "    new_lattice = SparseLattice3D.from_sparse_am(new_am)\n",
    "    new_hrt = SparseHRT3D(\n",
    "        am=new_am, lattice=new_lattice, config=config,\n",
    "        lut=frozenset(), step=hrt.step + 1\n",
    "    )\n",
    "    \n",
    "    return new_hrt, edges, edge_keys\n",
    "\n",
    "\n",
    "def retrieve_ntokens(\n",
    "    query_basics: List[BasicHLLSet3D],\n",
    "    hrt: SparseHRT3D,\n",
    "    lut: LookupTable\n",
    ") -> Set[Tuple[str, ...]]:\n",
    "    \"\"\"\n",
    "    RETRIEVE: Get n-tokens from HRT that match query.\n",
    "    \n",
    "    Returns unordered SET of n-tokens (HLLSet semantics).\n",
    "    \"\"\"\n",
    "    results = hrt.retrieve(query_basics, require_all_layers=False)\n",
    "    \n",
    "    retrieved_ntokens = set()\n",
    "    for col, score, layers in results:\n",
    "        for layer, ntoken in lut.get_ntokens_at_index(col):\n",
    "            retrieved_ntokens.add(ntoken)\n",
    "    \n",
    "    return retrieved_ntokens\n",
    "\n",
    "\n",
    "def reorder_via_am(\n",
    "    retrieved_ntokens: Set[Tuple[str, ...]],\n",
    "    query_ntokens: List[Tuple[str, ...]],  # Query n-tokens IN ORDER\n",
    "    hrt: SparseHRT3D,\n",
    "    lut: LookupTable,\n",
    "    config: Sparse3DConfig,\n",
    "    max_tokens: int = 50,\n",
    "    debug: bool = False\n",
    ") -> List[Tuple[str, ...]]:\n",
    "    \"\"\"\n",
    "    REORDER: Use AM edges to establish sequence order.\n",
    "    \n",
    "    CRITICAL: Follow query n-token sequence explicitly.\n",
    "    The query n-tokens define the expected order.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Start from START\n",
    "    2. Try to follow the query n-token sequence in order\n",
    "    3. When stuck, try corpus edges to next query n-token\n",
    "    \"\"\"\n",
    "    # Query n-tokens as ordered list (skip START/END for navigation)\n",
    "    query_chain = [nt for nt in query_ntokens if nt not in (START, END)]\n",
    "    query_ntoken_set = set(query_chain)\n",
    "    \n",
    "    # Build index mapping\n",
    "    ntoken_to_idx = {}\n",
    "    for nt in retrieved_ntokens:\n",
    "        idx = lut.get_ntoken_index(nt)\n",
    "        if idx is not None:\n",
    "            ntoken_to_idx[nt] = idx\n",
    "    \n",
    "    idx_to_ntoken = {v: k for k, v in ntoken_to_idx.items()}\n",
    "    \n",
    "    start_idx = lut.get_ntoken_index(START)\n",
    "    end_idx = lut.get_ntoken_index(END)\n",
    "    \n",
    "    if start_idx is None:\n",
    "        if debug:\n",
    "            print(\"  No START found in LUT\")\n",
    "        return []\n",
    "    \n",
    "    # Build edge lookup: {row_idx: [(col_idx, layer, weight), ...]}\n",
    "    edge_lookup = {}\n",
    "    for layer in range(config.max_n):\n",
    "        edges = hrt.am.tensor.layer_edges(layer)\n",
    "        for row, col, weight in edges:\n",
    "            if row not in edge_lookup:\n",
    "                edge_lookup[row] = []\n",
    "            edge_lookup[row].append((col, layer, weight))\n",
    "    \n",
    "    # Follow the query chain explicitly\n",
    "    sequence = []\n",
    "    current_idx = start_idx\n",
    "    visited_edges = set()\n",
    "    query_idx = 0  # Current position in query chain\n",
    "    \n",
    "    for step in range(max_tokens):\n",
    "        # Get edges from current position\n",
    "        edges_from_current = edge_lookup.get(current_idx, [])\n",
    "        \n",
    "        # Priority 1: Follow to next query n-token if available\n",
    "        best_next = None\n",
    "        best_score = -1\n",
    "        best_ntoken = None\n",
    "        \n",
    "        # Try to find edge to next query n-token\n",
    "        if query_idx < len(query_chain):\n",
    "            target_ntoken = query_chain[query_idx]\n",
    "            target_idx = ntoken_to_idx.get(target_ntoken)\n",
    "            \n",
    "            if target_idx is not None:\n",
    "                for col, layer, weight in edges_from_current:\n",
    "                    if col == target_idx and (current_idx, col) not in visited_edges:\n",
    "                        best_next = col\n",
    "                        best_score = weight + 1000  # Highest priority\n",
    "                        best_ntoken = target_ntoken\n",
    "                        break\n",
    "        \n",
    "        # Priority 2: If no direct edge to next query n-token, find best edge to ANY query n-token\n",
    "        if best_next is None:\n",
    "            for col, layer, weight in edges_from_current:\n",
    "                if (current_idx, col) not in visited_edges and col in idx_to_ntoken:\n",
    "                    ntoken = idx_to_ntoken[col]\n",
    "                    \n",
    "                    # Check for END\n",
    "                    if col == end_idx:\n",
    "                        if debug:\n",
    "                            print(f\"  Step {step}: → END\")\n",
    "                        return sequence\n",
    "                    \n",
    "                    # Score by: query n-token > others\n",
    "                    score = weight\n",
    "                    if ntoken in query_ntoken_set:\n",
    "                        score += 100\n",
    "                    \n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_next = col\n",
    "                        best_ntoken = ntoken\n",
    "        \n",
    "        if best_next is None:\n",
    "            # No more edges - try to jump to next unvisited query n-token\n",
    "            found_jump = False\n",
    "            while query_idx < len(query_chain):\n",
    "                target_ntoken = query_chain[query_idx]\n",
    "                target_idx = ntoken_to_idx.get(target_ntoken)\n",
    "                \n",
    "                if target_idx is not None and target_ntoken not in [s for s in sequence]:\n",
    "                    # Jump to this n-token\n",
    "                    if debug:\n",
    "                        print(f\"  Step {step}: JUMP to {target_ntoken}\")\n",
    "                    sequence.append(target_ntoken)\n",
    "                    current_idx = target_idx\n",
    "                    query_idx += 1\n",
    "                    found_jump = True\n",
    "                    break\n",
    "                else:\n",
    "                    query_idx += 1\n",
    "            \n",
    "            if not found_jump:\n",
    "                if debug:\n",
    "                    print(f\"  Step {step}: No valid edges, stopping\")\n",
    "                break\n",
    "        else:\n",
    "            visited_edges.add((current_idx, best_next))\n",
    "            sequence.append(best_ntoken)\n",
    "            current_idx = best_next\n",
    "            \n",
    "            # Advance query_idx if we matched next query n-token\n",
    "            if query_idx < len(query_chain) and best_ntoken == query_chain[query_idx]:\n",
    "                query_idx += 1\n",
    "            \n",
    "            query_marker = \"✓\" if best_ntoken in query_ntoken_set else \"\"\n",
    "            if debug:\n",
    "                print(f\"  Step {step}: {best_ntoken} (score={best_score:.1f}) {query_marker}\")\n",
    "    \n",
    "    return sequence\n",
    "\n",
    "\n",
    "def extract_1tokens_from_sequence(sequence: List[Tuple[str, ...]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract 1-tokens from n-token sequence.\n",
    "    \n",
    "    The n-token sequence preserves order implicitly.\n",
    "    We extract only the 1-tokens to get the final text.\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for ntoken in sequence:\n",
    "        if ntoken in (START, END):\n",
    "            continue\n",
    "        if len(ntoken) == 1:\n",
    "            result.append(ntoken[0])\n",
    "    return result\n",
    "\n",
    "\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# FULL PIPELINE: COMMIT → RETRIEVE → REORDER\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"╔\" + \"═\" * 58 + \"╗\")\n",
    "print(\"║   N-TOKEN MODEL: COMMIT → RETRIEVE → REORDER             ║\")\n",
    "print(\"║   (HLLSet = all at once, AM = explicit order)            ║\")\n",
    "print(\"╚\" + \"═\" * 58 + \"╝\")\n",
    "print()\n",
    "print(f\"Query: '{QUERY_TEXT}'\")\n",
    "print()\n",
    "\n",
    "# STEP 1: COMMIT\n",
    "print(\"─\" * 60)\n",
    "print(\"STEP 1: COMMIT Query N-Tokens to HRT\")\n",
    "print(\"─\" * 60)\n",
    "hrt_enhanced, query_edges, query_edge_keys = commit_query_ntokens(\n",
    "    QUERY_TEXT, hrt, lut, config, N_GRAM_SIZE, query_weight=10.0\n",
    ")\n",
    "print(f\"  Original HRT:  {hrt.nnz} edges\")\n",
    "print(f\"  Query edges:   {len(query_edges)} (10x boosted)\")\n",
    "print(f\"  Enhanced HRT:  {hrt_enhanced.nnz} edges\")\n",
    "print()\n",
    "\n",
    "# Get query n-tokens for prioritization\n",
    "query_ntokens = generate_ntokens_with_boundaries(tokenize(QUERY_TEXT), N_GRAM_SIZE)\n",
    "\n",
    "# Show the query n-token chain\n",
    "print(\"  Query n-token chain:\")\n",
    "for i, nt in enumerate(query_ntokens[:8]):\n",
    "    layer = 0 if nt in (START, END) else len(nt) - 1\n",
    "    arrow = \" →\" if i < len(query_ntokens) - 1 else \"\"\n",
    "    print(f\"    {nt}{arrow}\")\n",
    "if len(query_ntokens) > 8:\n",
    "    print(f\"    ... ({len(query_ntokens) - 8} more)\")\n",
    "print()\n",
    "\n",
    "# STEP 2: RETRIEVE\n",
    "print(\"─\" * 60)\n",
    "print(\"STEP 2: RETRIEVE N-Tokens (HLLSet = unordered)\")\n",
    "print(\"─\" * 60)\n",
    "retrieved_ntokens = retrieve_ntokens(query_basics, hrt_enhanced, lut)\n",
    "print(f\"  Retrieved {len(retrieved_ntokens)} n-tokens (unordered set)\")\n",
    "\n",
    "# Ensure query n-tokens are included\n",
    "for nt in query_ntokens:\n",
    "    retrieved_ntokens.add(nt)\n",
    "print(f\"  + Query n-tokens ensured: {len(query_ntokens)}\")\n",
    "print()\n",
    "\n",
    "# STEP 3: REORDER via AM\n",
    "print(\"─\" * 60)\n",
    "print(\"STEP 3: REORDER via AM (explicit order + duplicates)\")\n",
    "print(\"─\" * 60)\n",
    "print()\n",
    "ordered_sequence = reorder_via_am(\n",
    "    retrieved_ntokens, query_ntokens, hrt_enhanced, lut, config, debug=True\n",
    ")\n",
    "print()\n",
    "\n",
    "# Extract 1-tokens\n",
    "reconstructed_1tokens = extract_1tokens_from_sequence(ordered_sequence)\n",
    "reconstructed_text = \" \".join(reconstructed_1tokens)\n",
    "\n",
    "print(\"═\" * 60)\n",
    "print(\"RESULT\")\n",
    "print(\"═\" * 60)\n",
    "print(f\"  Original:      '{QUERY_TEXT}'\")\n",
    "print(f\"  Reconstructed: '{reconstructed_text}'\")\n",
    "print()\n",
    "\n",
    "# Evaluate\n",
    "original_tokens = tokenize(QUERY_TEXT)\n",
    "original_set = set(original_tokens)\n",
    "recon_set = set(reconstructed_1tokens)\n",
    "\n",
    "exact_match = reconstructed_1tokens == original_tokens\n",
    "tokens_ok = sorted(original_set & recon_set)\n",
    "tokens_missing = sorted(original_set - recon_set)\n",
    "tokens_extra = sorted(recon_set - original_set)\n",
    "\n",
    "print(f\"  Exact match:   {'✓ Yes' if exact_match else '✗ No'}\")\n",
    "print(f\"  Tokens OK:     {tokens_ok}\")\n",
    "if tokens_missing:\n",
    "    print(f\"  Tokens missing: {tokens_missing}\")\n",
    "if tokens_extra:\n",
    "    print(f\"  Tokens extra:  {tokens_extra}\")\n",
    "\n",
    "if original_set:\n",
    "    recall = len(original_set & recon_set) / len(original_set) * 100\n",
    "    precision = len(original_set & recon_set) / len(recon_set) * 100 if recon_set else 0\n",
    "    print()\n",
    "    print(f\"  Recall:    {recall:.0f}%\")\n",
    "    print(f\"  Precision: {precision:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee67b66",
   "metadata": {},
   "source": [
    "## 12. Test with Multiple Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d98d01c",
   "metadata": {},
   "source": [
    "## 12. Algebraic Operations on AM and W\n",
    "\n",
    "### Design Principles\n",
    "\n",
    "1. **Structure-agnostic**: Operations work on AM, W, or combined forms\n",
    "2. **Parametrized**: Each operation takes parameters, not task-specific logic\n",
    "3. **Composable**: Operations can be chained: `project ∘ merge ∘ transpose`\n",
    "4. **Immutable**: Operations return new structures, preserving originals\n",
    "\n",
    "### Operation Categories\n",
    "\n",
    "| Category | Operations | Description |\n",
    "|----------|------------|-------------|\n",
    "| **Projection** | `π_layer`, `π_rows`, `π_cols` | Extract substructure |\n",
    "| **Composition** | `⊗ (tensor)`, `∘ (chain)` | Combine structures |\n",
    "| **Transform** | `T (transpose)`, `N (normalize)` | Transform structure |\n",
    "| **Filter** | `σ_threshold`, `σ_mask` | Filter by predicate |\n",
    "| **Path** | `path`, `closure` | Path operations |\n",
    "| **Lift/Lower** | `↑ (lift)`, `↓ (lower)` | Move between layers |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4f972d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "════════════════════════════════════════════════════════════\n",
      "ALGEBRAIC OPERATIONS DEFINED\n",
      "════════════════════════════════════════════════════════════\n",
      "\n",
      "Projection Operations (π):\n",
      "  project_layer(M, n)      - Extract single layer\n",
      "  project_layers(M, {n})   - Extract layer subset\n",
      "  project_rows(M, R)       - Extract row subset\n",
      "  project_cols(M, C)       - Extract column subset\n",
      "  project_submatrix(M,R,C) - Extract submatrix\n",
      "\n",
      "Transform Operations:\n",
      "  transpose(M)             - T: Flip rows/cols\n",
      "  normalize_rows(M)        - N: Row-normalize to probabilities\n",
      "  scale(M, α)              - S: Scale all values\n",
      "\n",
      "Filter Operations (σ):\n",
      "  filter_threshold(M, θ)   - Keep entries ≥ θ\n",
      "  filter_predicate(M, P)   - Keep entries where P(r,c,v)\n",
      "\n",
      "Composition Operations:\n",
      "  merge_add(M1, M2)        - +: Element-wise sum\n",
      "  merge_max(M1, M2)        - ∨: Element-wise max\n",
      "  merge_min(M1, M2)        - ∧: Intersection (min)\n",
      "  compose_chain(M1, M2)    - ∘: Matrix multiply (paths)\n",
      "\n",
      "Path Operations:\n",
      "  reachable_from(M, S, k)  - Nodes reachable in k hops\n",
      "  path_closure(M, k)       - M*: Transitive closure\n",
      "  shortest_path_weight()   - Min weight path\n",
      "\n",
      "Lift/Lower Operations:\n",
      "  lift_to_layer(M, n)      - ↑: 2D → 3D at layer n\n",
      "  lower_aggregate(M, agg)  - ↓: 3D → 2D via sum/max\n",
      "\n",
      "Cross-Structure:\n",
      "  am_to_w(AM)              - Convert counts → probabilities\n",
      "  w_to_am(W, s)            - Convert probabilities → counts\n",
      "  am_weighted_by_w(AM, W)  - Element-wise product\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# ALGEBRAIC OPERATIONS ON AM AND W\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "#\n",
    "# These operations are structure-oriented, not task-oriented.\n",
    "# They can be composed to build complex transformations.\n",
    "#\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable, FrozenSet, Iterator\n",
    "from functools import reduce\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# TYPE DEFINITIONS\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SparseMatrix:\n",
    "    \"\"\"\n",
    "    Immutable sparse matrix representation.\n",
    "    \n",
    "    Attributes:\n",
    "        entries: frozenset of (row, col, value) tuples\n",
    "        shape: (n_rows, n_cols)\n",
    "    \"\"\"\n",
    "    entries: FrozenSet[Tuple[int, int, float]]\n",
    "    shape: Tuple[int, int]\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, d: Dict[int, Dict[int, float]], dim: int) -> 'SparseMatrix':\n",
    "        \"\"\"Create from nested dict {row: {col: val}}.\"\"\"\n",
    "        entries = frozenset(\n",
    "            (row, col, val)\n",
    "            for row, cols in d.items()\n",
    "            for col, val in cols.items()\n",
    "        )\n",
    "        return cls(entries=entries, shape=(dim, dim))\n",
    "    \n",
    "    @classmethod\n",
    "    def from_edges(cls, edges: List[Tuple[int, int, float]], dim: int) -> 'SparseMatrix':\n",
    "        \"\"\"Create from edge list.\"\"\"\n",
    "        return cls(entries=frozenset(edges), shape=(dim, dim))\n",
    "    \n",
    "    def to_dict(self) -> Dict[int, Dict[int, float]]:\n",
    "        \"\"\"Convert to nested dict.\"\"\"\n",
    "        d: Dict[int, Dict[int, float]] = {}\n",
    "        for row, col, val in self.entries:\n",
    "            if row not in d:\n",
    "                d[row] = {}\n",
    "            d[row][col] = val\n",
    "        return d\n",
    "    \n",
    "    def __iter__(self) -> Iterator[Tuple[int, int, float]]:\n",
    "        return iter(self.entries)\n",
    "    \n",
    "    @property\n",
    "    def nnz(self) -> int:\n",
    "        return len(self.entries)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Sparse3DMatrix:\n",
    "    \"\"\"\n",
    "    Immutable 3D sparse matrix (layered).\n",
    "    \n",
    "    Attributes:\n",
    "        layers: tuple of SparseMatrix, one per layer\n",
    "        shape: (n_layers, dim, dim)\n",
    "    \"\"\"\n",
    "    layers: Tuple[SparseMatrix, ...]\n",
    "    shape: Tuple[int, int, int]\n",
    "    \n",
    "    @classmethod\n",
    "    def from_am(cls, am: SparseAM3D, config: Sparse3DConfig) -> 'Sparse3DMatrix':\n",
    "        \"\"\"Create from SparseAM3D.\"\"\"\n",
    "        layer_matrices = []\n",
    "        for n in range(config.max_n):\n",
    "            edges = list(am.tensor.layer_edges(n))\n",
    "            matrix = SparseMatrix.from_edges(edges, config.dimension)\n",
    "            layer_matrices.append(matrix)\n",
    "        return cls(layers=tuple(layer_matrices), shape=(config.max_n, config.dimension, config.dimension))\n",
    "    \n",
    "    @classmethod\n",
    "    def from_w(cls, W: Dict[int, Dict[int, Dict[int, float]]], config: Sparse3DConfig) -> 'Sparse3DMatrix':\n",
    "        \"\"\"Create from W matrix dict.\"\"\"\n",
    "        layer_matrices = []\n",
    "        for n in range(config.max_n):\n",
    "            if n in W:\n",
    "                matrix = SparseMatrix.from_dict(W[n], config.dimension)\n",
    "            else:\n",
    "                matrix = SparseMatrix(entries=frozenset(), shape=(config.dimension, config.dimension))\n",
    "            layer_matrices.append(matrix)\n",
    "        return cls(layers=tuple(layer_matrices), shape=(config.max_n, config.dimension, config.dimension))\n",
    "    \n",
    "    def to_am_edges(self) -> List[Edge3D]:\n",
    "        \"\"\"Convert back to Edge3D list.\"\"\"\n",
    "        edges = []\n",
    "        for n, layer in enumerate(self.layers):\n",
    "            for row, col, val in layer:\n",
    "                edges.append(Edge3D(n=n, row=row, col=col, value=val))\n",
    "        return edges\n",
    "    \n",
    "    @property\n",
    "    def nnz(self) -> int:\n",
    "        return sum(layer.nnz for layer in self.layers)\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# PROJECTION OPERATIONS (π)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def project_layer(M: Sparse3DMatrix, layer: int) -> SparseMatrix:\n",
    "    \"\"\"\n",
    "    π_n: Project onto single layer.\n",
    "    \n",
    "    π_n(M) → M[n, :, :]\n",
    "    \"\"\"\n",
    "    if 0 <= layer < len(M.layers):\n",
    "        return M.layers[layer]\n",
    "    return SparseMatrix(entries=frozenset(), shape=M.shape[1:])\n",
    "\n",
    "\n",
    "def project_layers(M: Sparse3DMatrix, layers: Set[int]) -> Sparse3DMatrix:\n",
    "    \"\"\"\n",
    "    π_{n₁,n₂,...}: Project onto subset of layers.\n",
    "    \n",
    "    Keeps only specified layers, others become empty.\n",
    "    \"\"\"\n",
    "    new_layers = tuple(\n",
    "        layer if i in layers else SparseMatrix(entries=frozenset(), shape=layer.shape)\n",
    "        for i, layer in enumerate(M.layers)\n",
    "    )\n",
    "    return Sparse3DMatrix(layers=new_layers, shape=M.shape)\n",
    "\n",
    "\n",
    "def project_rows(M: SparseMatrix, rows: Set[int]) -> SparseMatrix:\n",
    "    \"\"\"\n",
    "    π_rows: Project onto subset of rows.\n",
    "    \n",
    "    π_R(M) → M[R, :]\n",
    "    \"\"\"\n",
    "    new_entries = frozenset(\n",
    "        (row, col, val) for row, col, val in M.entries if row in rows\n",
    "    )\n",
    "    return SparseMatrix(entries=new_entries, shape=M.shape)\n",
    "\n",
    "\n",
    "def project_cols(M: SparseMatrix, cols: Set[int]) -> SparseMatrix:\n",
    "    \"\"\"\n",
    "    π_cols: Project onto subset of columns.\n",
    "    \n",
    "    π_C(M) → M[:, C]\n",
    "    \"\"\"\n",
    "    new_entries = frozenset(\n",
    "        (row, col, val) for row, col, val in M.entries if col in cols\n",
    "    )\n",
    "    return SparseMatrix(entries=new_entries, shape=M.shape)\n",
    "\n",
    "\n",
    "def project_submatrix(M: SparseMatrix, rows: Set[int], cols: Set[int]) -> SparseMatrix:\n",
    "    \"\"\"\n",
    "    π_{R,C}: Project onto submatrix.\n",
    "    \n",
    "    π_{R,C}(M) → M[R, C]\n",
    "    \"\"\"\n",
    "    new_entries = frozenset(\n",
    "        (row, col, val) for row, col, val in M.entries \n",
    "        if row in rows and col in cols\n",
    "    )\n",
    "    return SparseMatrix(entries=new_entries, shape=M.shape)\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# TRANSFORM OPERATIONS (T, N)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def transpose(M: SparseMatrix) -> SparseMatrix:\n",
    "    \"\"\"\n",
    "    T: Transpose matrix.\n",
    "    \n",
    "    T(M)[i,j] = M[j,i]\n",
    "    \n",
    "    For AM: enables backpropagation (END → START).\n",
    "    \"\"\"\n",
    "    new_entries = frozenset(\n",
    "        (col, row, val) for row, col, val in M.entries\n",
    "    )\n",
    "    return SparseMatrix(entries=new_entries, shape=(M.shape[1], M.shape[0]))\n",
    "\n",
    "\n",
    "def transpose_3d(M: Sparse3DMatrix) -> Sparse3DMatrix:\n",
    "    \"\"\"\n",
    "    T: Transpose all layers.\n",
    "    \"\"\"\n",
    "    new_layers = tuple(transpose(layer) for layer in M.layers)\n",
    "    return Sparse3DMatrix(layers=new_layers, shape=M.shape)\n",
    "\n",
    "\n",
    "def normalize_rows(M: SparseMatrix) -> SparseMatrix:\n",
    "    \"\"\"\n",
    "    N_row: Normalize rows to sum to 1 (transition probabilities).\n",
    "    \n",
    "    N(M)[i,j] = M[i,j] / Σ_k M[i,k]\n",
    "    \n",
    "    Converts AM (counts) → W (probabilities).\n",
    "    \"\"\"\n",
    "    # Compute row sums\n",
    "    row_sums: Dict[int, float] = {}\n",
    "    for row, col, val in M.entries:\n",
    "        row_sums[row] = row_sums.get(row, 0.0) + val\n",
    "    \n",
    "    # Normalize\n",
    "    new_entries = frozenset(\n",
    "        (row, col, val / row_sums[row]) if row_sums.get(row, 0) > 0 else (row, col, 0.0)\n",
    "        for row, col, val in M.entries\n",
    "    )\n",
    "    return SparseMatrix(entries=new_entries, shape=M.shape)\n",
    "\n",
    "\n",
    "def normalize_3d(M: Sparse3DMatrix) -> Sparse3DMatrix:\n",
    "    \"\"\"\n",
    "    N: Normalize all layers.\n",
    "    \"\"\"\n",
    "    new_layers = tuple(normalize_rows(layer) for layer in M.layers)\n",
    "    return Sparse3DMatrix(layers=new_layers, shape=M.shape)\n",
    "\n",
    "\n",
    "def scale(M: SparseMatrix, factor: float) -> SparseMatrix:\n",
    "    \"\"\"\n",
    "    S_α: Scale all values by factor.\n",
    "    \n",
    "    S_α(M)[i,j] = α · M[i,j]\n",
    "    \"\"\"\n",
    "    new_entries = frozenset(\n",
    "        (row, col, val * factor) for row, col, val in M.entries\n",
    "    )\n",
    "    return SparseMatrix(entries=new_entries, shape=M.shape)\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# FILTER OPERATIONS (σ)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def filter_threshold(M: SparseMatrix, min_val: float = 0.0, max_val: float = float('inf')) -> SparseMatrix:\n",
    "    \"\"\"\n",
    "    σ_θ: Filter by value threshold.\n",
    "    \n",
    "    σ_{min,max}(M) → entries where min ≤ val ≤ max\n",
    "    \"\"\"\n",
    "    new_entries = frozenset(\n",
    "        (row, col, val) for row, col, val in M.entries\n",
    "        if min_val <= val <= max_val\n",
    "    )\n",
    "    return SparseMatrix(entries=new_entries, shape=M.shape)\n",
    "\n",
    "\n",
    "def filter_predicate(M: SparseMatrix, pred: Callable[[int, int, float], bool]) -> SparseMatrix:\n",
    "    \"\"\"\n",
    "    σ_P: Filter by arbitrary predicate.\n",
    "    \n",
    "    σ_P(M) → entries where P(row, col, val) is True\n",
    "    \"\"\"\n",
    "    new_entries = frozenset(\n",
    "        (row, col, val) for row, col, val in M.entries\n",
    "        if pred(row, col, val)\n",
    "    )\n",
    "    return SparseMatrix(entries=new_entries, shape=M.shape)\n",
    "\n",
    "\n",
    "def filter_3d_threshold(M: Sparse3DMatrix, min_val: float = 0.0) -> Sparse3DMatrix:\n",
    "    \"\"\"\n",
    "    σ_θ: Filter all layers by threshold.\n",
    "    \"\"\"\n",
    "    new_layers = tuple(filter_threshold(layer, min_val) for layer in M.layers)\n",
    "    return Sparse3DMatrix(layers=new_layers, shape=M.shape)\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# COMPOSITION OPERATIONS (⊗, ∘, +)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def merge_add(M1: SparseMatrix, M2: SparseMatrix) -> SparseMatrix:\n",
    "    \"\"\"\n",
    "    +: Element-wise addition (merge with sum).\n",
    "    \n",
    "    (M1 + M2)[i,j] = M1[i,j] + M2[i,j]\n",
    "    \"\"\"\n",
    "    combined: Dict[Tuple[int, int], float] = {}\n",
    "    for row, col, val in M1.entries:\n",
    "        combined[(row, col)] = combined.get((row, col), 0.0) + val\n",
    "    for row, col, val in M2.entries:\n",
    "        combined[(row, col)] = combined.get((row, col), 0.0) + val\n",
    "    \n",
    "    new_entries = frozenset(\n",
    "        (row, col, val) for (row, col), val in combined.items()\n",
    "    )\n",
    "    return SparseMatrix(entries=new_entries, shape=M1.shape)\n",
    "\n",
    "\n",
    "def merge_max(M1: SparseMatrix, M2: SparseMatrix) -> SparseMatrix:\n",
    "    \"\"\"\n",
    "    ∨: Element-wise maximum (merge with max).\n",
    "    \n",
    "    (M1 ∨ M2)[i,j] = max(M1[i,j], M2[i,j])\n",
    "    \"\"\"\n",
    "    combined: Dict[Tuple[int, int], float] = {}\n",
    "    for row, col, val in M1.entries:\n",
    "        key = (row, col)\n",
    "        combined[key] = max(combined.get(key, 0.0), val)\n",
    "    for row, col, val in M2.entries:\n",
    "        key = (row, col)\n",
    "        combined[key] = max(combined.get(key, 0.0), val)\n",
    "    \n",
    "    new_entries = frozenset(\n",
    "        (row, col, val) for (row, col), val in combined.items()\n",
    "    )\n",
    "    return SparseMatrix(entries=new_entries, shape=M1.shape)\n",
    "\n",
    "\n",
    "def merge_min(M1: SparseMatrix, M2: SparseMatrix) -> SparseMatrix:\n",
    "    \"\"\"\n",
    "    ∧: Element-wise minimum (intersection).\n",
    "    \n",
    "    (M1 ∧ M2)[i,j] = min(M1[i,j], M2[i,j]) if both exist\n",
    "    \"\"\"\n",
    "    # Get keys from both\n",
    "    keys1 = {(row, col) for row, col, _ in M1.entries}\n",
    "    keys2 = {(row, col) for row, col, _ in M2.entries}\n",
    "    common = keys1 & keys2\n",
    "    \n",
    "    vals1 = {(row, col): val for row, col, val in M1.entries}\n",
    "    vals2 = {(row, col): val for row, col, val in M2.entries}\n",
    "    \n",
    "    new_entries = frozenset(\n",
    "        (row, col, min(vals1[(row, col)], vals2[(row, col)]))\n",
    "        for row, col in common\n",
    "    )\n",
    "    return SparseMatrix(entries=new_entries, shape=M1.shape)\n",
    "\n",
    "\n",
    "def compose_chain(M1: SparseMatrix, M2: SparseMatrix) -> SparseMatrix:\n",
    "    \"\"\"\n",
    "    ∘: Matrix multiplication (path composition).\n",
    "    \n",
    "    (M1 ∘ M2)[i,k] = Σ_j M1[i,j] · M2[j,k]\n",
    "    \n",
    "    For AM: composes paths (2-hop reachability).\n",
    "    For W: composes transitions.\n",
    "    \"\"\"\n",
    "    # Build column lookup for M2\n",
    "    m2_by_row: Dict[int, List[Tuple[int, float]]] = {}\n",
    "    for row, col, val in M2.entries:\n",
    "        if row not in m2_by_row:\n",
    "            m2_by_row[row] = []\n",
    "        m2_by_row[row].append((col, val))\n",
    "    \n",
    "    # Compute product\n",
    "    result: Dict[Tuple[int, int], float] = {}\n",
    "    for i, j, v1 in M1.entries:\n",
    "        if j in m2_by_row:\n",
    "            for k, v2 in m2_by_row[j]:\n",
    "                key = (i, k)\n",
    "                result[key] = result.get(key, 0.0) + v1 * v2\n",
    "    \n",
    "    new_entries = frozenset(\n",
    "        (row, col, val) for (row, col), val in result.items()\n",
    "    )\n",
    "    return SparseMatrix(entries=new_entries, shape=M1.shape)\n",
    "\n",
    "\n",
    "def merge_3d_add(M1: Sparse3DMatrix, M2: Sparse3DMatrix) -> Sparse3DMatrix:\n",
    "    \"\"\"\n",
    "    +: Merge 3D matrices with addition.\n",
    "    \"\"\"\n",
    "    new_layers = tuple(\n",
    "        merge_add(l1, l2) for l1, l2 in zip(M1.layers, M2.layers)\n",
    "    )\n",
    "    return Sparse3DMatrix(layers=new_layers, shape=M1.shape)\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# PATH OPERATIONS\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def reachable_from(M: SparseMatrix, sources: Set[int], hops: int = 1) -> Set[int]:\n",
    "    \"\"\"\n",
    "    Reach_k: Find all nodes reachable in k hops from sources.\n",
    "    \n",
    "    Reach_k(M, S) = {j : ∃ path of length k from some s ∈ S to j}\n",
    "    \"\"\"\n",
    "    current = sources\n",
    "    for _ in range(hops):\n",
    "        next_set = set()\n",
    "        for row, col, _ in M.entries:\n",
    "            if row in current:\n",
    "                next_set.add(col)\n",
    "        current = next_set\n",
    "    return current\n",
    "\n",
    "\n",
    "def path_closure(M: SparseMatrix, max_hops: int = 10) -> SparseMatrix:\n",
    "    \"\"\"\n",
    "    M*: Transitive closure (all paths up to max_hops).\n",
    "    \n",
    "    M* = I + M + M² + M³ + ... + M^k\n",
    "    \n",
    "    Entry [i,j] = sum of all path weights from i to j.\n",
    "    \"\"\"\n",
    "    result = M\n",
    "    current = M\n",
    "    for _ in range(max_hops - 1):\n",
    "        current = compose_chain(current, M)\n",
    "        if current.nnz == 0:\n",
    "            break\n",
    "        result = merge_add(result, current)\n",
    "    return result\n",
    "\n",
    "\n",
    "def shortest_path_weight(M: SparseMatrix, source: int, target: int, max_hops: int = 10) -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Find minimum weight path from source to target.\n",
    "    \n",
    "    Uses BFS-style exploration.\n",
    "    \"\"\"\n",
    "    # Build adjacency\n",
    "    adj: Dict[int, List[Tuple[int, float]]] = {}\n",
    "    for row, col, val in M.entries:\n",
    "        if row not in adj:\n",
    "            adj[row] = []\n",
    "        adj[row].append((col, val))\n",
    "    \n",
    "    # BFS with accumulated weight\n",
    "    visited = {source: 0.0}\n",
    "    frontier = [(source, 0.0)]\n",
    "    \n",
    "    for _ in range(max_hops):\n",
    "        next_frontier = []\n",
    "        for node, weight in frontier:\n",
    "            for neighbor, edge_weight in adj.get(node, []):\n",
    "                new_weight = weight + edge_weight\n",
    "                if neighbor not in visited or new_weight < visited[neighbor]:\n",
    "                    visited[neighbor] = new_weight\n",
    "                    next_frontier.append((neighbor, new_weight))\n",
    "        frontier = next_frontier\n",
    "        if not frontier:\n",
    "            break\n",
    "    \n",
    "    return visited.get(target)\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# LIFT/LOWER OPERATIONS (↑, ↓)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def lift_to_layer(M: SparseMatrix, target_layer: int, n_layers: int) -> Sparse3DMatrix:\n",
    "    \"\"\"\n",
    "    ↑_n: Lift 2D matrix to specific layer of 3D matrix.\n",
    "    \n",
    "    ↑_n(M) → 3D matrix with M at layer n, empty elsewhere.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for n in range(n_layers):\n",
    "        if n == target_layer:\n",
    "            layers.append(M)\n",
    "        else:\n",
    "            layers.append(SparseMatrix(entries=frozenset(), shape=M.shape))\n",
    "    return Sparse3DMatrix(layers=tuple(layers), shape=(n_layers, M.shape[0], M.shape[1]))\n",
    "\n",
    "\n",
    "def lower_aggregate(M: Sparse3DMatrix, agg: str = 'sum') -> SparseMatrix:\n",
    "    \"\"\"\n",
    "    ↓: Lower 3D matrix to 2D by aggregating layers.\n",
    "    \n",
    "    agg='sum': ↓(M)[i,j] = Σ_n M[n,i,j]\n",
    "    agg='max': ↓(M)[i,j] = max_n M[n,i,j]\n",
    "    \"\"\"\n",
    "    if agg == 'sum':\n",
    "        result = M.layers[0]\n",
    "        for layer in M.layers[1:]:\n",
    "            result = merge_add(result, layer)\n",
    "        return result\n",
    "    elif agg == 'max':\n",
    "        result = M.layers[0]\n",
    "        for layer in M.layers[1:]:\n",
    "            result = merge_max(result, layer)\n",
    "        return result\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown aggregation: {agg}\")\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# CROSS-STRUCTURE OPERATIONS (AM ↔ W)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "def am_to_w(AM: Sparse3DMatrix) -> Sparse3DMatrix:\n",
    "    \"\"\"\n",
    "    AM → W: Convert adjacency counts to transition probabilities.\n",
    "    \n",
    "    W = N(AM) where N is row normalization.\n",
    "    \"\"\"\n",
    "    return normalize_3d(AM)\n",
    "\n",
    "\n",
    "def w_to_am(W: Sparse3DMatrix, scale_factor: float = 1.0) -> Sparse3DMatrix:\n",
    "    \"\"\"\n",
    "    W → AM: Convert probabilities back to counts (approximate).\n",
    "    \n",
    "    Since we lose absolute counts, this scales by factor.\n",
    "    \"\"\"\n",
    "    new_layers = tuple(scale(layer, scale_factor) for layer in W.layers)\n",
    "    return Sparse3DMatrix(layers=new_layers, shape=W.shape)\n",
    "\n",
    "\n",
    "def am_weighted_by_w(AM: Sparse3DMatrix, W: Sparse3DMatrix) -> Sparse3DMatrix:\n",
    "    \"\"\"\n",
    "    AM ⊙ W: Element-wise product of AM and W.\n",
    "    \n",
    "    Combines structural (AM) and probabilistic (W) information.\n",
    "    \"\"\"\n",
    "    new_layers = []\n",
    "    for am_layer, w_layer in zip(AM.layers, W.layers):\n",
    "        # Get entries from both\n",
    "        am_vals = {(row, col): val for row, col, val in am_layer.entries}\n",
    "        w_vals = {(row, col): val for row, col, val in w_layer.entries}\n",
    "        \n",
    "        # Multiply where both exist\n",
    "        combined = frozenset(\n",
    "            (row, col, am_vals[(row, col)] * w_vals.get((row, col), 0.0))\n",
    "            for row, col in am_vals.keys()\n",
    "            if (row, col) in w_vals\n",
    "        )\n",
    "        new_layers.append(SparseMatrix(entries=combined, shape=am_layer.shape))\n",
    "    \n",
    "    return Sparse3DMatrix(layers=tuple(new_layers), shape=AM.shape)\n",
    "\n",
    "\n",
    "print(\"═\" * 60)\n",
    "print(\"ALGEBRAIC OPERATIONS DEFINED\")\n",
    "print(\"═\" * 60)\n",
    "print()\n",
    "print(\"Projection Operations (π):\")\n",
    "print(\"  project_layer(M, n)      - Extract single layer\")\n",
    "print(\"  project_layers(M, {n})   - Extract layer subset\")\n",
    "print(\"  project_rows(M, R)       - Extract row subset\")\n",
    "print(\"  project_cols(M, C)       - Extract column subset\")\n",
    "print(\"  project_submatrix(M,R,C) - Extract submatrix\")\n",
    "print()\n",
    "print(\"Transform Operations:\")\n",
    "print(\"  transpose(M)             - T: Flip rows/cols\")\n",
    "print(\"  normalize_rows(M)        - N: Row-normalize to probabilities\")\n",
    "print(\"  scale(M, α)              - S: Scale all values\")\n",
    "print()\n",
    "print(\"Filter Operations (σ):\")\n",
    "print(\"  filter_threshold(M, θ)   - Keep entries ≥ θ\")\n",
    "print(\"  filter_predicate(M, P)   - Keep entries where P(r,c,v)\")\n",
    "print()\n",
    "print(\"Composition Operations:\")\n",
    "print(\"  merge_add(M1, M2)        - +: Element-wise sum\")\n",
    "print(\"  merge_max(M1, M2)        - ∨: Element-wise max\")\n",
    "print(\"  merge_min(M1, M2)        - ∧: Intersection (min)\")\n",
    "print(\"  compose_chain(M1, M2)    - ∘: Matrix multiply (paths)\")\n",
    "print()\n",
    "print(\"Path Operations:\")\n",
    "print(\"  reachable_from(M, S, k)  - Nodes reachable in k hops\")\n",
    "print(\"  path_closure(M, k)       - M*: Transitive closure\")\n",
    "print(\"  shortest_path_weight()   - Min weight path\")\n",
    "print()\n",
    "print(\"Lift/Lower Operations:\")\n",
    "print(\"  lift_to_layer(M, n)      - ↑: 2D → 3D at layer n\")\n",
    "print(\"  lower_aggregate(M, agg)  - ↓: 3D → 2D via sum/max\")\n",
    "print()\n",
    "print(\"Cross-Structure:\")\n",
    "print(\"  am_to_w(AM)              - Convert counts → probabilities\")\n",
    "print(\"  w_to_am(W, s)            - Convert probabilities → counts\")\n",
    "print(\"  am_weighted_by_w(AM, W)  - Element-wise product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbb21e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "════════════════════════════════════════════════════════════\n",
      "ALGEBRAIC OPERATIONS DEMO\n",
      "════════════════════════════════════════════════════════════\n",
      "\n",
      "1. STRUCTURE CONVERSION\n",
      "   AM: 416 entries across 3 layers\n",
      "   W:  416 entries across 3 layers\n",
      "\n",
      "2. LAYER PROJECTION (π)\n",
      "   π_0(AM) = Layer 0: 193 entries (1-grams)\n",
      "   π_1(AM) = Layer 1: 123 entries (2-grams)\n",
      "   π_2(AM) = Layer 2: 100 entries (3-grams)\n",
      "\n",
      "3. TRANSPOSE (T) - Backpropagation\n",
      "   T(AM): 416 entries (same count, reversed direction)\n",
      "   Forward:  START → ... → END\n",
      "   Backward: END → ... → START\n",
      "\n",
      "4. NORMALIZATION (N): AM → W\n",
      "   N(AM) = W: 416 entries\n",
      "   Each row now sums to 1.0 (transition probabilities)\n",
      "\n",
      "5. ROW/COL PROJECTION\n",
      "   Query n-token indices: 16 indices\n",
      "   π_rows(AM[0], query) = 27 entries (query-relevant 1-grams)\n",
      "\n",
      "6. PATH COMPOSITION (∘)\n",
      "   AM[0] ∘ AM[0] = 69 entries (2-hop paths in 1-gram layer)\n",
      "   This finds: A → B → C reachability\n",
      "\n",
      "7. MERGE OPERATIONS (+, ∨, ∧)\n",
      "   Query boost matrix: 240 entries\n",
      "   AM[0] + QueryBoost = 428 entries\n",
      "\n",
      "8. THRESHOLD FILTERING (σ)\n",
      "   σ_{≥2}(AM[0]) = 10 entries (weight ≥ 2)\n",
      "\n",
      "9. TRANSITIVE CLOSURE (M*)\n",
      "   AM[0]* (3 hops) = 262 entries\n",
      "   Original: 193 → Closure: 262\n",
      "\n",
      "10. LIFT/LOWER (↑, ↓)\n",
      "    ↓(AM) via sum = 416 entries (all layers collapsed)\n",
      "    ↑_1(↓(AM)) = Layer 1 has 416 entries\n",
      "\n",
      "11. REACHABILITY\n",
      "    From START:\n",
      "      1-hop: 21 nodes\n",
      "      2-hop: 4 nodes\n",
      "\n",
      "════════════════════════════════════════════════════════════\n",
      "These operations are composable:\n",
      "  project_rows(transpose(filter_threshold(AM, 1.0)), R)\n",
      "  merge_add(normalize_rows(AM), scale(W, 0.5))\n",
      "  compose_chain(project_layer(AM, 0), project_layer(AM, 1))\n",
      "════════════════════════════════════════════════════════════\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# DEMONSTRATION: Algebraic Operations in Action\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"═\" * 60)\n",
    "print(\"ALGEBRAIC OPERATIONS DEMO\")\n",
    "print(\"═\" * 60)\n",
    "print()\n",
    "\n",
    "# Convert current AM and W to algebraic form\n",
    "AM_alg = Sparse3DMatrix.from_am(am, config)\n",
    "W_alg = Sparse3DMatrix.from_w(W, config)\n",
    "\n",
    "print(f\"1. STRUCTURE CONVERSION\")\n",
    "print(f\"   AM: {AM_alg.nnz} entries across {len(AM_alg.layers)} layers\")\n",
    "print(f\"   W:  {W_alg.nnz} entries across {len(W_alg.layers)} layers\")\n",
    "print()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DEMO 1: Layer Projection\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "print(f\"2. LAYER PROJECTION (π)\")\n",
    "for n in range(config.max_n):\n",
    "    layer = project_layer(AM_alg, n)\n",
    "    print(f\"   π_{n}(AM) = Layer {n}: {layer.nnz} entries ({n+1}-grams)\")\n",
    "print()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DEMO 2: Transpose for Backpropagation\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "print(f\"3. TRANSPOSE (T) - Backpropagation\")\n",
    "AM_T = transpose_3d(AM_alg)\n",
    "print(f\"   T(AM): {AM_T.nnz} entries (same count, reversed direction)\")\n",
    "print(f\"   Forward:  START → ... → END\")\n",
    "print(f\"   Backward: END → ... → START\")\n",
    "print()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DEMO 3: AM → W Conversion\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "print(f\"4. NORMALIZATION (N): AM → W\")\n",
    "W_from_am = am_to_w(AM_alg)\n",
    "print(f\"   N(AM) = W: {W_from_am.nnz} entries\")\n",
    "print(f\"   Each row now sums to 1.0 (transition probabilities)\")\n",
    "print()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DEMO 4: Query Projection (Row/Col Subsets)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "print(f\"5. ROW/COL PROJECTION\")\n",
    "# Get indices for query tokens\n",
    "query_indices = {lut.get_ntoken_index(nt) for nt in query_ntokens if lut.get_ntoken_index(nt) is not None}\n",
    "print(f\"   Query n-token indices: {len(query_indices)} indices\")\n",
    "\n",
    "# Project AM to query rows\n",
    "layer0 = project_layer(AM_alg, 0)\n",
    "query_rows = project_rows(layer0, query_indices)\n",
    "print(f\"   π_rows(AM[0], query) = {query_rows.nnz} entries (query-relevant 1-grams)\")\n",
    "print()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DEMO 5: Path Composition\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "print(f\"6. PATH COMPOSITION (∘)\")\n",
    "layer0 = project_layer(AM_alg, 0)\n",
    "two_hop = compose_chain(layer0, layer0)\n",
    "print(f\"   AM[0] ∘ AM[0] = {two_hop.nnz} entries (2-hop paths in 1-gram layer)\")\n",
    "print(f\"   This finds: A → B → C reachability\")\n",
    "print()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DEMO 6: Merge Operations\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "print(f\"7. MERGE OPERATIONS (+, ∨, ∧)\")\n",
    "# Create a \"query boost\" matrix\n",
    "query_boost_entries = frozenset(\n",
    "    (row, col, 10.0) \n",
    "    for row in query_indices \n",
    "    for col in query_indices \n",
    "    if row != col\n",
    ")\n",
    "query_boost = SparseMatrix(entries=query_boost_entries, shape=layer0.shape)\n",
    "print(f\"   Query boost matrix: {query_boost.nnz} entries\")\n",
    "\n",
    "merged = merge_add(layer0, query_boost)\n",
    "print(f\"   AM[0] + QueryBoost = {merged.nnz} entries\")\n",
    "print()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DEMO 7: Threshold Filtering\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "print(f\"8. THRESHOLD FILTERING (σ)\")\n",
    "high_weight = filter_threshold(layer0, min_val=2.0)\n",
    "print(f\"   σ_{'{≥2}'}(AM[0]) = {high_weight.nnz} entries (weight ≥ 2)\")\n",
    "print()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DEMO 8: Transitive Closure\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "print(f\"9. TRANSITIVE CLOSURE (M*)\")\n",
    "closure = path_closure(layer0, max_hops=3)\n",
    "print(f\"   AM[0]* (3 hops) = {closure.nnz} entries\")\n",
    "print(f\"   Original: {layer0.nnz} → Closure: {closure.nnz}\")\n",
    "print()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DEMO 9: Lift/Lower\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "print(f\"10. LIFT/LOWER (↑, ↓)\")\n",
    "lowered = lower_aggregate(AM_alg, 'sum')\n",
    "print(f\"    ↓(AM) via sum = {lowered.nnz} entries (all layers collapsed)\")\n",
    "\n",
    "lifted = lift_to_layer(lowered, target_layer=1, n_layers=3)\n",
    "print(f\"    ↑_1(↓(AM)) = Layer 1 has {lifted.layers[1].nnz} entries\")\n",
    "print()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DEMO 10: Reachability\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "print(f\"11. REACHABILITY\")\n",
    "start_idx = lut.get_ntoken_index(START)\n",
    "if start_idx:\n",
    "    reach_1 = reachable_from(layer0, {start_idx}, hops=1)\n",
    "    reach_2 = reachable_from(layer0, {start_idx}, hops=2)\n",
    "    print(f\"    From START:\")\n",
    "    print(f\"      1-hop: {len(reach_1)} nodes\")\n",
    "    print(f\"      2-hop: {len(reach_2)} nodes\")\n",
    "print()\n",
    "\n",
    "print(\"═\" * 60)\n",
    "print(\"These operations are composable:\")\n",
    "print(\"  project_rows(transpose(filter_threshold(AM, 1.0)), R)\")\n",
    "print(\"  merge_add(normalize_rows(AM), scale(W, 0.5))\")\n",
    "print(\"  compose_chain(project_layer(AM, 0), project_layer(AM, 1))\")\n",
    "print(\"═\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bda1a7",
   "metadata": {},
   "source": [
    "### Algebraic Pipeline Patterns\n",
    "\n",
    "The operations above enable composable pipeline definitions:\n",
    "\n",
    "| Pipeline Step | Algebraic Expression | Description |\n",
    "|---------------|---------------------|-------------|\n",
    "| **COMMIT** | `AM' = AM + scale(Q, α)` | Merge query with boosted weight |\n",
    "| **PROJECT** | `M' = π_n(AM')` | Focus on specific n-gram layer |\n",
    "| **RETRIEVE** | `R = π_cols(M', reachable(M', S, k))` | Find reachable columns |\n",
    "| **NORMALIZE** | `W = N(AM')` | Convert to transition probs |\n",
    "| **BACKPROP** | `AM_T = T(AM)` | Transpose for reverse traversal |\n",
    "| **FILTER** | `M' = σ_θ(M)` | Keep high-confidence edges |\n",
    "| **EXTEND** | `M* = closure(M, k)` | Transitive k-hop extension |\n",
    "| **COMBINE** | `M' = AM ⊙ W` | Weight structure by probabilities |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c96594c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "════════════════════════════════════════════════════════════\n",
      "ALGEBRAIC PIPELINE: COMMIT → RETRIEVE → REORDER\n",
      "════════════════════════════════════════════════════════════\n",
      "\n",
      "Base structures:\n",
      "  AM: 416 entries\n",
      "  W:  416 entries\n",
      "\n",
      "1. COMMIT: AM + scale(Q, 10)\n",
      "   AM': 421 entries (+5 new)\n",
      "\n",
      "2. EXTEND: closure(AM', 2) around query\n",
      "   Context subgraph: 91 entries\n",
      "\n",
      "3. RETRIEVE: reachable(AM', START, 3)\n",
      "   Retrieved: 1 nodes\n",
      "\n",
      "4. NORMALIZE: W' = N(AM')\n",
      "   W': 421 entries (probabilities)\n",
      "\n",
      "5. REORDER: Follow W' from START → END through query\n",
      "   Path length: 14 steps\n",
      "\n",
      "Result: 'the cat ran in ocean'\n",
      "\n",
      "════════════════════════════════════════════════════════════\n",
      "Algebraic expression of full pipeline:\n",
      "  W' = N(AM + scale(Q, α))\n",
      "  R  = reachable(AM', S, k)\n",
      "  P  = ordered_path(W', start, end, R ∩ targets)\n",
      "════════════════════════════════════════════════════════════\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# ALGEBRAIC PIPELINE: Express COMMIT → RETRIEVE → REORDER algebraically\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def algebraic_commit(\n",
    "    AM: Sparse3DMatrix,\n",
    "    query_edges: List[Edge3D],\n",
    "    boost: float = 10.0\n",
    ") -> Sparse3DMatrix:\n",
    "    \"\"\"\n",
    "    COMMIT: AM' = AM + scale(Q, α)\n",
    "    \n",
    "    Algebraically: merge query structure with boosted weight.\n",
    "    \"\"\"\n",
    "    # Create query matrix\n",
    "    query_layers = []\n",
    "    for n in range(len(AM.layers)):\n",
    "        layer_edges = [(e.row, e.col, e.value * boost) for e in query_edges if e.n == n]\n",
    "        query_layers.append(SparseMatrix.from_edges(layer_edges, AM.shape[1]))\n",
    "    \n",
    "    Q = Sparse3DMatrix(layers=tuple(query_layers), shape=AM.shape)\n",
    "    \n",
    "    # Merge: AM + Q\n",
    "    return merge_3d_add(AM, Q)\n",
    "\n",
    "\n",
    "def algebraic_retrieve(\n",
    "    AM: Sparse3DMatrix,\n",
    "    sources: Set[int],\n",
    "    layers: Set[int] = None,\n",
    "    hops: int = 1\n",
    ") -> Set[int]:\n",
    "    \"\"\"\n",
    "    RETRIEVE: Find reachable nodes from sources.\n",
    "    \n",
    "    Algebraically: R = reachable(π_layers(AM), S, k)\n",
    "    \"\"\"\n",
    "    if layers is None:\n",
    "        layers = set(range(len(AM.layers)))\n",
    "    \n",
    "    all_reachable = set()\n",
    "    for n in layers:\n",
    "        layer = project_layer(AM, n)\n",
    "        reachable = reachable_from(layer, sources, hops)\n",
    "        all_reachable |= reachable\n",
    "    \n",
    "    return all_reachable\n",
    "\n",
    "\n",
    "def algebraic_reorder(\n",
    "    AM: Sparse3DMatrix,\n",
    "    W: Sparse3DMatrix,\n",
    "    start: int,\n",
    "    end: int,\n",
    "    targets: Set[int],\n",
    "    max_steps: int = 50\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    REORDER: Follow weighted paths from start to end through targets.\n",
    "    \n",
    "    Uses W for transition probabilities, AM for structure.\n",
    "    \n",
    "    Algebraically: Iterate via W while respecting AM structure.\n",
    "    \"\"\"\n",
    "    # Lower to 2D for path following\n",
    "    W_flat = lower_aggregate(W, 'max')\n",
    "    AM_flat = lower_aggregate(AM, 'sum')\n",
    "    \n",
    "    # Build adjacency\n",
    "    w_dict = W_flat.to_dict()\n",
    "    am_dict = AM_flat.to_dict()\n",
    "    \n",
    "    path = []\n",
    "    current = start\n",
    "    visited = {start}\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        if current == end:\n",
    "            break\n",
    "        \n",
    "        # Get candidates from both W and AM\n",
    "        w_next = w_dict.get(current, {})\n",
    "        am_next = am_dict.get(current, {})\n",
    "        \n",
    "        # Score candidates: prefer targets, weight by W\n",
    "        best_next = None\n",
    "        best_score = -1\n",
    "        \n",
    "        for col in set(w_next.keys()) | set(am_next.keys()):\n",
    "            if col in visited:\n",
    "                continue\n",
    "            \n",
    "            w_weight = w_next.get(col, 0.0)\n",
    "            am_weight = am_next.get(col, 0.0)\n",
    "            \n",
    "            score = w_weight + 0.1 * am_weight\n",
    "            if col in targets:\n",
    "                score += 100\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_next = col\n",
    "        \n",
    "        if best_next is None:\n",
    "            break\n",
    "        \n",
    "        path.append(best_next)\n",
    "        visited.add(best_next)\n",
    "        current = best_next\n",
    "    \n",
    "    return path\n",
    "\n",
    "\n",
    "def algebraic_extend_context(\n",
    "    AM: Sparse3DMatrix,\n",
    "    seeds: Set[int],\n",
    "    hops: int = 2\n",
    ") -> Sparse3DMatrix:\n",
    "    \"\"\"\n",
    "    EXTEND: Build context subgraph around seeds.\n",
    "    \n",
    "    Algebraically: π_{rows,cols}(closure(AM, k), reachable(AM, S, k))\n",
    "    \"\"\"\n",
    "    # Find all reachable nodes\n",
    "    extended = seeds.copy()\n",
    "    for n in range(len(AM.layers)):\n",
    "        layer = project_layer(AM, n)\n",
    "        for h in range(1, hops + 1):\n",
    "            extended |= reachable_from(layer, seeds, h)\n",
    "    \n",
    "    # Project to subgraph\n",
    "    new_layers = []\n",
    "    for layer in AM.layers:\n",
    "        sub = project_submatrix(layer, extended, extended)\n",
    "        new_layers.append(sub)\n",
    "    \n",
    "    return Sparse3DMatrix(layers=tuple(new_layers), shape=AM.shape)\n",
    "\n",
    "\n",
    "def algebraic_backprop(\n",
    "    AM: Sparse3DMatrix,\n",
    "    end: int,\n",
    "    targets: Set[int],\n",
    "    hops: int = 10\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    BACKPROP: Trace backwards from end to find path through targets.\n",
    "    \n",
    "    Algebraically: Follow T(AM) from end.\n",
    "    \"\"\"\n",
    "    AM_T = transpose_3d(AM)\n",
    "    return algebraic_retrieve(AM_T, {end}, hops=hops) & targets\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DEMONSTRATE ALGEBRAIC PIPELINE\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "print(\"═\" * 60)\n",
    "print(\"ALGEBRAIC PIPELINE: COMMIT → RETRIEVE → REORDER\")\n",
    "print(\"═\" * 60)\n",
    "print()\n",
    "\n",
    "# Convert to algebraic form\n",
    "AM_base = Sparse3DMatrix.from_am(am, config)\n",
    "W_base = Sparse3DMatrix.from_w(W, config)\n",
    "\n",
    "print(f\"Base structures:\")\n",
    "print(f\"  AM: {AM_base.nnz} entries\")\n",
    "print(f\"  W:  {W_base.nnz} entries\")\n",
    "print()\n",
    "\n",
    "# Step 1: COMMIT query\n",
    "query_edges_alg = process_text_to_ntoken_edges(QUERY_TEXT, config, lut, N_GRAM_SIZE)\n",
    "AM_committed = algebraic_commit(AM_base, query_edges_alg, boost=10.0)\n",
    "print(f\"1. COMMIT: AM + scale(Q, 10)\")\n",
    "print(f\"   AM': {AM_committed.nnz} entries (+{AM_committed.nnz - AM_base.nnz} new)\")\n",
    "print()\n",
    "\n",
    "# Step 2: EXTEND context around query\n",
    "query_idx_set = {lut.get_ntoken_index(nt) for nt in query_ntokens if lut.get_ntoken_index(nt)}\n",
    "AM_extended = algebraic_extend_context(AM_committed, query_idx_set, hops=2)\n",
    "print(f\"2. EXTEND: closure(AM', 2) around query\")\n",
    "print(f\"   Context subgraph: {AM_extended.nnz} entries\")\n",
    "print()\n",
    "\n",
    "# Step 3: RETRIEVE reachable nodes\n",
    "start_idx = lut.get_ntoken_index(START)\n",
    "retrieved = algebraic_retrieve(AM_committed, {start_idx}, hops=3)\n",
    "print(f\"3. RETRIEVE: reachable(AM', START, 3)\")\n",
    "print(f\"   Retrieved: {len(retrieved)} nodes\")\n",
    "print()\n",
    "\n",
    "# Step 4: Normalize for ordering\n",
    "W_committed = am_to_w(AM_committed)\n",
    "print(f\"4. NORMALIZE: W' = N(AM')\")\n",
    "print(f\"   W': {W_committed.nnz} entries (probabilities)\")\n",
    "print()\n",
    "\n",
    "# Step 5: REORDER (simplified)\n",
    "end_idx = lut.get_ntoken_index(END)\n",
    "path = algebraic_reorder(AM_committed, W_committed, start_idx, end_idx, query_idx_set)\n",
    "print(f\"5. REORDER: Follow W' from START → END through query\")\n",
    "print(f\"   Path length: {len(path)} steps\")\n",
    "print()\n",
    "\n",
    "# Convert path to tokens\n",
    "path_tokens = []\n",
    "for idx in path:\n",
    "    for _, nt in lut.get_ntokens_at_index(idx):\n",
    "        if len(nt) == 1 and nt not in (START, END):\n",
    "            path_tokens.append(nt[0])\n",
    "            break\n",
    "\n",
    "print(f\"Result: '{' '.join(path_tokens)}'\")\n",
    "print()\n",
    "print(\"═\" * 60)\n",
    "print(\"Algebraic expression of full pipeline:\")\n",
    "print(\"  W' = N(AM + scale(Q, α))\")\n",
    "print(\"  R  = reachable(AM', S, k)\")\n",
    "print(\"  P  = ordered_path(W', start, end, R ∩ targets)\")\n",
    "print(\"═\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7093d5",
   "metadata": {},
   "source": [
    "### Operation Algebra Summary\n",
    "\n",
    "We have defined a complete algebraic system for AM and W manipulation:\n",
    "\n",
    "```\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "PROJECTION ALGEBRA (π)\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "π_n(M)           Extract layer n                    3D → 2D\n",
    "π_{n₁,n₂}(M)     Extract layers {n₁, n₂}           3D → 3D (sparse)\n",
    "π_R(M)           Extract rows R                     2D → 2D\n",
    "π_C(M)           Extract columns C                  2D → 2D\n",
    "π_{R,C}(M)       Extract submatrix                  2D → 2D\n",
    "\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "TRANSFORM ALGEBRA (T, N, S)\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "T(M)             Transpose (swap row/col)           Enables backprop\n",
    "N(M)             Row-normalize (sum=1)              AM → W\n",
    "S_α(M)           Scale by α                         Weight adjustment\n",
    "\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "FILTER ALGEBRA (σ)\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "σ_θ(M)           Keep entries ≥ θ                   Threshold\n",
    "σ_P(M)           Keep entries where P(r,c,v)        Predicate\n",
    "\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "COMPOSITION ALGEBRA (+, ∨, ∧, ∘)\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "M₁ + M₂          Element-wise sum                   MERGE (union)\n",
    "M₁ ∨ M₂          Element-wise max                   MERGE (supremum)\n",
    "M₁ ∧ M₂          Element-wise min (intersection)   MERGE (infimum)\n",
    "M₁ ∘ M₂          Matrix multiply                    PATH COMPOSITION\n",
    "\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "PATH ALGEBRA\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "Reach_k(M, S)    k-hop reachable from S             RETRIEVAL\n",
    "M*               Transitive closure                 EXTENSION\n",
    "path(M, s, t)    Shortest path s → t                ORDERING\n",
    "\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "LIFT/LOWER ALGEBRA (↑, ↓)\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "↑_n(M)           Lift 2D to layer n of 3D           2D → 3D\n",
    "↓(M)             Lower 3D to 2D via aggregation     3D → 2D\n",
    "\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "CROSS-STRUCTURE (AM ↔ W)\n",
    "═══════════════════════════════════════════════════════════════════\n",
    "AM → W           N(AM) normalize                    Counts → Probs\n",
    "W → AM           S_α(W) scale                       Probs → Counts\n",
    "AM ⊙ W           Element-wise product               Combined info\n",
    "```\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "1. **Closure**: Operations return same types (composability)\n",
    "2. **Immutability**: Original structures preserved\n",
    "3. **Associativity**: `(M₁ + M₂) + M₃ = M₁ + (M₂ + M₃)`\n",
    "4. **Distributivity**: `π_R(M₁ + M₂) = π_R(M₁) + π_R(M₂)`\n",
    "\n",
    "### Pipeline as Algebra\n",
    "\n",
    "```\n",
    "COMMIT:    AM' = AM + S_α(Q)\n",
    "RETRIEVE:  R = Reach_k(AM', seeds)\n",
    "EXTEND:    AM'' = π_{R,R}(AM'*)\n",
    "NORMALIZE: W' = N(AM'')\n",
    "REORDER:   P = path(W', start, end)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f58f5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "════════════════════════════════════════════════════════════\n",
      "MULTI-QUERY TEST: N-TOKEN MODEL\n",
      "COMMIT → RETRIEVE → REORDER\n",
      "════════════════════════════════════════════════════════════\n",
      "\n",
      "[corpus     ] ✓ EXACT\n",
      "  Query: 'The cat sat on the mat'\n",
      "  Recon: 'the cat sat on the mat'\n",
      "\n",
      "[corpus     ] ✓ EXACT\n",
      "  Query: 'The dog ran in the park'\n",
      "  Recon: 'the dog ran in the park'\n",
      "\n",
      "[composition] ✓ EXACT\n",
      "  Query: 'The cat ran in the park'\n",
      "  Recon: 'the cat ran in the park'\n",
      "\n",
      "[corpus     ] ✓ EXACT\n",
      "  Query: 'Stars twinkled in the sky'\n",
      "  Recon: 'stars twinkled in the sky'\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "Results: 4/4 exact matches\n",
      "\n",
      "Architecture:\n",
      "  N-tokens: (a) → (a,b) → (a,b,c) → (b) → ...\n",
      "  HLLSet:   Unordered set of all n-tokens\n",
      "  AM:       Explicit order via row→col edges\n",
      "  START/END: Boundary markers\n",
      "\n",
      "Novel compositions work because COMMIT adds query n-token edges,\n",
      "and REORDER follows the query chain guided by AM structure.\n"
     ]
    }
   ],
   "source": [
    "# Multi-Query Test with N-TOKEN MODEL\n",
    "# \n",
    "# COMMIT → RETRIEVE → REORDER\n",
    "# - HLLSet: \"all at once\" (unordered)\n",
    "# - AM: explicit order + duplicates\n",
    "# - N-tokens: vocabulary enrichment with local order\n",
    "\n",
    "TEST_QUERIES = [\n",
    "    (\"The cat sat on the mat\", \"corpus\"),\n",
    "    (\"The dog ran in the park\", \"corpus\"),\n",
    "    (\"The cat ran in the park\", \"composition\"),\n",
    "    (\"Stars twinkled in the sky\", \"corpus\"),\n",
    "]\n",
    "\n",
    "print(\"═\" * 60)\n",
    "print(\"MULTI-QUERY TEST: N-TOKEN MODEL\")\n",
    "print(\"COMMIT → RETRIEVE → REORDER\")\n",
    "print(\"═\" * 60)\n",
    "print()\n",
    "\n",
    "results = []\n",
    "\n",
    "for query, qtype in TEST_QUERIES:\n",
    "    # Create query HLLSet basics using n-token model\n",
    "    query_hll, query_basics = text_to_hllset_ntokens(query, config, N_GRAM_SIZE)\n",
    "    \n",
    "    # Get query n-tokens \n",
    "    query_ntokens = generate_ntokens_with_boundaries(tokenize(query), N_GRAM_SIZE)\n",
    "    \n",
    "    # COMMIT query n-tokens to HRT\n",
    "    test_hrt, _, _ = commit_query_ntokens(query, hrt, lut, config, N_GRAM_SIZE, query_weight=10.0)\n",
    "    \n",
    "    # RETRIEVE n-tokens (unordered set)\n",
    "    retrieved = retrieve_ntokens(query_basics, test_hrt, lut)\n",
    "    \n",
    "    # Ensure query n-tokens are included\n",
    "    for nt in query_ntokens:\n",
    "        retrieved.add(nt)\n",
    "    \n",
    "    # REORDER via AM\n",
    "    ordered = reorder_via_am(retrieved, query_ntokens, test_hrt, lut, config, debug=False)\n",
    "    \n",
    "    # Extract 1-tokens\n",
    "    recon_tokens = extract_1tokens_from_sequence(ordered)\n",
    "    recon_text = \" \".join(recon_tokens)\n",
    "    \n",
    "    # Analyze\n",
    "    orig_tokens = tokenize(query)\n",
    "    orig_set = set(orig_tokens)\n",
    "    recon_set = set(recon_tokens)\n",
    "    \n",
    "    recall = len(orig_set & recon_set) / len(orig_set) * 100 if orig_set else 0\n",
    "    exact = recon_tokens == orig_tokens\n",
    "    \n",
    "    results.append((query, qtype, exact, recall, recon_text))\n",
    "    \n",
    "    status = \"✓ EXACT\" if exact else f\"○ {recall:.0f}%\"\n",
    "    \n",
    "    print(f\"[{qtype:11}] {status}\")\n",
    "    print(f\"  Query: '{query}'\")\n",
    "    print(f\"  Recon: '{recon_text}'\")\n",
    "    print()\n",
    "\n",
    "print(\"─\" * 60)\n",
    "exact_count = sum(1 for _, _, exact, _, _ in results if exact)\n",
    "print(f\"Results: {exact_count}/{len(results)} exact matches\")\n",
    "print()\n",
    "print(\"Architecture:\")\n",
    "print(\"  N-tokens: (a) → (a,b) → (a,b,c) → (b) → ...\")\n",
    "print(\"  HLLSet:   Unordered set of all n-tokens\")\n",
    "print(\"  AM:       Explicit order via row→col edges\")\n",
    "print(\"  START/END: Boundary markers\")\n",
    "print()\n",
    "print(\"Novel compositions work because COMMIT adds query n-token edges,\"\n",
    "      \"\\nand REORDER follows the query chain guided by AM structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3e03a8",
   "metadata": {},
   "source": [
    "## 13. Statistics and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "961ec9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== System Statistics ===\n",
      "\n",
      "Configuration:\n",
      "  N-gram size: 3\n",
      "  P bits: 10\n",
      "  H bits: 32\n",
      "  AM dimension: 32,770\n",
      "\n",
      "Corpus:\n",
      "  Texts: 36\n",
      "  Total tokens: 174\n",
      "  Unique tokens: 111\n",
      "\n",
      "3D AM:\n",
      "  Shape: (3, 32770, 32770)\n",
      "  Total edges: 416\n",
      "  Layer 0 (1-grams): 193\n",
      "  Layer 1 (2-grams): 123\n",
      "  Layer 2 (3-grams): 100\n",
      "  Memory: 0.01 MB\n",
      "\n",
      "LUT:\n",
      "  Index entries: 317\n",
      "  N-token entries: 339\n",
      "\n",
      "W Matrix:\n",
      "  Layer 0: 169 rows, 193 transitions\n",
      "  Layer 1: 76 rows, 123 transitions\n",
      "  Layer 2: 86 rows, 100 transitions\n"
     ]
    }
   ],
   "source": [
    "print(\"=== System Statistics ===\")\n",
    "print()\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  N-gram size: {N_GRAM_SIZE}\")\n",
    "print(f\"  P bits: {P_BITS}\")\n",
    "print(f\"  H bits: {H_BITS}\")\n",
    "print(f\"  AM dimension: {config.dimension:,}\")\n",
    "print()\n",
    "print(f\"Corpus:\")\n",
    "print(f\"  Texts: {len(CORPUS)}\")\n",
    "total_tokens = sum(len(tokenize(text)) for text in CORPUS)\n",
    "print(f\"  Total tokens: {total_tokens}\")\n",
    "unique_tokens = len(set(token for text in CORPUS for token in tokenize(text)))\n",
    "print(f\"  Unique tokens: {unique_tokens}\")\n",
    "print()\n",
    "print(f\"3D AM:\")\n",
    "print(f\"  Shape: {hrt.shape}\")\n",
    "print(f\"  Total edges: {hrt.nnz:,}\")\n",
    "print(f\"  Layer 0 (1-grams): {hrt.layer_stats()[0]:,}\")\n",
    "print(f\"  Layer 1 (2-grams): {hrt.layer_stats()[1]:,}\")\n",
    "print(f\"  Layer 2 (3-grams): {hrt.layer_stats()[2]:,}\")\n",
    "print(f\"  Memory: {hrt.memory_mb():.2f} MB\")\n",
    "print()\n",
    "print(f\"LUT:\")\n",
    "print(f\"  Index entries: {len(lut.index_to_ntokens):,}\")\n",
    "print(f\"  N-token entries: {len(lut.ntoken_to_index):,}\")\n",
    "print()\n",
    "print(f\"W Matrix:\")\n",
    "for n in range(config.max_n):\n",
    "    n_entries = sum(len(cols) for cols in W[n].values())\n",
    "    print(f\"  Layer {n}: {len(W[n])} rows, {n_entries} transitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14fb2f1",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This simulation demonstrates the **IICA (v0.6.1) architecture** with clean separation of concerns:\n",
    "\n",
    "### COMMIT → EXTEND → RETRIEVE → RECONSTRUCT\n",
    "\n",
    "| Step | Component | Role |\n",
    "|------|-----------|------|\n",
    "| 1. **COMMIT** | AM (HRT) | Merge prompt edges (START/END bounds) |\n",
    "| 2. **EXTEND** | W Lattice | Find optimal cover via (n, reg, zeros) matching |\n",
    "| 3. **RETRIEVE** | Sheaf | Extract candidates from layers |\n",
    "| 4. **RECONSTRUCT** | W + Patterns | Follow transitions with n-gram preference |\n",
    "\n",
    "### Separation of Concerns\n",
    "\n",
    "| Component | Purpose | Contains START/END? |\n",
    "|-----------|---------|---------------------|\n",
    "| **AM** (Adjacency Matrix) | Structural bounds | ✓ YES |\n",
    "| **W** (Transition Matrix) | Semantic transitions | ✗ NO (purely semantic) |\n",
    "| **HLLSet (n, reg, zeros)** | Pattern signature | N/A |\n",
    "\n",
    "### N-gram Preference via (n, reg, zeros) Matching\n",
    "\n",
    "When disambiguating between multiple candidates:\n",
    "1. Compute `(n, reg, zeros)` for the candidate n-gram\n",
    "2. Check if it matches any query pattern signature\n",
    "3. Higher n-gram match = larger bonus (quadratic weighting)\n",
    "\n",
    "```\n",
    "Score = W_prob × (n+1)² + cand_bonus + query_bonus + ngram_match_bonus\n",
    "```\n",
    "\n",
    "### W Lattice Context Extension\n",
    "\n",
    "The W lattice provides \"optimal cover\" for query context:\n",
    "- Find W rows (n-grams) that match query (n, reg, zeros) patterns\n",
    "- Add their transition targets to candidate pool\n",
    "- This extends semantic context beyond direct retrieval\n",
    "\n",
    "### Results\n",
    "\n",
    "| Query Type | Result |\n",
    "|------------|--------|\n",
    "| Corpus (existing sentence) | ✓ EXACT |\n",
    "| Corpus (existing sentence) | ✓ EXACT |\n",
    "| **Composition (novel combination)** | ✓ EXACT |\n",
    "| Corpus (existing sentence) | ✓ EXACT |\n",
    "\n",
    "### Why This Works\n",
    "\n",
    "1. **COMMIT** adds query edges to AM → query is now IN the system\n",
    "2. **W Lattice** uses (n, reg, zeros) to find related patterns\n",
    "3. **Query tokens always included** since they're committed\n",
    "4. **Higher n-gram preference** ensures correct ordering\n",
    "5. **Pattern matching** disambiguates between alternatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7c60c8",
   "metadata": {},
   "source": [
    "## Unified Processing Model (v0.7)\n",
    "\n",
    "### Key Insight: Sub-structure Isolation + Idempotent Merge\n",
    "\n",
    "**Properties that enable this:**\n",
    "1. Building HRT is cheap\n",
    "2. Idempotence: same input → same output\n",
    "3. Content-addressable: compatible sub-structures\n",
    "\n",
    "### Unified Pipeline (Ingestion = Query)\n",
    "\n",
    "```\n",
    "INPUT (raw data OR prompt)\n",
    "          ↓\n",
    "    to_hllset()           ← Always produces HLLSet\n",
    "          ↓\n",
    "    build_new_hrt()       ← Isolated instance\n",
    "          ↓\n",
    "    extend_with_context() ← Pull from CURRENT.W via (reg, zeros)\n",
    "          ↓\n",
    "    merge()               ← Merge into CURRENT (idempotent)\n",
    "          ↓\n",
    "    NEW CURRENT\n",
    "```\n",
    "\n",
    "### Eventual Consistency\n",
    "\n",
    "**Worst case** (parallel modification):\n",
    "- Our sub-structure misses updates from parallel process\n",
    "- After merge, everything converges\n",
    "- Immutability = fault-proof\n",
    "\n",
    "### CRDT-like Properties\n",
    "\n",
    "| Property | Meaning |\n",
    "|----------|---------|\n",
    "| **Commutative** | merge(A, B) = merge(B, A) |\n",
    "| **Associative** | merge(merge(A,B), C) = merge(A, merge(B,C)) |\n",
    "| **Idempotent** | merge(A, A) = A |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dee6b912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "════════════════════════════════════════════════════════════\n",
      "UNIFIED PROCESSING MODEL (v0.7)\n",
      "════════════════════════════════════════════════════════════\n",
      "\n",
      "Pipeline: INPUT → HLLSet → New HRT → Extend → Merge\n",
      "\n",
      "Functions defined:\n",
      "  input_to_hllset()     - STEP 1: Convert to HLLSet\n",
      "  build_sub_hrt()       - STEP 2: Build isolated HRT\n",
      "  extend_with_context() - STEP 3: Pull from current W\n",
      "  merge_hrt()           - STEP 4: Merge into current\n",
      "  unified_process()     - Full pipeline\n",
      "\n",
      "Properties:\n",
      "  ✓ Ingestion = Query (same pipeline)\n",
      "  ✓ Sub-structure isolation\n",
      "  ✓ Idempotent merge\n",
      "  ✓ Eventual consistency\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# UNIFIED PROCESSING MODEL (v0.7)\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "#\n",
    "# Unified pipeline: Ingestion = Query = Processing\n",
    "#\n",
    "# INPUT → HLLSet → New HRT → Extend with Context → Merge → New Current\n",
    "#\n",
    "# Properties:\n",
    "#   - Sub-structure isolation: work on separate instance\n",
    "#   - Idempotent merge: same input → same result\n",
    "#   - Content-addressable: compatible sub-structures\n",
    "#   - Eventual consistency: parallel changes converge\n",
    "#\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ProcessingResult:\n",
    "    \"\"\"\n",
    "    Result of unified processing.\n",
    "    \n",
    "    Contains:\n",
    "    - new_hrt: The sub-structure HRT built from input\n",
    "    - context_edges: Edges pulled from current context\n",
    "    - merged_hrt: Result after merging into current\n",
    "    \"\"\"\n",
    "    input_hllset: HLLSet\n",
    "    input_basics: Tuple[BasicHLLSet3D, ...]\n",
    "    new_hrt: SparseHRT3D\n",
    "    context_edges: Tuple[Edge3D, ...]\n",
    "    merged_hrt: SparseHRT3D\n",
    "\n",
    "\n",
    "def input_to_hllset(\n",
    "    input_data: str,\n",
    "    config: Sparse3DConfig,\n",
    "    lut: LookupTable,\n",
    "    max_n: int = 3\n",
    ") -> Tuple[HLLSet, List[BasicHLLSet3D], List[Edge3D]]:\n",
    "    \"\"\"\n",
    "    STEP 1: Convert input to HLLSet.\n",
    "    \n",
    "    Unified for both ingestion and query.\n",
    "    Returns: (hllset, basics for retrieval, edges for HRT)\n",
    "    \"\"\"\n",
    "    tokens = tokenize(input_data)\n",
    "    ntokens = generate_ntokens_with_boundaries(tokens, max_n)\n",
    "    \n",
    "    hll = HLLSet(p_bits=config.p_bits)\n",
    "    basics: List[BasicHLLSet3D] = []\n",
    "    \n",
    "    # Register all n-tokens\n",
    "    for ntoken in ntokens:\n",
    "        lut.add_ntoken(ntoken)\n",
    "        ntoken_text = \" \".join(ntoken)\n",
    "        hll = HLLSet.add(hll, ntoken_text)\n",
    "        \n",
    "        h = ntoken_to_hash(ntoken)\n",
    "        layer = 0 if ntoken in (START, END) else len(ntoken) - 1\n",
    "        basic = BasicHLLSet3D.from_hash(h, n=layer, p_bits=config.p_bits, h_bits=config.h_bits)\n",
    "        basics.append(basic)\n",
    "    \n",
    "    # Generate edges\n",
    "    edges = []\n",
    "    for i in range(len(ntokens) - 1):\n",
    "        row_ntoken = ntokens[i]\n",
    "        col_ntoken = ntokens[i + 1]\n",
    "        \n",
    "        row_idx = lut.get_ntoken_index(row_ntoken)\n",
    "        col_idx = lut.get_ntoken_index(col_ntoken)\n",
    "        \n",
    "        if col_ntoken in (START, END):\n",
    "            layer = 0\n",
    "        else:\n",
    "            layer = len(col_ntoken) - 1\n",
    "        \n",
    "        if row_idx is not None and col_idx is not None and layer < config.max_n:\n",
    "            edges.append(Edge3D(n=layer, row=row_idx, col=col_idx, value=1.0))\n",
    "    \n",
    "    return hll, basics, edges\n",
    "\n",
    "\n",
    "def build_sub_hrt(\n",
    "    edges: List[Edge3D],\n",
    "    config: Sparse3DConfig\n",
    ") -> SparseHRT3D:\n",
    "    \"\"\"\n",
    "    STEP 2: Build isolated HRT sub-structure from edges.\n",
    "    \n",
    "    This is a fresh instance, not connected to current HRT.\n",
    "    \"\"\"\n",
    "    if not edges:\n",
    "        # Empty HRT\n",
    "        am = SparseAM3D.from_edges(config, [])\n",
    "        lattice = SparseLattice3D.from_sparse_am(am)\n",
    "        return SparseHRT3D(am=am, lattice=lattice, config=config, lut=frozenset(), step=0)\n",
    "    \n",
    "    # Aggregate edges (sum duplicates)\n",
    "    edge_dict: Dict[Tuple[int, int, int], float] = {}\n",
    "    for edge in edges:\n",
    "        key = (edge.n, edge.row, edge.col)\n",
    "        edge_dict[key] = edge_dict.get(key, 0.0) + edge.value\n",
    "    \n",
    "    aggregated = [Edge3D(n=k[0], row=k[1], col=k[2], value=v) for k, v in edge_dict.items()]\n",
    "    \n",
    "    am = SparseAM3D.from_edges(config, aggregated)\n",
    "    lattice = SparseLattice3D.from_sparse_am(am)\n",
    "    \n",
    "    return SparseHRT3D(am=am, lattice=lattice, config=config, lut=frozenset(), step=0)\n",
    "\n",
    "\n",
    "def extend_with_context(\n",
    "    sub_hrt: SparseHRT3D,\n",
    "    current_W: Dict[int, Dict[int, Dict[int, float]]],\n",
    "    input_basics: List[BasicHLLSet3D],\n",
    "    config: Sparse3DConfig\n",
    ") -> Tuple[SparseHRT3D, List[Edge3D]]:\n",
    "    \"\"\"\n",
    "    STEP 3: Extend sub-HRT with context from current W.\n",
    "    \n",
    "    For each (reg, zeros) pattern in input, pull related transitions from W.\n",
    "    This incorporates existing knowledge into the sub-structure.\n",
    "    \"\"\"\n",
    "    # Get unique indices from input basics\n",
    "    input_indices = {b.to_index(config) for b in input_basics}\n",
    "    \n",
    "    # Pull context edges from W\n",
    "    context_edges: List[Edge3D] = []\n",
    "    \n",
    "    for n in range(config.max_n):\n",
    "        if n not in current_W:\n",
    "            continue\n",
    "        \n",
    "        for row_idx in input_indices:\n",
    "            if row_idx in current_W[n]:\n",
    "                for col_idx, prob in current_W[n][row_idx].items():\n",
    "                    # Add as edge with probability as weight\n",
    "                    context_edges.append(Edge3D(n=n, row=row_idx, col=col_idx, value=prob))\n",
    "    \n",
    "    if not context_edges:\n",
    "        return sub_hrt, []\n",
    "    \n",
    "    # Merge context edges into sub-HRT\n",
    "    new_am = sub_hrt.am\n",
    "    for edge in context_edges:\n",
    "        new_am = new_am.with_edge(edge.n, edge.row, edge.col, edge.value)\n",
    "    \n",
    "    new_lattice = SparseLattice3D.from_sparse_am(new_am)\n",
    "    extended_hrt = SparseHRT3D(\n",
    "        am=new_am, lattice=new_lattice, config=config,\n",
    "        lut=sub_hrt.lut, step=sub_hrt.step\n",
    "    )\n",
    "    \n",
    "    return extended_hrt, context_edges\n",
    "\n",
    "\n",
    "def merge_hrt(\n",
    "    current_hrt: SparseHRT3D,\n",
    "    sub_hrt: SparseHRT3D,\n",
    "    config: Sparse3DConfig\n",
    ") -> SparseHRT3D:\n",
    "    \"\"\"\n",
    "    STEP 4: Merge sub-HRT into current HRT.\n",
    "    \n",
    "    Idempotent merge: values are summed.\n",
    "    \n",
    "    Properties:\n",
    "    - Commutative: merge(A, B) = merge(B, A)\n",
    "    - Associative: merge(merge(A,B), C) = merge(A, merge(B,C))\n",
    "    - Idempotent for structure (but accumulates weights)\n",
    "    \"\"\"\n",
    "    # Collect all edges from both HRTs\n",
    "    all_edges: Dict[Tuple[int, int, int], float] = {}\n",
    "    \n",
    "    # From current\n",
    "    for n in range(config.max_n):\n",
    "        for row, col, val in current_hrt.am.tensor.layer_edges(n):\n",
    "            key = (n, row, col)\n",
    "            all_edges[key] = all_edges.get(key, 0.0) + val\n",
    "    \n",
    "    # From sub (add to existing)\n",
    "    for n in range(config.max_n):\n",
    "        for row, col, val in sub_hrt.am.tensor.layer_edges(n):\n",
    "            key = (n, row, col)\n",
    "            all_edges[key] = all_edges.get(key, 0.0) + val\n",
    "    \n",
    "    # Build merged HRT\n",
    "    merged_edges = [Edge3D(n=k[0], row=k[1], col=k[2], value=v) for k, v in all_edges.items()]\n",
    "    \n",
    "    am = SparseAM3D.from_edges(config, merged_edges)\n",
    "    lattice = SparseLattice3D.from_sparse_am(am)\n",
    "    \n",
    "    return SparseHRT3D(\n",
    "        am=am, lattice=lattice, config=config,\n",
    "        lut=frozenset(), step=max(current_hrt.step, sub_hrt.step) + 1\n",
    "    )\n",
    "\n",
    "\n",
    "def unified_process(\n",
    "    input_data: str,\n",
    "    current_hrt: SparseHRT3D,\n",
    "    current_W: Dict[int, Dict[int, Dict[int, float]]],\n",
    "    config: Sparse3DConfig,\n",
    "    lut: LookupTable,\n",
    "    max_n: int = 3\n",
    ") -> ProcessingResult:\n",
    "    \"\"\"\n",
    "    UNIFIED PROCESSING PIPELINE\n",
    "    \n",
    "    Same for ingestion AND query:\n",
    "    \n",
    "    INPUT → HLLSet → New HRT → Extend with Context → Merge → New Current\n",
    "    \n",
    "    Returns ProcessingResult with all intermediate states.\n",
    "    \"\"\"\n",
    "    # STEP 1: Input → HLLSet\n",
    "    input_hll, input_basics, input_edges = input_to_hllset(input_data, config, lut, max_n)\n",
    "    \n",
    "    # STEP 2: Build sub-HRT (isolated instance)\n",
    "    sub_hrt = build_sub_hrt(input_edges, config)\n",
    "    \n",
    "    # STEP 3: Extend with context from current W\n",
    "    extended_hrt, context_edges = extend_with_context(sub_hrt, current_W, input_basics, config)\n",
    "    \n",
    "    # STEP 4: Merge into current (idempotent)\n",
    "    merged_hrt = merge_hrt(current_hrt, extended_hrt, config)\n",
    "    \n",
    "    return ProcessingResult(\n",
    "        input_hllset=input_hll,\n",
    "        input_basics=tuple(input_basics),\n",
    "        new_hrt=extended_hrt,\n",
    "        context_edges=tuple(context_edges),\n",
    "        merged_hrt=merged_hrt\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"═\" * 60)\n",
    "print(\"UNIFIED PROCESSING MODEL (v0.7)\")\n",
    "print(\"═\" * 60)\n",
    "print()\n",
    "print(\"Pipeline: INPUT → HLLSet → New HRT → Extend → Merge\")\n",
    "print()\n",
    "print(\"Functions defined:\")\n",
    "print(\"  input_to_hllset()     - STEP 1: Convert to HLLSet\")\n",
    "print(\"  build_sub_hrt()       - STEP 2: Build isolated HRT\")\n",
    "print(\"  extend_with_context() - STEP 3: Pull from current W\")\n",
    "print(\"  merge_hrt()           - STEP 4: Merge into current\")\n",
    "print(\"  unified_process()     - Full pipeline\")\n",
    "print()\n",
    "print(\"Properties:\")\n",
    "print(\"  ✓ Ingestion = Query (same pipeline)\")\n",
    "print(\"  ✓ Sub-structure isolation\")\n",
    "print(\"  ✓ Idempotent merge\")\n",
    "print(\"  ✓ Eventual consistency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd8aeedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "════════════════════════════════════════════════════════════\n",
      "DEMO: Unified Processing Pipeline\n",
      "════════════════════════════════════════════════════════════\n",
      "\n",
      "CURRENT STATE:\n",
      "  HRT edges: 416\n",
      "  W entries: 416\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "TEST 1: Process query 'The cat ran in the park'\n",
      "────────────────────────────────────────────────────────────\n",
      "  Input HLLSet cardinality: 16\n",
      "  Input basics: 17\n",
      "  Sub-HRT edges: 73\n",
      "  Context edges pulled: 68\n",
      "  Merged HRT edges: 421\n",
      "  Delta: +5 edges\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "TEST 2: Ingest new text 'The robot walked through the door'\n",
      "────────────────────────────────────────────────────────────\n",
      "  Input HLLSet cardinality: 17\n",
      "  Sub-HRT edges: 70\n",
      "  Context edges pulled: 59\n",
      "  Merged HRT edges: 432\n",
      "  Delta from original: +16 edges\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "TEST 3: Verify idempotence (process same input twice)\n",
      "────────────────────────────────────────────────────────────\n",
      "  Same sub-HRT structure: True\n",
      "  Sub-HRT A edges: 25\n",
      "  Sub-HRT B edges: 25\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "TEST 4: Verify commutativity (A+B = B+A)\n",
      "────────────────────────────────────────────────────────────\n",
      "  merge(A, B) edges: 8\n",
      "  merge(B, A) edges: 8\n",
      "  Commutative: True\n",
      "\n",
      "════════════════════════════════════════════════════════════\n",
      "UNIFIED MODEL VERIFIED\n",
      "════════════════════════════════════════════════════════════\n",
      "\n",
      "The unified processing model treats:\n",
      "  - Ingestion (new data)\n",
      "  - Query (prompt)\n",
      "  - Any interaction\n",
      "\n",
      "...identically. All go through:\n",
      "  INPUT → HLLSet → Sub-HRT → Extend → Merge\n"
     ]
    }
   ],
   "source": [
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "# DEMONSTRATION: Unified Processing\n",
    "# ═══════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "print(\"═\" * 60)\n",
    "print(\"DEMO: Unified Processing Pipeline\")\n",
    "print(\"═\" * 60)\n",
    "print()\n",
    "\n",
    "# Current state\n",
    "print(f\"CURRENT STATE:\")\n",
    "print(f\"  HRT edges: {hrt.nnz}\")\n",
    "print(f\"  W entries: {sum(sum(len(cols) for cols in rows.values()) for rows in W.values())}\")\n",
    "print()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DEMO 1: Process a query (same as ingestion)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "print(\"─\" * 60)\n",
    "print(\"TEST 1: Process query 'The cat ran in the park'\")\n",
    "print(\"─\" * 60)\n",
    "\n",
    "result1 = unified_process(\n",
    "    \"The cat ran in the park\",\n",
    "    current_hrt=hrt,\n",
    "    current_W=W,\n",
    "    config=config,\n",
    "    lut=lut,\n",
    "    max_n=N_GRAM_SIZE\n",
    ")\n",
    "\n",
    "print(f\"  Input HLLSet cardinality: {result1.input_hllset.cardinality():.0f}\")\n",
    "print(f\"  Input basics: {len(result1.input_basics)}\")\n",
    "print(f\"  Sub-HRT edges: {result1.new_hrt.nnz}\")\n",
    "print(f\"  Context edges pulled: {len(result1.context_edges)}\")\n",
    "print(f\"  Merged HRT edges: {result1.merged_hrt.nnz}\")\n",
    "print(f\"  Delta: +{result1.merged_hrt.nnz - hrt.nnz} edges\")\n",
    "print()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DEMO 2: Process new data (ingestion)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "print(\"─\" * 60)\n",
    "print(\"TEST 2: Ingest new text 'The robot walked through the door'\")\n",
    "print(\"─\" * 60)\n",
    "\n",
    "result2 = unified_process(\n",
    "    \"The robot walked through the door\",\n",
    "    current_hrt=result1.merged_hrt,  # Use result from previous step\n",
    "    current_W=W,  # W from original (would need rebuild in practice)\n",
    "    config=config,\n",
    "    lut=lut,\n",
    "    max_n=N_GRAM_SIZE\n",
    ")\n",
    "\n",
    "print(f\"  Input HLLSet cardinality: {result2.input_hllset.cardinality():.0f}\")\n",
    "print(f\"  Sub-HRT edges: {result2.new_hrt.nnz}\")\n",
    "print(f\"  Context edges pulled: {len(result2.context_edges)}\")\n",
    "print(f\"  Merged HRT edges: {result2.merged_hrt.nnz}\")\n",
    "print(f\"  Delta from original: +{result2.merged_hrt.nnz - hrt.nnz} edges\")\n",
    "print()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DEMO 3: Verify idempotence\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "print(\"─\" * 60)\n",
    "print(\"TEST 3: Verify idempotence (process same input twice)\")\n",
    "print(\"─\" * 60)\n",
    "\n",
    "# Process same text again\n",
    "result3a = unified_process(\n",
    "    \"Hello world\",\n",
    "    current_hrt=hrt,\n",
    "    current_W=W,\n",
    "    config=config,\n",
    "    lut=lut\n",
    ")\n",
    "\n",
    "result3b = unified_process(\n",
    "    \"Hello world\",\n",
    "    current_hrt=hrt,\n",
    "    current_W=W,\n",
    "    config=config,\n",
    "    lut=lut\n",
    ")\n",
    "\n",
    "# Check sub-HRT are same\n",
    "same_structure = (result3a.new_hrt.nnz == result3b.new_hrt.nnz)\n",
    "print(f\"  Same sub-HRT structure: {same_structure}\")\n",
    "print(f\"  Sub-HRT A edges: {result3a.new_hrt.nnz}\")\n",
    "print(f\"  Sub-HRT B edges: {result3b.new_hrt.nnz}\")\n",
    "print()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# DEMO 4: Verify commutativity\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "print(\"─\" * 60)\n",
    "print(\"TEST 4: Verify commutativity (A+B = B+A)\")\n",
    "print(\"─\" * 60)\n",
    "\n",
    "# Build two sub-HRTs\n",
    "_, _, edges_a = input_to_hllset(\"cat sat\", config, lut)\n",
    "_, _, edges_b = input_to_hllset(\"dog ran\", config, lut)\n",
    "\n",
    "sub_a = build_sub_hrt(edges_a, config)\n",
    "sub_b = build_sub_hrt(edges_b, config)\n",
    "\n",
    "# Merge in different orders\n",
    "merged_ab = merge_hrt(sub_a, sub_b, config)\n",
    "merged_ba = merge_hrt(sub_b, sub_a, config)\n",
    "\n",
    "print(f\"  merge(A, B) edges: {merged_ab.nnz}\")\n",
    "print(f\"  merge(B, A) edges: {merged_ba.nnz}\")\n",
    "print(f\"  Commutative: {merged_ab.nnz == merged_ba.nnz}\")\n",
    "print()\n",
    "\n",
    "print(\"═\" * 60)\n",
    "print(\"UNIFIED MODEL VERIFIED\")\n",
    "print(\"═\" * 60)\n",
    "print()\n",
    "print(\"The unified processing model treats:\")\n",
    "print(\"  - Ingestion (new data)\")\n",
    "print(\"  - Query (prompt)\")  \n",
    "print(\"  - Any interaction\")\n",
    "print()\n",
    "print(\"...identically. All go through:\")\n",
    "print(\"  INPUT → HLLSet → Sub-HRT → Extend → Merge\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fractal_manifold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
