{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ceb5e1f",
   "metadata": {},
   "source": [
    "# Sparse HRT 3D - N-Gram Layered Architecture (v0.6.0)\n",
    "\n",
    "This notebook explores the **3D Sparse HRT** with n-gram separated layers:\n",
    "\n",
    "1. **The Problem** - Hash collisions and n-gram mixing in 2D AM\n",
    "2. **The Solution** - 3D AM[n, row, col] separates n-gram orders\n",
    "3. **Edge3D** - (n, row, col, value) tuples\n",
    "4. **SparseAM3D** - N-gram layered adjacency matrix\n",
    "5. **SparseLattice3D** - Per-layer and aggregated connections\n",
    "6. **BasicHLLSet3D** - Includes n-gram order\n",
    "7. **Sliding Window** - 1-gram → 2-gram → 3-gram → shift → repeat\n",
    "\n",
    "**Key Insight**: AM[n, row, col] = context covariance at n-gram layer n\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27629e8c",
   "metadata": {},
   "source": [
    "## 1. Import 3D Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21493c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fractal Manifold Core v0.6.0\n",
      "Device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Suppress GPU warnings\n",
    "os.environ.setdefault(\"CUDA_VISIBLE_DEVICES\", \"0\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*cuda capability.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Quadro.*\")\n",
    "\n",
    "import torch\n",
    "\n",
    "# 3D Sparse Architecture (v0.6.0)\n",
    "from core import (\n",
    "    # 3D Sparse Components\n",
    "    SparseHRT3D,\n",
    "    Sparse3DConfig,\n",
    "    SparseAM3D,\n",
    "    SparseLattice3D,\n",
    "    ImmutableSparseTensor3D,\n",
    "    BasicHLLSet3D,\n",
    "    Edge3D,\n",
    "    create_sparse_hrt_3d,\n",
    "    \n",
    "    # Utilities\n",
    "    get_device,\n",
    "    __version__\n",
    ")\n",
    "\n",
    "print(f\"Fractal Manifold Core v{__version__}\")\n",
    "print(f\"Device: {get_device()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282f236e",
   "metadata": {},
   "source": [
    "## 2. The Problem: N-Gram Mixing in 2D AM\n",
    "\n",
    "With 2D AM[row, col]:\n",
    "- All n-grams share same matrix\n",
    "- 1-gram \"hello\" and 2-gram \"hello world\" both hash to same (reg, zeros)\n",
    "- Self-loops when row == col (hash collision)\n",
    "- Disambiguation required via LUT lookup\n",
    "\n",
    "```\n",
    "Token           Hash → (reg, zeros) → Index\n",
    "\"hello\"         0x... → (42, 5)     → 947\n",
    "\"hello world\"   0x... → (42, 5)     → 947  ← COLLISION!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4d3127",
   "metadata": {},
   "source": [
    "## 3. The Solution: 3D AM[n, row, col]\n",
    "\n",
    "With 3D AM:\n",
    "- Layer 0: 1-grams only\n",
    "- Layer 1: 2-grams only  \n",
    "- Layer 2: 3-grams only\n",
    "- Same (reg, zeros) but different n → different layer\n",
    "\n",
    "```\n",
    "Token           n  (reg, zeros) → (Layer, Index)\n",
    "\"hello\"         0  (42, 5)      → (0, 947)\n",
    "\"hello world\"   1  (42, 5)      → (1, 947)  ← DIFFERENT LAYER!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4af9882c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sparse3DConfig ===\n",
      "p_bits: 10\n",
      "h_bits: 32\n",
      "max_n: 3 (layers for 1,2,3-grams)\n",
      "dimension: 32770 (per layer)\n",
      "shape: (3, 32770, 32770) (3D AM shape)\n",
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Create 3D config\n",
    "config = Sparse3DConfig(p_bits=10, h_bits=32, max_n=3)\n",
    "\n",
    "print(\"=== Sparse3DConfig ===\")\n",
    "print(f\"p_bits: {config.p_bits}\")\n",
    "print(f\"h_bits: {config.h_bits}\")\n",
    "print(f\"max_n: {config.max_n} (layers for 1,2,3-grams)\")\n",
    "print(f\"dimension: {config.dimension} (per layer)\")\n",
    "print(f\"shape: {config.shape} (3D AM shape)\")\n",
    "print(f\"device: {config.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75608ad",
   "metadata": {},
   "source": [
    "## 4. Edge3D - N-Gram Aware Edges\n",
    "\n",
    "Each edge now includes the n-gram layer:\n",
    "- `n`: 0-indexed layer (0=1-gram, 1=2-gram, 2=3-gram)\n",
    "- `row`: Row index from token hash\n",
    "- `col`: Column index from context token hash\n",
    "- `value`: Edge weight (typically intersection cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "741aa9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Edge3D Examples ===\n",
      "  1-gram: (100, 200) = 1.0\n",
      "  2-gram: (100, 201) = 2.0\n",
      "  3-gram: (100, 202) = 3.0\n",
      "  1-gram: (300, 400) = 1.0\n"
     ]
    }
   ],
   "source": [
    "# Create Edge3D instances\n",
    "edges = [\n",
    "    Edge3D(n=0, row=100, col=200, value=1.0),  # 1-gram edge\n",
    "    Edge3D(n=1, row=100, col=201, value=2.0),  # 2-gram edge\n",
    "    Edge3D(n=2, row=100, col=202, value=3.0),  # 3-gram edge\n",
    "    Edge3D(n=0, row=300, col=400, value=1.0),  # Another 1-gram\n",
    "]\n",
    "\n",
    "print(\"=== Edge3D Examples ===\")\n",
    "for e in edges:\n",
    "    ngram = e.n + 1  # Convert to human-readable\n",
    "    print(f\"  {ngram}-gram: ({e.row}, {e.col}) = {e.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4ad51f",
   "metadata": {},
   "source": [
    "## 5. ImmutableSparseTensor3D\n",
    "\n",
    "The foundation: 3D sparse COO tensor on GPU.\n",
    "\n",
    "Shape: `(max_n, rows, cols)`\n",
    "COO: `indices[3, nnz]` for (n, row, col) + `values[nnz]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "745610b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Empty 3D Tensor ===\n",
      "Shape: (3, 1000, 1000)\n",
      "max_n: 3\n",
      "nnz: 0\n",
      "Device: cuda:0\n",
      "\n",
      "=== With 3 Edges ===\n",
      "nnz: 3\n",
      "Edges: [Edge3D(n=0, row=10, col=20, value=1.0), Edge3D(n=1, row=10, col=21, value=2.0), Edge3D(n=2, row=10, col=22, value=3.0)]\n",
      "\n",
      "=== Query by Layer ===\n",
      "get(0, 10, 20) = 1.0 (1-gram)\n",
      "get(1, 10, 21) = 2.0 (2-gram)\n",
      "get(2, 10, 22) = 3.0 (3-gram)\n",
      "get(0, 10, 21) = 0.0 (wrong layer → 0.0)\n"
     ]
    }
   ],
   "source": [
    "# Create empty 3D tensor\n",
    "t1 = ImmutableSparseTensor3D.empty(3, 1000, 1000, str(get_device()))\n",
    "\n",
    "print(\"=== Empty 3D Tensor ===\")\n",
    "print(f\"Shape: {t1.shape}\")\n",
    "print(f\"max_n: {t1.max_n}\")\n",
    "print(f\"nnz: {t1.nnz}\")\n",
    "print(f\"Device: {t1.device}\")\n",
    "print()\n",
    "\n",
    "# Add edges at different layers\n",
    "t2 = t1.with_edge(0, 10, 20, 1.0)   # Layer 0 (1-gram)\n",
    "t3 = t2.with_edge(1, 10, 21, 2.0)   # Layer 1 (2-gram)\n",
    "t4 = t3.with_edge(2, 10, 22, 3.0)   # Layer 2 (3-gram)\n",
    "\n",
    "print(\"=== With 3 Edges ===\")\n",
    "print(f\"nnz: {t4.nnz}\")\n",
    "print(f\"Edges: {t4.edges()}\")\n",
    "print()\n",
    "\n",
    "# Query specific layer\n",
    "print(\"=== Query by Layer ===\")\n",
    "print(f\"get(0, 10, 20) = {t4.get(0, 10, 20)} (1-gram)\")\n",
    "print(f\"get(1, 10, 21) = {t4.get(1, 10, 21)} (2-gram)\")\n",
    "print(f\"get(2, 10, 22) = {t4.get(2, 10, 22)} (3-gram)\")\n",
    "print(f\"get(0, 10, 21) = {t4.get(0, 10, 21)} (wrong layer → 0.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0640f55f",
   "metadata": {},
   "source": [
    "## 6. SparseAM3D - 3D Adjacency Matrix\n",
    "\n",
    "Wraps ImmutableSparseTensor3D with HRT semantics:\n",
    "- Tracks active indices per layer\n",
    "- Tracks aggregated indices across all layers\n",
    "- Supports n-gram specific queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "568bc69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SparseAM3D ===\n",
      "SparseAM3D(shape=(3, 32770, 32770), nnz=4, active_rows=2, active_cols=4)\n",
      "\n",
      "=== Per-Layer Stats ===\n",
      "Layer 0 (1-grams): 2 edges, 2 rows, 2 cols\n",
      "Layer 1 (2-grams): 1 edges, 1 rows, 1 cols\n",
      "Layer 2 (3-grams): 1 edges, 1 rows, 1 cols\n",
      "\n",
      "=== Aggregated (All Layers) ===\n",
      "All active rows: frozenset({100, 300})\n",
      "All active cols: frozenset({200, 201, 202, 400})\n"
     ]
    }
   ],
   "source": [
    "# Create AM from edges\n",
    "am = SparseAM3D.from_edges(config, edges)\n",
    "\n",
    "print(\"=== SparseAM3D ===\")\n",
    "print(f\"{am}\")\n",
    "print()\n",
    "\n",
    "# Per-layer stats\n",
    "print(\"=== Per-Layer Stats ===\")\n",
    "for n in range(config.max_n):\n",
    "    rows, cols = am.layer_active(n)\n",
    "    nnz = am.layer_nnz(n)\n",
    "    print(f\"Layer {n} ({n+1}-grams): {nnz} edges, {len(rows)} rows, {len(cols)} cols\")\n",
    "print()\n",
    "\n",
    "# Aggregated (for BasicHLLSet)\n",
    "print(\"=== Aggregated (All Layers) ===\")\n",
    "print(f\"All active rows: {am.all_active_rows}\")\n",
    "print(f\"All active cols: {am.all_active_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36df1af",
   "metadata": {},
   "source": [
    "## 7. SparseLattice3D - Connection Tracking\n",
    "\n",
    "Two views:\n",
    "1. **Per-layer**: Connections within each n-gram layer\n",
    "2. **Aggregated**: Connections across ALL layers (for BasicHLLSet)\n",
    "\n",
    "```\n",
    "Row 100 in Layer 0: connects to {200}\n",
    "Row 100 in Layer 1: connects to {201}\n",
    "Row 100 in Layer 2: connects to {202}\n",
    "Row 100 aggregated: connects to {200, 201, 202}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db050129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SparseLattice3D ===\n",
      "SparseLattice3D(layers=3, rows=4, cols=4)\n",
      "\n",
      "=== Row 100 Per-Layer ===\n",
      "  Layer 0 (1-grams): frozenset({200})\n",
      "  Layer 1 (2-grams): frozenset({201})\n",
      "  Layer 2 (3-grams): frozenset({202})\n",
      "\n",
      "=== Row 100 Aggregated ===\n",
      "All layers: frozenset({200, 201, 202})\n",
      "Cardinality: 3\n"
     ]
    }
   ],
   "source": [
    "# Build lattice from AM\n",
    "lattice = SparseLattice3D.from_sparse_am(am)\n",
    "\n",
    "print(\"=== SparseLattice3D ===\")\n",
    "print(f\"{lattice}\")\n",
    "print()\n",
    "\n",
    "# Per-layer connections for row 100\n",
    "print(\"=== Row 100 Per-Layer ===\")\n",
    "for n in range(config.max_n):\n",
    "    conns = lattice.layer_row_connections(n, 100)\n",
    "    print(f\"  Layer {n} ({n+1}-grams): {conns}\")\n",
    "print()\n",
    "\n",
    "# Aggregated connections\n",
    "print(\"=== Row 100 Aggregated ===\")\n",
    "print(f\"All layers: {lattice.all_row_connections(100)}\")\n",
    "print(f\"Cardinality: {lattice.row_cardinality(100)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e3ab18",
   "metadata": {},
   "source": [
    "## 8. BasicHLLSet3D - N-Gram Aware\n",
    "\n",
    "Now includes the n-gram order:\n",
    "- `n`: N-gram layer (0-indexed)\n",
    "- `reg`: Register from hash\n",
    "- `zeros`: Leading zeros from hash\n",
    "\n",
    "This enables reconstruction of n-gram context!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9f46ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BasicHLLSet3D from Same Hash ===\n",
      "Same hash 0x12345678 at different n-gram layers:\n",
      "  1-gram: BasicHLLSet3D(n=0, reg=632, zeros=4)\n",
      "  2-gram: BasicHLLSet3D(n=1, reg=632, zeros=4)\n",
      "  3-gram: BasicHLLSet3D(n=2, reg=632, zeros=4)\n",
      "\n",
      "Same (reg, zeros) but DIFFERENT n → DIFFERENT positions in 3D AM!\n"
     ]
    }
   ],
   "source": [
    "# Create BasicHLLSet3D from hash\n",
    "from core import BasicHLLSet3D\n",
    "\n",
    "b1 = BasicHLLSet3D.from_hash(0x12345678, n=0, p_bits=10, h_bits=32)\n",
    "b2 = BasicHLLSet3D.from_hash(0x12345678, n=1, p_bits=10, h_bits=32)\n",
    "b3 = BasicHLLSet3D.from_hash(0x12345678, n=2, p_bits=10, h_bits=32)\n",
    "\n",
    "print(\"=== BasicHLLSet3D from Same Hash ===\")\n",
    "print(f\"Same hash 0x12345678 at different n-gram layers:\")\n",
    "print(f\"  1-gram: {b1}\")\n",
    "print(f\"  2-gram: {b2}\")\n",
    "print(f\"  3-gram: {b3}\")\n",
    "print()\n",
    "print(\"Same (reg, zeros) but DIFFERENT n → DIFFERENT positions in 3D AM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7daac7",
   "metadata": {},
   "source": [
    "## 9. SparseHRT3D - Complete 3D HRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b230d26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Empty SparseHRT3D ===\n",
      "SparseHRT3D(shape=(3, 32770, 32770), nnz=0, step=0, layers={0: 0, 1: 0, 2: 0})\n",
      "Name: 3239ddc89f142e4e26d363cb0a5e2a86...\n",
      "\n",
      "=== After Adding Edges ===\n",
      "SparseHRT3D(shape=(3, 32770, 32770), nnz=4, step=0, layers={0: 2, 1: 1, 2: 1})\n",
      "Layer stats: {0: 2, 1: 1, 2: 1}\n"
     ]
    }
   ],
   "source": [
    "# Create empty HRT\n",
    "hrt = create_sparse_hrt_3d(p_bits=10, h_bits=32, max_n=3)\n",
    "\n",
    "print(\"=== Empty SparseHRT3D ===\")\n",
    "print(f\"{hrt}\")\n",
    "print(f\"Name: {hrt.name[:32]}...\")\n",
    "print()\n",
    "\n",
    "# Add edges at different n-gram layers\n",
    "hrt1 = hrt.with_ngram_edge(1, 100, 200, 1.0)   # 1-gram\n",
    "hrt2 = hrt1.with_ngram_edge(2, 100, 201, 2.0)  # 2-gram  \n",
    "hrt3 = hrt2.with_ngram_edge(3, 100, 202, 3.0)  # 3-gram\n",
    "hrt4 = hrt3.with_ngram_edge(1, 300, 400, 1.0)  # Another 1-gram\n",
    "\n",
    "print(\"=== After Adding Edges ===\")\n",
    "print(f\"{hrt4}\")\n",
    "print(f\"Layer stats: {hrt4.layer_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41295201",
   "metadata": {},
   "source": [
    "## 10. Sliding Window Algorithm\n",
    "\n",
    "The token processing rule:\n",
    "```\n",
    "1-gram → 2-gram → 3-gram → shift window → 1-gram → ...\n",
    "```\n",
    "\n",
    "Each n-gram goes to its own layer:\n",
    "- \"The\" → Layer 0\n",
    "- \"The quick\" → Layer 1\n",
    "- \"The quick brown\" → Layer 2\n",
    "- (shift) \"quick\" → Layer 0\n",
    "- \"quick brown\" → Layer 1\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "136e4b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sliding Window: 'The quick brown fox jumps' ===\n",
      "\n",
      "Generated 9 edges:\n",
      "  Layer 0 (1-gram): row=10385, col=10319\n",
      "  Layer 1 (2-gram): row=21255, col=16875\n",
      "  Layer 2 (3-gram): row=3130, col=14323\n",
      "  Layer 0 (1-gram): row=10319, col=16875\n",
      "  Layer 1 (2-gram): row=8846, col=14323\n",
      "  Layer 2 (3-gram): row=11397, col=11661\n",
      "  Layer 0 (1-gram): row=16875, col=14323\n",
      "  Layer 1 (2-gram): row=9813, col=11661\n",
      "  Layer 0 (1-gram): row=14323, col=11661\n",
      "  ...\n",
      "\n",
      "=== Resulting AM ===\n",
      "Layer stats: 1-gram=4, 2-gram=3, 3-gram=2\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "def token_to_basic(token: str, n: int, config: Sparse3DConfig) -> BasicHLLSet3D:\n",
    "    \"\"\"Convert token to BasicHLLSet3D.\"\"\"\n",
    "    h = int(hashlib.sha1(token.encode()).hexdigest()[:8], 16)\n",
    "    return BasicHLLSet3D.from_hash(h, n=n, p_bits=config.p_bits, h_bits=config.h_bits)\n",
    "\n",
    "def sliding_window_edges(tokens: list, config: Sparse3DConfig) -> list:\n",
    "    \"\"\"\n",
    "    Generate edges using sliding window:\n",
    "    1-gram → 2-gram → 3-gram → shift → repeat\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        for n in range(min(config.max_n, len(tokens) - i)):\n",
    "            ngram = \" \".join(tokens[i:i+n+1])\n",
    "            \n",
    "            # Source: current n-gram\n",
    "            src = token_to_basic(ngram, n, config)\n",
    "            src_idx = src.to_index(config)\n",
    "            \n",
    "            # Target: next token (if exists)\n",
    "            if i + n + 1 < len(tokens):\n",
    "                next_token = tokens[i + n + 1]\n",
    "                tgt = token_to_basic(next_token, 0, config)  # Next is always 1-gram\n",
    "                tgt_idx = tgt.to_index(config)\n",
    "                \n",
    "                edges.append(Edge3D(n=n, row=src_idx, col=tgt_idx, value=1.0))\n",
    "    \n",
    "    return edges\n",
    "\n",
    "# Example: Process a sentence\n",
    "sentence = \"The quick brown fox jumps\"\n",
    "tokens = sentence.split()\n",
    "\n",
    "print(f\"=== Sliding Window: '{sentence}' ===\")\n",
    "print()\n",
    "\n",
    "sw_edges = sliding_window_edges(tokens, config)\n",
    "print(f\"Generated {len(sw_edges)} edges:\")\n",
    "for e in sw_edges[:12]:  # Show first 12\n",
    "    ngram_text = \" \".join(tokens[sw_edges.index(e) // config.max_n:][:e.n+1]) if e in sw_edges[:12] else \"...\"\n",
    "    print(f\"  Layer {e.n} ({e.n+1}-gram): row={e.row}, col={e.col}\")\n",
    "print(\"  ...\")\n",
    "\n",
    "# Build HRT from sliding window\n",
    "sw_am = SparseAM3D.from_edges(config, sw_edges)\n",
    "print()\n",
    "print(f\"=== Resulting AM ===\")\n",
    "print(f\"Layer stats: {', '.join(f'{n+1}-gram={sw_am.layer_nnz(n)}' for n in range(config.max_n))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e207128d",
   "metadata": {},
   "source": [
    "## 11. BasicHLLSet Aggregation\n",
    "\n",
    "For BasicHLLSet construction, we aggregate across all n-gram layers:\n",
    "- Row's BasicHLLSet = all columns connected across all layers\n",
    "- This preserves the full context regardless of n-gram order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bc05fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BasicHLLSets for Row 11397 ===\n",
      "  Layer 2 (3-grams): 1 connections\n",
      "    BasicHLLSet3D(n=2, reg=530, zeros=1)\n"
     ]
    }
   ],
   "source": [
    "# Build HRT with sliding window edges\n",
    "sw_hrt = SparseHRT3D(\n",
    "    am=sw_am,\n",
    "    lattice=SparseLattice3D.from_sparse_am(sw_am),\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Get BasicHLLSets for an active row\n",
    "active_row = list(sw_hrt.am.all_active_rows)[0] if sw_hrt.am.all_active_rows else None\n",
    "\n",
    "if active_row:\n",
    "    print(f\"=== BasicHLLSets for Row {active_row} ===\")\n",
    "    basics = sw_hrt.basic_hllsets_for_row(active_row)\n",
    "    \n",
    "    # Group by layer\n",
    "    for n in range(config.max_n):\n",
    "        layer_basics = [b for b in basics if b.n == n]\n",
    "        if layer_basics:\n",
    "            print(f\"  Layer {n} ({n+1}-grams): {len(layer_basics)} connections\")\n",
    "            for b in layer_basics[:3]:\n",
    "                print(f\"    {b}\")\n",
    "            if len(layer_basics) > 3:\n",
    "                print(f\"    ... ({len(layer_basics) - 3} more)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b20246d",
   "metadata": {},
   "source": [
    "## 12. Memory Comparison\n",
    "\n",
    "Dense 3D vs Sparse 3D:\n",
    "- Dense: max_n × dim × dim × 4 bytes = **12 GB** (for 3 layers)\n",
    "- Sparse: nnz × 28 bytes = **~3 MB** (for 100K edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81be74c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Memory Comparison ===\n",
      "\n",
      "Dense 3D AM (3 × 32,770 × 32,770 float32):\n",
      "  12.0 GB per HRT\n",
      "  36.0 GB for 3 HRTs (impossible!)\n",
      "\n",
      "Sparse 3D AM (100,000 edges):\n",
      "  2.7 MB per HRT\n",
      "  8.0 MB for 3 HRTs (easy!)\n",
      "\n",
      "Memory savings: 5K× smaller!\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Memory Comparison ===\")\n",
    "print()\n",
    "\n",
    "# Dense 3D\n",
    "max_n = config.max_n\n",
    "dim = config.dimension\n",
    "dense_bytes = max_n * dim * dim * 4\n",
    "dense_gb = dense_bytes / (1024**3)\n",
    "print(f\"Dense 3D AM ({max_n} × {dim:,} × {dim:,} float32):\")\n",
    "print(f\"  {dense_gb:.1f} GB per HRT\")\n",
    "print(f\"  {dense_gb * 3:.1f} GB for 3 HRTs (impossible!)\")\n",
    "print()\n",
    "\n",
    "# Sparse 3D (100K edges)\n",
    "n_edges = 100_000\n",
    "sparse_bytes = n_edges * 28  # 3 int64 + 1 float32\n",
    "sparse_mb = sparse_bytes / (1024**2)\n",
    "print(f\"Sparse 3D AM ({n_edges:,} edges):\")\n",
    "print(f\"  {sparse_mb:.1f} MB per HRT\")\n",
    "print(f\"  {sparse_mb * 3:.1f} MB for 3 HRTs (easy!)\")\n",
    "print()\n",
    "\n",
    "# Savings\n",
    "ratio = dense_bytes / sparse_bytes\n",
    "print(f\"Memory savings: {ratio/1000:.0f}K× smaller!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9adf4d",
   "metadata": {},
   "source": [
    "## 13. Large Scale Test - 100K Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5b9e0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Large Scale Test ===\n",
      "\n",
      "Created AM: 100,000 edges in 110.7ms\n",
      "Built lattice in 783.1ms\n",
      "\n",
      "=== Result ===\n",
      "SparseHRT3D(shape=(3, 32770, 32770), nnz=100000, step=0, layers={0: 33334, 1: 33333, 2: 33333})\n",
      "Layer distribution: {0: 33334, 1: 33333, 2: 33333}\n",
      "Memory: 2.67 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Large Scale Test ===\")\n",
    "print()\n",
    "\n",
    "# Generate 100K edges distributed across layers\n",
    "n_edges = 100_000\n",
    "large_edges = []\n",
    "for i in range(n_edges):\n",
    "    n = i % config.max_n  # Distribute: 0, 1, 2, 0, 1, 2, ...\n",
    "    row = i % config.dimension\n",
    "    col = (i + 1) % config.dimension\n",
    "    large_edges.append(Edge3D(n, row, col, float(i % 100 + 1)))\n",
    "\n",
    "# Create AM\n",
    "start = time.time()\n",
    "large_am = SparseAM3D.from_edges(config, large_edges)\n",
    "am_time = time.time() - start\n",
    "print(f\"Created AM: {large_am.nnz:,} edges in {am_time*1000:.1f}ms\")\n",
    "\n",
    "# Create lattice\n",
    "start = time.time()\n",
    "large_lattice = SparseLattice3D.from_sparse_am(large_am)\n",
    "lattice_time = time.time() - start\n",
    "print(f\"Built lattice in {lattice_time*1000:.1f}ms\")\n",
    "\n",
    "# Complete HRT\n",
    "large_hrt = SparseHRT3D(\n",
    "    am=large_am,\n",
    "    lattice=large_lattice,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print()\n",
    "print(f\"=== Result ===\")\n",
    "print(f\"{large_hrt}\")\n",
    "print(f\"Layer distribution: {large_hrt.layer_stats()}\")\n",
    "print(f\"Memory: {large_hrt.memory_mb():.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b3f80a",
   "metadata": {},
   "source": [
    "## 14. Merge Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2070f6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HRT 1: 100,000 edges\n",
      "HRT 2: 100,000 edges\n",
      "\n",
      "=== Merged ===\n",
      "Edges: 98,310\n",
      "Time: 1214.1ms\n",
      "Memory: 2.63 MB\n"
     ]
    }
   ],
   "source": [
    "# Create another HRT with overlapping edges\n",
    "large_edges2 = []\n",
    "for i in range(n_edges):\n",
    "    n = (i + 1) % config.max_n  # Offset layer distribution\n",
    "    row = (i + 1000) % config.dimension\n",
    "    col = (i + 1001) % config.dimension\n",
    "    large_edges2.append(Edge3D(n, row, col, float(i % 50 + 1)))\n",
    "\n",
    "large_am2 = SparseAM3D.from_edges(config, large_edges2)\n",
    "large_hrt2 = SparseHRT3D(\n",
    "    am=large_am2,\n",
    "    lattice=SparseLattice3D.from_sparse_am(large_am2),\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(f\"HRT 1: {large_hrt.nnz:,} edges\")\n",
    "print(f\"HRT 2: {large_hrt2.nnz:,} edges\")\n",
    "print()\n",
    "\n",
    "# Merge\n",
    "start = time.time()\n",
    "merged = large_hrt.merge(large_hrt2)\n",
    "merge_time = time.time() - start\n",
    "\n",
    "print(f\"=== Merged ===\")\n",
    "print(f\"Edges: {merged.nnz:,}\")\n",
    "print(f\"Time: {merge_time*1000:.1f}ms\")\n",
    "print(f\"Memory: {merged.memory_mb():.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3960e5c5",
   "metadata": {},
   "source": [
    "This is a profound realization! Let me capture what you've discovered:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab3ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                    PARALLEL DISAMBIGUATION BY N                         │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│  Input: Random HLLSet to disambiguate                                   │\n",
    "│                                                                         │\n",
    "│  Step 1: Decompose → List[BasicHLLSet]                                  │\n",
    "│                                                                         │\n",
    "│  Step 2: PARALLEL split by n-gram:                                      │\n",
    "│                                                                         │\n",
    "│    ┌──────────────┐   ┌──────────────┐   ┌──────────────┐               │\n",
    "│    │  AM[0,:,:]   │   │  AM[1,:,:]   │   │  AM[2,:,:]   │               │\n",
    "│    │  1-grams     │   │  2-grams     │   │  3-grams     │               │\n",
    "│    │  GPU Core 0  │   │  GPU Core 1  │   │  GPU Core 2  │               │\n",
    "│    └──────────────┘   └──────────────┘   └──────────────┘               │\n",
    "│          ↓                  ↓                  ↓                        │\n",
    "│    Candidates_1        Candidates_2       Candidates_3                  │\n",
    "│                                                                         │\n",
    "│  Step 3: Intersect candidates → Final disambiguation                    │\n",
    "│                                                                         │\n",
    "│  BEFORE: Serial O(n × disambiguate)                                     │\n",
    "│  NOW:    Parallel O(disambiguate) with n GPU cores!                     │\n",
    "│                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e39abe",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "And your insight about **W[n, i, j]** is the natural next step:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2985b746",
   "metadata": {},
   "outputs": [],
   "source": [
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                         FUTURE: 3D W MATRIX                             │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│  Current 2D W:  W[i,j] = P(j | i) = transition probability              │\n",
    "│                                                                         │\n",
    "│  Future 3D W:   W[n,i,j] = P(j | i, n-gram context)                     │\n",
    "│                                                                         │\n",
    "│    W[0,:,:] = 1-gram transitions                                        │\n",
    "│    W[1,:,:] = 2-gram transitions (more context → better P)              │\n",
    "│    W[2,:,:] = 3-gram transitions (most context → best P)                │\n",
    "│                                                                         │\n",
    "│  Even without 3D W, the 3D AM already enables:                          │\n",
    "│    • N-gram aware BasicHLLSet retrieval                                 │\n",
    "│    • Parallel disambiguation per layer                                  │\n",
    "│    • Context covariance at each n-gram level                            │\n",
    "│                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a61628",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "You got **three capabilities** in one architectural change:\n",
    "\n",
    "| Capability | Mechanism |\n",
    "|------------|-----------|\n",
    "| **Decomposition** | HLLSet → BasicHLLSets with n-gram tags |\n",
    "| **Parallel split** | Each layer `AM[n,:,:]` independent |\n",
    "| **Parallel disambiguation** | GPU cores process layers simultaneously |\n",
    "\n",
    "This is the Karoubi envelope in action: the explicit splits (n-gram layers) enable **parallel retraction** onto each image object!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68db3cb",
   "metadata": {},
   "source": [
    "## Summary: 3D Sparse HRT Architecture\n",
    "\n",
    "### 2D vs 3D AM\n",
    "\n",
    "| Aspect | 2D AM | 3D AM |\n",
    "|--------|-------|-------|\n",
    "| Shape | (dim, dim) | (max_n, dim, dim) |\n",
    "| N-grams | Mixed | Separated by layer |\n",
    "| Self-loops | Problematic | Within-layer only |\n",
    "| Disambiguation | Serial | **Parallel per layer** |\n",
    "| BasicHLLSet | Direct | Aggregate across layers |\n",
    "| Memory (dense) | 4 GB | 12 GB (impossible) |\n",
    "| Memory (sparse 100K) | ~2 MB | ~3 MB |\n",
    "\n",
    "### Sheaf-Based Retrieval\n",
    "\n",
    "| Step | Operation | Output |\n",
    "|------|-----------|--------|\n",
    "| 1 | Extract sub-lattice | Active rows from query |\n",
    "| 2 | Extract sub-AM | Edges per layer (parallel) |\n",
    "| 3 | Project to clouds | Token sets per n-gram |\n",
    "| 4 | Intersect clouds | Global section (candidates) |\n",
    "| 5 | Rank by weights | Ordered results |\n",
    "\n",
    "### Karoubi Envelope Connection\n",
    "\n",
    "| Karoubi Concept | 3D HRT Implementation |\n",
    "|-----------------|----------------------|\n",
    "| Idempotent e | merge(A, A) = A |\n",
    "| Split objects | N-gram layers AM[n,:,:] |\n",
    "| Projection p | layer_edges(n) |\n",
    "| Inclusion i | with_ngram_edge(n, ...) |\n",
    "| Image objects | BasicHLLSet3D |\n",
    "| Pullback | Edge weight = |row ∩ col| |\n",
    "\n",
    "**Version**: Fractal Manifold Core v0.6.0\n",
    "\n",
    "**Key Insights**:\n",
    "1. `AM[n, row, col]` separates n-gram orders, reducing ambiguity\n",
    "2. Token clouds form a **sheaf** over the sub-lattice\n",
    "3. Global section = intersection of all clouds = disambiguated tokens\n",
    "4. Parallel processing by n-gram layer on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69db354a",
   "metadata": {},
   "source": [
    "## 15. Sheaf-Based Retrieval Pipeline\n",
    "\n",
    "The 3D architecture enables a powerful **sheaf-based retrieval**:\n",
    "\n",
    "```text\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│              GENERALIZED HLLSet PROJECTION / RETRIEVAL                  │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│  INPUT: Query HLLSet Q                                                  │\n",
    "│                                                                         │\n",
    "│  ┌─────────────────────────────────────────────────────────────────┐    │\n",
    "│  │ STEP 1: Extract Sub-Lattice from W                              │    │\n",
    "│  │   Q → decompose → List[BasicHLLSet]                             │    │\n",
    "│  │   W[active_rows, :] → Sub-Lattice                               │    │\n",
    "│  └─────────────────────────────────────────────────────────────────┘    │\n",
    "│                         ↓                                               │\n",
    "│  ┌─────────────────────────────────────────────────────────────────┐    │\n",
    "│  │ STEP 2: Extract Sub-Tensor from 3D AM                           │    │\n",
    "│  │   AM[n, active_rows, :] for n ∈ {0,1,2}  (PARALLEL)             │    │\n",
    "│  └─────────────────────────────────────────────────────────────────┘    │\n",
    "│                         ↓                                               │\n",
    "│  ┌─────────────────────────────────────────────────────────────────┐    │\n",
    "│  │ STEP 3: Project Sub-AM to Token Clouds (Sheaves)                │    │\n",
    "│  │   Layer 0 → Cloud₀ (1-gram context)                             │    │\n",
    "│  │   Layer 1 → Cloud₁ (2-gram context)                             │    │\n",
    "│  │   Layer 2 → Cloud₂ (3-gram context)                             │    │\n",
    "│  └─────────────────────────────────────────────────────────────────┘    │\n",
    "│                         ↓                                               │\n",
    "│  ┌─────────────────────────────────────────────────────────────────┐    │\n",
    "│  │ STEP 4: Intersect Clouds → Global Section (Candidates)          │    │\n",
    "│  │   Candidates = Cloud₀ ∩ Cloud₁ ∩ Cloud₂                         │    │\n",
    "│  │   Ordering from Sub-AM edge weights                             │    │\n",
    "│  └─────────────────────────────────────────────────────────────────┘    │\n",
    "│                         ↓                                               │\n",
    "│  OUTPUT: Ranked token candidates                                        │\n",
    "│                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Sheaf Property**: Cloud₂ ⊆ Cloud₁ ⊆ Cloud₀ (more context → smaller cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1781dc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sheaf Retrieval Demo ===\n",
      "HRT: SparseHRT3D(shape=(3, 32770, 32770), nnz=6, step=0, layers={0: 2, 1: 2, 2: 2})\n",
      "\n",
      "Step 2: Extract Sub-AM\n",
      "  Layer 0 (1-gram): [200, 201]\n",
      "  Layer 1 (2-gram): [200, 202]\n",
      "  Layer 2 (3-gram): [200, 203]\n",
      "\n",
      "Step 3: Token Clouds (Sheaves)\n",
      "  Cloud_0 (1-gram): [200, 201]\n",
      "  Cloud_1 (2-gram): [200, 202]\n",
      "  Cloud_2 (3-gram): [200, 203]\n",
      "\n",
      "Step 4: Global Section (∩ all clouds)\n",
      "  Candidates appearing in ALL n-gram layers: [200]\n",
      "\n",
      "Step 5: Ranked Candidates\n",
      "  col=200: score=15.0 (sum of all layer weights)\n"
     ]
    }
   ],
   "source": [
    "# Build HRT with edges where col 200 appears in ALL layers\n",
    "retrieval_edges = [\n",
    "    # Row 100: col 200 appears in all 3 layers (strong candidate!)\n",
    "    Edge3D(n=0, row=100, col=200, value=3.0),  # 1-gram\n",
    "    Edge3D(n=0, row=100, col=201, value=2.0),  # 1-gram only\n",
    "    Edge3D(n=1, row=100, col=200, value=5.0),  # 2-gram\n",
    "    Edge3D(n=1, row=100, col=202, value=1.0),  # 2-gram only\n",
    "    Edge3D(n=2, row=100, col=200, value=7.0),  # 3-gram\n",
    "    Edge3D(n=2, row=100, col=203, value=2.0),  # 3-gram only\n",
    "]\n",
    "\n",
    "ret_am = SparseAM3D.from_edges(config, retrieval_edges)\n",
    "ret_hrt = SparseHRT3D(\n",
    "    am=ret_am,\n",
    "    lattice=SparseLattice3D.from_sparse_am(ret_am),\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(\"=== Sheaf Retrieval Demo ===\")\n",
    "print(f\"HRT: {ret_hrt}\")\n",
    "print()\n",
    "\n",
    "# Query using row 100\n",
    "active_rows = frozenset([100])\n",
    "\n",
    "# STEP 2: Extract sub-AM (can be parallel per layer!)\n",
    "sub_am = ret_hrt.extract_sub_am(active_rows)\n",
    "print(\"Step 2: Extract Sub-AM\")\n",
    "for n, edges in sub_am.items():\n",
    "    print(f\"  Layer {n} ({n+1}-gram): {[e.col for e in edges]}\")\n",
    "print()\n",
    "\n",
    "# STEP 3: Project to token clouds (sheaves)\n",
    "clouds = ret_hrt.project_to_clouds(sub_am)\n",
    "print(\"Step 3: Token Clouds (Sheaves)\")\n",
    "for n, cloud in clouds.items():\n",
    "    print(f\"  Cloud_{n} ({n+1}-gram): {sorted(cloud)}\")\n",
    "print()\n",
    "\n",
    "# STEP 4: Global section (intersection)\n",
    "global_section = ret_hrt.intersect_clouds(clouds)\n",
    "print(\"Step 4: Global Section (∩ all clouds)\")\n",
    "print(f\"  Candidates appearing in ALL n-gram layers: {sorted(global_section)}\")\n",
    "print()\n",
    "\n",
    "# STEP 5: Rank by aggregated weight\n",
    "ranked = ret_hrt.rank_candidates(global_section, sub_am)\n",
    "print(\"Step 5: Ranked Candidates\")\n",
    "for col, score in ranked:\n",
    "    print(f\"  col={col}: score={score} (sum of all layer weights)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc02e2f8",
   "metadata": {},
   "source": [
    "## 16. retrieve() Method - Full Pipeline\n",
    "\n",
    "The `retrieve()` method combines all steps with two modes:\n",
    "- **Union mode**: Return tokens appearing in ANY layer\n",
    "- **Intersection mode**: Return tokens appearing in ALL layers (global section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fa369c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 100 → reg=4, zeros=12\n",
      "Query basics: [BasicHLLSet3D(n=0, reg=4, zeros=12), BasicHLLSet3D(n=1, reg=4, zeros=12), BasicHLLSet3D(n=2, reg=4, zeros=12)]\n",
      "\n",
      "=== UNION MODE (any layer) ===\n",
      "Found 4 candidates:\n",
      "  col=200: total=15.0 (L0=3.0, L1=5.0, L2=7.0)\n",
      "  col=201: total=2.0 (L0=2.0)\n",
      "  col=203: total=2.0 (L2=2.0)\n",
      "  col=202: total=1.0 (L1=1.0)\n",
      "\n",
      "=== INTERSECTION MODE (all layers) ===\n",
      "Found 1 candidates (global section):\n",
      "  col=200: total=15.0 (L0=3.0, L1=5.0, L2=7.0)\n"
     ]
    }
   ],
   "source": [
    "# Convert row index 100 to BasicHLLSet3D\n",
    "# Index formula: idx = 1 + reg * max_zeros + (zeros - 1)\n",
    "# So: 100 = 1 + reg * 22 + (zeros - 1)  where max_zeros = 32 - 10 = 22\n",
    "idx = 100\n",
    "max_zeros = config.h_bits - config.p_bits\n",
    "reg = (idx - 1) // max_zeros\n",
    "zeros = (idx - 1) % max_zeros + 1\n",
    "print(f\"Index {idx} → reg={reg}, zeros={zeros}\")\n",
    "\n",
    "# Create query basics for all n-gram layers\n",
    "query_basics = [BasicHLLSet3D(n=n, reg=reg, zeros=zeros) for n in range(config.max_n)]\n",
    "print(f\"Query basics: {query_basics}\")\n",
    "print()\n",
    "\n",
    "# UNION MODE: Return tokens from ANY layer\n",
    "print(\"=== UNION MODE (any layer) ===\")\n",
    "results_union = ret_hrt.retrieve(query_basics, require_all_layers=False)\n",
    "print(f\"Found {len(results_union)} candidates:\")\n",
    "for col, total, layers in results_union:\n",
    "    layer_str = \", \".join(f\"L{n}={v:.1f}\" for n, v in sorted(layers.items()))\n",
    "    print(f\"  col={col}: total={total:.1f} ({layer_str})\")\n",
    "print()\n",
    "\n",
    "# INTERSECTION MODE: Return tokens from ALL layers (global section)\n",
    "print(\"=== INTERSECTION MODE (all layers) ===\")\n",
    "results_inter = ret_hrt.retrieve(query_basics, require_all_layers=True)\n",
    "print(f\"Found {len(results_inter)} candidates (global section):\")\n",
    "for col, total, layers in results_inter:\n",
    "    layer_str = \", \".join(f\"L{n}={v:.1f}\" for n, v in sorted(layers.items()))\n",
    "    print(f\"  col={col}: total={total:.1f} ({layer_str})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba063a6",
   "metadata": {},
   "source": [
    "## 17. Sheaf Interpretation\n",
    "\n",
    "The token clouds form a **sheaf** over the sub-lattice:\n",
    "\n",
    "```text\n",
    "┌─────────────────────────────────────────────────────────────────────────┐\n",
    "│                    TOKEN CLOUDS AS SHEAVES                              │\n",
    "├─────────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                         │\n",
    "│  A SHEAF assigns data to each \"open set\" consistently                   │\n",
    "│                                                                         │\n",
    "│  In our case:                                                           │\n",
    "│    Base space     = Sub-Lattice (connectivity from W)                   │\n",
    "│    Open sets      = N-gram layers (AM[n,:,:])                           │\n",
    "│    Stalk at point = Token cloud for that (row, layer)                   │\n",
    "│    Restriction    = More context → smaller cloud                        │\n",
    "│                                                                         │\n",
    "│       Cloud₂ (3-grams) ⊆ Cloud₁ (2-grams) ⊆ Cloud₀ (1-grams)            │\n",
    "│                                                                         │\n",
    "│       Global Section = ∩ all clouds = DISAMBIGUATED TOKENS              │\n",
    "│                                                                         │\n",
    "└─────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Key Insight**: The global section (intersection of all clouds) gives us the most context-consistent tokens!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fractal_manifold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
